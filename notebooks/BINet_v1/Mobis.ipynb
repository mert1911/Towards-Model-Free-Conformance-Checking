{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/mobis.csv', sep=',')  \n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log, \n",
    "        case_id='case', \n",
    "        activity_key='activity', \n",
    "        timestamp_key='start'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case'])\n",
    "dataframe_log = dataframe_log.drop(columns=['cost'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])\n",
    "dataframe_log = dataframe_log.drop(columns=['concept:name'])\n",
    "dataframe_log = dataframe_log.drop(columns=['@@index'])\n",
    "dataframe_log = dataframe_log.drop(columns=['travel_start'])\n",
    "dataframe_log = dataframe_log.drop(columns=['travel_end'])\n",
    "dataframe_log = dataframe_log.drop(columns=['start'])\n",
    "dataframe_log = dataframe_log.drop(columns=['end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['activity'])\n",
    "dataframe_log['activity'] = codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_6575/2141769449.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe_log['type'].fillna('missing', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values with a placeholder before factorization\n",
    "dataframe_log['type'].fillna('missing', inplace=True)\n",
    "\n",
    "# Factorize the 'type' column\n",
    "codes, uniques = pd.factorize(dataframe_log['type'])\n",
    "dataframe_log['type'] = codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_6575/3800960158.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe_log['user'].fillna('missing', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values with a placeholder before factorization\n",
    "dataframe_log['user'].fillna('missing', inplace=True)\n",
    "\n",
    "# Factorize the 'type' column\n",
    "codes, uniques = pd.factorize(dataframe_log['user'])\n",
    "dataframe_log['user'] = codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.160944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.160857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.160842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.160842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.144594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>3353</td>\n",
       "      <td>-0.742182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>3353</td>\n",
       "      <td>-0.736776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>3353</td>\n",
       "      <td>-0.583572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>3353</td>\n",
       "      <td>-0.404036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>3353</td>\n",
       "      <td>-0.194961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity  type  user  @@case_index  standardized_elapsed_time\n",
       "0             1     1     1             0                  -1.160944\n",
       "1             2     1     1             0                  -1.160857\n",
       "2             3     1     1             0                  -1.160842\n",
       "3             4     2     2             0                  -1.160842\n",
       "4             5     1     1             0                  -1.144594\n",
       "...         ...   ...   ...           ...                        ...\n",
       "55804         8     1   333          3353                  -0.742182\n",
       "55805         9     3    53          3353                  -0.736776\n",
       "55806        10     1   333          3353                  -0.583572\n",
       "55807        11     4    59          3353                  -0.404036\n",
       "55808        12     4    59          3353                  -0.194961\n",
       "\n",
       "[55809 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity = dataframe_log[['activity', '@@case_index']]\n",
    "df_type = dataframe_log[['type', '@@case_index']]\n",
    "df_user = dataframe_log[['user', '@@case_index']]\n",
    "df_timestamp = dataframe_log[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:24:03.573795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_prefix_windows(df, case_id_column='@@case_index', max_len=None):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "    \n",
    "    for case_id in df[case_id_column].unique():\n",
    "        case_data = df[df[case_id_column] == case_id].drop(columns=[case_id_column]).to_numpy()\n",
    "        \n",
    "        # Optional: Make sure to sort the case data if there's an implicit order (e.g., by timestamps)\n",
    "        # case_data = case_data.sort_values(by='timestamp_column').to_numpy()  # Uncomment and adjust if needed\n",
    "        \n",
    "        for i in range(1, len(case_data)):\n",
    "            window = case_data[:i]\n",
    "            target = case_data[i]\n",
    "            windows.append(window)\n",
    "            targets.append(target)\n",
    "            case_indices.append(case_id)\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = max(len(window) for window in windows)\n",
    "    \n",
    "    # Pad sequences\n",
    "    windows_padded = pad_sequences(windows, maxlen=max_len, padding='post', dtype='float32')\n",
    "    \n",
    "    # Convert targets to numpy array\n",
    "    targets_array = np.array(targets, dtype='float32')\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_padded, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_activity, targets_activity, case_indices = generate_prefix_windows(df_activity)\n",
    "windows_type, targets_type, case_indices = generate_prefix_windows(df_type)\n",
    "windows_user, targets_user, case_indices = generate_prefix_windows(df_user)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_prefix_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the @@case_index column and count the rows in each group\n",
    "case_lengths = dataframe_log.groupby('@@case_index').size()\n",
    "\n",
    "# Find the maximum value among the case lengths\n",
    "E = case_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " activity_input (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " type_input (InputLayer)     [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " user_input (InputLayer)     [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " activity_embedding (Embedd  (None, None, 50)             1350      ['activity_input[0][0]']      \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " type_embedding (Embedding)  (None, None, 50)             300       ['type_input[0][0]']          \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)  (None, None, 50)             16750     ['user_input[0][0]']          \n",
      "                                                                                                  \n",
      " timestamp_input (InputLaye  [(None, None, 1)]            0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " activity_encoder (GRU)      (None, None, 98)             44100     ['activity_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " type_encoder (GRU)          (None, None, 98)             44100     ['type_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " user_encoder (GRU)          (None, None, 98)             44100     ['user_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " timestamp_encoder (GRU)     (None, None, 98)             29694     ['timestamp_input[0][0]']     \n",
      "                                                                                                  \n",
      " bn_activity (BatchNormaliz  (None, None, 98)             392       ['activity_encoder[0][0]']    \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " bn_type (BatchNormalizatio  (None, None, 98)             392       ['type_encoder[0][0]']        \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " bn_user (BatchNormalizatio  (None, None, 98)             392       ['user_encoder[0][0]']        \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " bn_timestamp (BatchNormali  (None, None, 98)             392       ['timestamp_encoder[0][0]']   \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_encodings (Con  (None, None, 392)            0         ['bn_activity[0][0]',         \n",
      " catenate)                                                           'bn_type[0][0]',             \n",
      "                                                                     'bn_user[0][0]',             \n",
      "                                                                     'bn_timestamp[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)           (None, 98)                   144648    ['concatenate_encodings[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 98)                   0         ['decoder_gru[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 27)                   2673      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_type (Dense)         (None, 6)                    594       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_user (Dense)         (None, 335)                  33165     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    99        ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 363141 (1.39 MB)\n",
      "Trainable params: 362357 (1.38 MB)\n",
      "Non-trainable params: 784 (3.06 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Dropout, Concatenate, BatchNormalization\n",
    "\n",
    "def create_binetv3(num_activities, num_types, num_users, embedding_dim, gru_units, dropout_rate):\n",
    "    # Input layers for each attribute\n",
    "    input_activity = Input(shape=(None,), name='activity_input')\n",
    "    input_type = Input(shape=(None,), name='type_input')\n",
    "    input_user = Input(shape=(None,), name='user_input')\n",
    "    input_timestamp = Input(shape=(None, 1), name='timestamp_input')  # Continuous input\n",
    "\n",
    "    # Embedding layers for categorical attributes\n",
    "    embedding_activity = Embedding(input_dim=num_activities + 1, output_dim=embedding_dim, mask_zero=True, name='activity_embedding')(input_activity)\n",
    "    embedding_type = Embedding(input_dim=num_types + 1, output_dim=embedding_dim, mask_zero=True, name='type_embedding')(input_type)\n",
    "    embedding_user = Embedding(input_dim=num_users + 1, output_dim=embedding_dim, mask_zero=True, name='user_embedding')(input_user)\n",
    "\n",
    "    # Encoder GRUs with Batch Normalization for categorical attributes\n",
    "    encoded_activity = GRU(units=gru_units, return_sequences=True, name='activity_encoder')(embedding_activity)\n",
    "    bn_activity = BatchNormalization(name='bn_activity')(encoded_activity)\n",
    "    encoded_type = GRU(units=gru_units, return_sequences=True, name='type_encoder')(embedding_type)\n",
    "    bn_type = BatchNormalization(name='bn_type')(encoded_type)\n",
    "    encoded_user = GRU(units=gru_units, return_sequences=True, name='user_encoder')(embedding_user)\n",
    "    bn_user = BatchNormalization(name='bn_user')(encoded_user)\n",
    "\n",
    "    # Encoder GRU with Batch Normalization for continuous attribute\n",
    "    encoded_timestamp = GRU(units=gru_units, return_sequences=True, name='timestamp_encoder')(input_timestamp)\n",
    "    bn_timestamp = BatchNormalization(name='bn_timestamp')(encoded_timestamp)\n",
    "\n",
    "    # Concatenation of encoded outputs\n",
    "    concatenated = Concatenate(name='concatenate_encodings')([bn_activity, bn_type, bn_user, bn_timestamp])\n",
    "\n",
    "    # Decoder GRU\n",
    "    decoder_output = GRU(units=gru_units, return_sequences=False, name='decoder_gru')(concatenated)\n",
    "    dropout_layer = Dropout(rate=dropout_rate, name='dropout')(decoder_output)\n",
    "\n",
    "    # Output layers for predicting the next event's attributes\n",
    "    output_activity = Dense(num_activities + 1, activation='softmax', name='output_activity')(dropout_layer)\n",
    "    output_type = Dense(num_types + 1, activation='softmax', name='output_type')(dropout_layer)\n",
    "    output_user = Dense(num_users + 1, activation='softmax', name='output_user')(dropout_layer)\n",
    "    output_timestamp = Dense(1, activation='linear', name='output_timestamp')(dropout_layer)  # Linear activation for continuous output\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[input_activity, input_type, input_user, input_timestamp], outputs=[output_activity, output_type, output_user, output_timestamp])\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss={\n",
    "            'output_activity': 'categorical_crossentropy', \n",
    "            'output_type': 'categorical_crossentropy', \n",
    "            'output_user': 'categorical_crossentropy', \n",
    "            'output_timestamp': 'mean_squared_error'\n",
    "        },\n",
    "        metrics={\n",
    "            'output_activity': ['accuracy'], \n",
    "            'output_type': ['accuracy'], \n",
    "            'output_user': ['accuracy'], \n",
    "            'output_timestamp': ['mean_squared_error']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "gru_units = int(2 * E)\n",
    "num_activities = dataframe_log['activity'].max()\n",
    "num_types = dataframe_log['type'].max()\n",
    "num_users = dataframe_log['user'].max()\n",
    "embedding_dim = 50\n",
    "dropout_rate = 0.2\n",
    "model = create_binetv3(num_activities, num_types, num_users, embedding_dim, gru_units, dropout_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for each attribute including timestamps\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "train_type, test_type, train_targets_type, test_targets_type = train_test_split(\n",
    "    windows_type, targets_type, test_size=0.3, random_state=42)\n",
    "\n",
    "train_user, test_user, train_targets_user, test_targets_user = train_test_split(\n",
    "    windows_user, targets_user, test_size=0.3, random_state=42)\n",
    "\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert categorical targets to one-hot encoding\n",
    "train_targets_activity = to_categorical(train_targets_activity, num_classes=num_activities + 1)\n",
    "test_targets_activity = to_categorical(test_targets_activity, num_classes=num_activities + 1)\n",
    "\n",
    "train_targets_type = to_categorical(train_targets_type, num_classes=num_types + 1)\n",
    "test_targets_type = to_categorical(test_targets_type, num_classes=num_types + 1)\n",
    "\n",
    "train_targets_user = to_categorical(train_targets_user, num_classes=num_users + 1)\n",
    "test_targets_user = to_categorical(test_targets_user, num_classes=num_users + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "74/74 [==============================] - 227s 2s/step - loss: 6.6847 - output_activity_loss: 1.2939 - output_type_loss: 0.5693 - output_user_loss: 4.5574 - output_timestamp_loss: 0.2641 - output_activity_accuracy: 0.6680 - output_type_accuracy: 0.7908 - output_user_accuracy: 0.1923 - output_timestamp_mean_squared_error: 0.2641 - val_loss: 10.0645 - val_output_activity_loss: 2.6565 - val_output_type_loss: 1.2287 - val_output_user_loss: 5.4201 - val_output_timestamp_loss: 0.7592 - val_output_activity_accuracy: 0.6430 - val_output_type_accuracy: 0.4914 - val_output_user_accuracy: 0.2224 - val_output_timestamp_mean_squared_error: 0.7592\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 103s 1s/step - loss: 4.2594 - output_activity_loss: 0.3982 - output_type_loss: 0.2476 - output_user_loss: 3.4090 - output_timestamp_loss: 0.2046 - output_activity_accuracy: 0.8697 - output_type_accuracy: 0.8980 - output_user_accuracy: 0.3382 - output_timestamp_mean_squared_error: 0.2046 - val_loss: 8.9821 - val_output_activity_loss: 2.2718 - val_output_type_loss: 1.0010 - val_output_user_loss: 5.0488 - val_output_timestamp_loss: 0.6605 - val_output_activity_accuracy: 0.7932 - val_output_type_accuracy: 0.7547 - val_output_user_accuracy: 0.3267 - val_output_timestamp_mean_squared_error: 0.6605\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 101s 1s/step - loss: 3.6184 - output_activity_loss: 0.3532 - output_type_loss: 0.2376 - output_user_loss: 2.8718 - output_timestamp_loss: 0.1558 - output_activity_accuracy: 0.8731 - output_type_accuracy: 0.9025 - output_user_accuracy: 0.4456 - output_timestamp_mean_squared_error: 0.1558 - val_loss: 7.3332 - val_output_activity_loss: 1.7178 - val_output_type_loss: 0.6902 - val_output_user_loss: 4.3946 - val_output_timestamp_loss: 0.5306 - val_output_activity_accuracy: 0.8055 - val_output_type_accuracy: 0.8799 - val_output_user_accuracy: 0.3976 - val_output_timestamp_mean_squared_error: 0.5306\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 98s 1s/step - loss: 2.9501 - output_activity_loss: 0.3473 - output_type_loss: 0.2385 - output_user_loss: 2.2357 - output_timestamp_loss: 0.1286 - output_activity_accuracy: 0.8740 - output_type_accuracy: 0.9030 - output_user_accuracy: 0.5775 - output_timestamp_mean_squared_error: 0.1286 - val_loss: 5.1725 - val_output_activity_loss: 1.0711 - val_output_type_loss: 0.4250 - val_output_user_loss: 3.3264 - val_output_timestamp_loss: 0.3501 - val_output_activity_accuracy: 0.8604 - val_output_type_accuracy: 0.8929 - val_output_user_accuracy: 0.5504 - val_output_timestamp_mean_squared_error: 0.3501\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 87s 1s/step - loss: 2.3426 - output_activity_loss: 0.3460 - output_type_loss: 0.2421 - output_user_loss: 1.6408 - output_timestamp_loss: 0.1136 - output_activity_accuracy: 0.8757 - output_type_accuracy: 0.9015 - output_user_accuracy: 0.6831 - output_timestamp_mean_squared_error: 0.1136 - val_loss: 3.3266 - val_output_activity_loss: 0.6016 - val_output_type_loss: 0.3024 - val_output_user_loss: 2.1883 - val_output_timestamp_loss: 0.2343 - val_output_activity_accuracy: 0.8640 - val_output_type_accuracy: 0.8942 - val_output_user_accuracy: 0.6717 - val_output_timestamp_mean_squared_error: 0.2343\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 95s 1s/step - loss: 1.9054 - output_activity_loss: 0.3326 - output_type_loss: 0.2370 - output_user_loss: 1.2305 - output_timestamp_loss: 0.1053 - output_activity_accuracy: 0.8765 - output_type_accuracy: 0.9044 - output_user_accuracy: 0.7429 - output_timestamp_mean_squared_error: 0.1053 - val_loss: 2.1999 - val_output_activity_loss: 0.3796 - val_output_type_loss: 0.2338 - val_output_user_loss: 1.4305 - val_output_timestamp_loss: 0.1561 - val_output_activity_accuracy: 0.8814 - val_output_type_accuracy: 0.9030 - val_output_user_accuracy: 0.7205 - val_output_timestamp_mean_squared_error: 0.1561\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 95s 1s/step - loss: 1.6352 - output_activity_loss: 0.3181 - output_type_loss: 0.2319 - output_user_loss: 0.9847 - output_timestamp_loss: 0.1005 - output_activity_accuracy: 0.8774 - output_type_accuracy: 0.9040 - output_user_accuracy: 0.7692 - output_timestamp_mean_squared_error: 0.1005 - val_loss: 1.6887 - val_output_activity_loss: 0.3185 - val_output_type_loss: 0.2239 - val_output_user_loss: 1.0202 - val_output_timestamp_loss: 0.1260 - val_output_activity_accuracy: 0.8833 - val_output_type_accuracy: 0.9094 - val_output_user_accuracy: 0.7793 - val_output_timestamp_mean_squared_error: 0.1260\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 98s 1s/step - loss: 1.4576 - output_activity_loss: 0.3028 - output_type_loss: 0.2252 - output_user_loss: 0.8340 - output_timestamp_loss: 0.0957 - output_activity_accuracy: 0.8808 - output_type_accuracy: 0.9061 - output_user_accuracy: 0.7890 - output_timestamp_mean_squared_error: 0.0957 - val_loss: 1.4248 - val_output_activity_loss: 0.2946 - val_output_type_loss: 0.2158 - val_output_user_loss: 0.8260 - val_output_timestamp_loss: 0.0884 - val_output_activity_accuracy: 0.8880 - val_output_type_accuracy: 0.9101 - val_output_user_accuracy: 0.7893 - val_output_timestamp_mean_squared_error: 0.0884\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.3454 - output_activity_loss: 0.2958 - output_type_loss: 0.2194 - output_user_loss: 0.7386 - output_timestamp_loss: 0.0916 - output_activity_accuracy: 0.8799 - output_type_accuracy: 0.9069 - output_user_accuracy: 0.7976 - output_timestamp_mean_squared_error: 0.0916 - val_loss: 1.3241 - val_output_activity_loss: 0.2920 - val_output_type_loss: 0.2213 - val_output_user_loss: 0.7346 - val_output_timestamp_loss: 0.0762 - val_output_activity_accuracy: 0.8817 - val_output_type_accuracy: 0.9023 - val_output_user_accuracy: 0.7923 - val_output_timestamp_mean_squared_error: 0.0762\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 90s 1s/step - loss: 1.2624 - output_activity_loss: 0.2836 - output_type_loss: 0.2122 - output_user_loss: 0.6772 - output_timestamp_loss: 0.0894 - output_activity_accuracy: 0.8833 - output_type_accuracy: 0.9103 - output_user_accuracy: 0.8043 - output_timestamp_mean_squared_error: 0.0894 - val_loss: 1.2316 - val_output_activity_loss: 0.2876 - val_output_type_loss: 0.2184 - val_output_user_loss: 0.6588 - val_output_timestamp_loss: 0.0668 - val_output_activity_accuracy: 0.8852 - val_output_type_accuracy: 0.9074 - val_output_user_accuracy: 0.8000 - val_output_timestamp_mean_squared_error: 0.0668\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 89s 1s/step - loss: 1.2154 - output_activity_loss: 0.2780 - output_type_loss: 0.2107 - output_user_loss: 0.6403 - output_timestamp_loss: 0.0865 - output_activity_accuracy: 0.8854 - output_type_accuracy: 0.9101 - output_user_accuracy: 0.8061 - output_timestamp_mean_squared_error: 0.0865 - val_loss: 1.2090 - val_output_activity_loss: 0.2840 - val_output_type_loss: 0.2242 - val_output_user_loss: 0.6379 - val_output_timestamp_loss: 0.0629 - val_output_activity_accuracy: 0.8836 - val_output_type_accuracy: 0.9081 - val_output_user_accuracy: 0.8062 - val_output_timestamp_mean_squared_error: 0.0629\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 101s 1s/step - loss: 1.1775 - output_activity_loss: 0.2723 - output_type_loss: 0.2095 - output_user_loss: 0.6111 - output_timestamp_loss: 0.0847 - output_activity_accuracy: 0.8862 - output_type_accuracy: 0.9097 - output_user_accuracy: 0.8092 - output_timestamp_mean_squared_error: 0.0847 - val_loss: 1.1665 - val_output_activity_loss: 0.2808 - val_output_type_loss: 0.2216 - val_output_user_loss: 0.6050 - val_output_timestamp_loss: 0.0591 - val_output_activity_accuracy: 0.8833 - val_output_type_accuracy: 0.9030 - val_output_user_accuracy: 0.8017 - val_output_timestamp_mean_squared_error: 0.0591\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 109s 1s/step - loss: 1.1479 - output_activity_loss: 0.2674 - output_type_loss: 0.2076 - output_user_loss: 0.5897 - output_timestamp_loss: 0.0832 - output_activity_accuracy: 0.8878 - output_type_accuracy: 0.9117 - output_user_accuracy: 0.8143 - output_timestamp_mean_squared_error: 0.0832 - val_loss: 1.1526 - val_output_activity_loss: 0.2837 - val_output_type_loss: 0.2211 - val_output_user_loss: 0.5870 - val_output_timestamp_loss: 0.0609 - val_output_activity_accuracy: 0.8795 - val_output_type_accuracy: 0.9020 - val_output_user_accuracy: 0.7990 - val_output_timestamp_mean_squared_error: 0.0609\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 148s 2s/step - loss: 1.1117 - output_activity_loss: 0.2607 - output_type_loss: 0.2003 - output_user_loss: 0.5699 - output_timestamp_loss: 0.0808 - output_activity_accuracy: 0.8893 - output_type_accuracy: 0.9140 - output_user_accuracy: 0.8156 - output_timestamp_mean_squared_error: 0.0808 - val_loss: 1.1232 - val_output_activity_loss: 0.2790 - val_output_type_loss: 0.2183 - val_output_user_loss: 0.5686 - val_output_timestamp_loss: 0.0573 - val_output_activity_accuracy: 0.8826 - val_output_type_accuracy: 0.9063 - val_output_user_accuracy: 0.8040 - val_output_timestamp_mean_squared_error: 0.0573\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 205s 3s/step - loss: 1.0891 - output_activity_loss: 0.2571 - output_type_loss: 0.1990 - output_user_loss: 0.5531 - output_timestamp_loss: 0.0799 - output_activity_accuracy: 0.8908 - output_type_accuracy: 0.9129 - output_user_accuracy: 0.8185 - output_timestamp_mean_squared_error: 0.0799 - val_loss: 1.1271 - val_output_activity_loss: 0.2793 - val_output_type_loss: 0.2200 - val_output_user_loss: 0.5707 - val_output_timestamp_loss: 0.0571 - val_output_activity_accuracy: 0.8817 - val_output_type_accuracy: 0.9079 - val_output_user_accuracy: 0.8054 - val_output_timestamp_mean_squared_error: 0.0571\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 274s 4s/step - loss: 1.0743 - output_activity_loss: 0.2539 - output_type_loss: 0.1965 - output_user_loss: 0.5450 - output_timestamp_loss: 0.0789 - output_activity_accuracy: 0.8913 - output_type_accuracy: 0.9152 - output_user_accuracy: 0.8187 - output_timestamp_mean_squared_error: 0.0789 - val_loss: 1.1490 - val_output_activity_loss: 0.2847 - val_output_type_loss: 0.2271 - val_output_user_loss: 0.5826 - val_output_timestamp_loss: 0.0546 - val_output_activity_accuracy: 0.8800 - val_output_type_accuracy: 0.9006 - val_output_user_accuracy: 0.8012 - val_output_timestamp_mean_squared_error: 0.0546\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 297s 4s/step - loss: 1.0607 - output_activity_loss: 0.2509 - output_type_loss: 0.1956 - output_user_loss: 0.5360 - output_timestamp_loss: 0.0782 - output_activity_accuracy: 0.8922 - output_type_accuracy: 0.9157 - output_user_accuracy: 0.8219 - output_timestamp_mean_squared_error: 0.0782 - val_loss: 1.1235 - val_output_activity_loss: 0.2789 - val_output_type_loss: 0.2222 - val_output_user_loss: 0.5665 - val_output_timestamp_loss: 0.0558 - val_output_activity_accuracy: 0.8843 - val_output_type_accuracy: 0.9095 - val_output_user_accuracy: 0.8108 - val_output_timestamp_mean_squared_error: 0.0558\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 283s 4s/step - loss: 1.0443 - output_activity_loss: 0.2470 - output_type_loss: 0.1928 - output_user_loss: 0.5275 - output_timestamp_loss: 0.0770 - output_activity_accuracy: 0.8935 - output_type_accuracy: 0.9163 - output_user_accuracy: 0.8216 - output_timestamp_mean_squared_error: 0.0770 - val_loss: 1.1251 - val_output_activity_loss: 0.2812 - val_output_type_loss: 0.2233 - val_output_user_loss: 0.5640 - val_output_timestamp_loss: 0.0567 - val_output_activity_accuracy: 0.8812 - val_output_type_accuracy: 0.9046 - val_output_user_accuracy: 0.8052 - val_output_timestamp_mean_squared_error: 0.0567\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 284s 4s/step - loss: 1.0272 - output_activity_loss: 0.2451 - output_type_loss: 0.1896 - output_user_loss: 0.5159 - output_timestamp_loss: 0.0766 - output_activity_accuracy: 0.8953 - output_type_accuracy: 0.9175 - output_user_accuracy: 0.8239 - output_timestamp_mean_squared_error: 0.0766 - val_loss: 1.1281 - val_output_activity_loss: 0.2839 - val_output_type_loss: 0.2246 - val_output_user_loss: 0.5651 - val_output_timestamp_loss: 0.0546 - val_output_activity_accuracy: 0.8769 - val_output_type_accuracy: 0.9074 - val_output_user_accuracy: 0.8044 - val_output_timestamp_mean_squared_error: 0.0546\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 267s 4s/step - loss: 1.0159 - output_activity_loss: 0.2430 - output_type_loss: 0.1893 - output_user_loss: 0.5084 - output_timestamp_loss: 0.0752 - output_activity_accuracy: 0.8948 - output_type_accuracy: 0.9182 - output_user_accuracy: 0.8279 - output_timestamp_mean_squared_error: 0.0752 - val_loss: 1.1191 - val_output_activity_loss: 0.2824 - val_output_type_loss: 0.2238 - val_output_user_loss: 0.5585 - val_output_timestamp_loss: 0.0544 - val_output_activity_accuracy: 0.8794 - val_output_type_accuracy: 0.9101 - val_output_user_accuracy: 0.8045 - val_output_timestamp_mean_squared_error: 0.0544\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_activity, train_type, train_user, train_timestamp],\n",
    "    [train_targets_activity, train_targets_type, train_targets_user, train_targets_timestamp],\n",
    "    validation_data=([test_activity, test_type, test_user, test_timestamp], [test_targets_activity, test_targets_type, test_targets_user, test_targets_timestamp]),\n",
    "    epochs=20,\n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 60s 242ms/step - loss: 1.1191 - output_activity_loss: 0.2824 - output_type_loss: 0.2238 - output_user_loss: 0.5585 - output_timestamp_loss: 0.0544 - output_activity_accuracy: 0.8794 - output_type_accuracy: 0.9101 - output_user_accuracy: 0.8045 - output_timestamp_mean_squared_error: 0.0544\n",
      "Validation Loss: 1.1190799474716187, Validation Accuracy: 0.28235024213790894\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "results = model.evaluate(\n",
    "    [test_activity, test_type, test_user, test_timestamp],\n",
    "    [test_targets_activity, test_targets_type, test_targets_user, test_targets_timestamp],\n",
    "    batch_size=64\n",
    ")\n",
    "print(f\"Validation Loss: {results[0]}, Validation Accuracy: {results[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640/1640 [==============================] - 193s 98ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_activity, windows_type, windows_user, windows_timestamp])\n",
    "\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_activity = predictions[0]\n",
    "predictions_type = predictions[1]\n",
    "predictions_user = predictions[2]  \n",
    "predictions_timestamp = predictions[3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_scores(predictions, targets):\n",
    "    scores = []\n",
    "    # Loop through each example in the predictions\n",
    "    for i in range(predictions.shape[0]):\n",
    "        actual_prob = predictions[i, targets[i]]  # Extract the probability of the true class using target index\n",
    "        # Calculate anomaly score as sum of probabilities greater than the probability of the actual value\n",
    "        anomaly_score = np.sum(predictions[i][predictions[i] > actual_prob])\n",
    "        scores.append(anomaly_score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_user = calculate_anomaly_scores(predictions_user, targets_user.astype(int))\n",
    "anomaly_scores_activity = calculate_anomaly_scores(predictions_activity, targets_activity.astype(int))\n",
    "anomaly_scores_type = calculate_anomaly_scores(predictions_type, targets_type.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals):\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    max_diff = np.max(differences)\n",
    "    normalized_scores = differences / max_diff if max_diff != 0 else differences\n",
    "    \n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp)\n",
    "anomaly_scores_timestamp = anomaly_scores_timestamp.flatten()\n",
    "anomaly_scores_timestamp = anomaly_scores_timestamp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold (lowest plateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_ratio(scores, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the anomaly ratio for a given threshold.\n",
    "    \"\"\"\n",
    "    return np.mean(scores > threshold)\n",
    "\n",
    "def find_plateaus(scores, epsilon=1e-4, min_plateau_length=10):\n",
    "    \"\"\"\n",
    "    Identify the lowest plateau in the anomaly ratio function and calculate the mean-centered threshold.\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)  # Convert scores to a NumPy array\n",
    "    sorted_scores = np.sort(scores)\n",
    "    \n",
    "    # Remove duplicate values\n",
    "    unique_thresholds, unique_indices = np.unique(sorted_scores, return_index=True)\n",
    "    anomaly_ratios = np.array([calculate_anomaly_ratio(scores, t) for t in unique_thresholds])\n",
    "    \n",
    "    # Calculate first and second derivatives\n",
    "    first_derivatives = np.diff(anomaly_ratios) / np.diff(unique_thresholds)\n",
    "    second_derivatives = np.diff(first_derivatives) / np.diff(unique_thresholds[:-1])\n",
    "    \n",
    "    # Identify plateaus where the first derivative is close to zero\n",
    "    plateau_indices = np.where(np.abs(first_derivatives) < epsilon)[0]\n",
    "    \n",
    "    # Group consecutive indices to identify continuous plateaus\n",
    "    grouped_plateaus = np.split(plateau_indices, np.where(np.diff(plateau_indices) != 1)[0] + 1)\n",
    "    \n",
    "    # Filter plateaus based on minimum length\n",
    "    long_plateaus = [g for g in grouped_plateaus if len(g) >= min_plateau_length]\n",
    "    \n",
    "    if long_plateaus:\n",
    "        # Take the first long plateau and find the mean threshold in this plateau\n",
    "        first_plateau = long_plateaus[0]\n",
    "        plateau_thresholds = unique_thresholds[first_plateau]\n",
    "        return np.mean(plateau_thresholds)\n",
    "    else:\n",
    "        # If no plateau is found, return a default value, e.g., the 90th percentile\n",
    "        percentile_90 = np.percentile(sorted_scores, 90)\n",
    "        if percentile_90 == 1.0:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return percentile_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_activity = find_plateaus(anomaly_scores_activity)\n",
    "threshold_user = find_plateaus(anomaly_scores_user)\n",
    "threshold_type = find_plateaus(anomaly_scores_type)\n",
    "threshold_timestamp = find_plateaus(anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(anomaly_scores, threshold):\n",
    "    labels = [1 if score > threshold else 0 for score in anomaly_scores]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies based on the calculated anomaly scores and thresholds\n",
    "labels_activity = detect_anomalies(anomaly_scores_activity, threshold_activity)\n",
    "labels_user = detect_anomalies(anomaly_scores_user, threshold_user)\n",
    "labels_type = detect_anomalies(anomaly_scores_type, threshold_type)\n",
    "labels_timestamp = detect_anomalies(anomaly_scores_timestamp, threshold_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted_activity</th>\n",
       "      <th>predicted_user</th>\n",
       "      <th>predicted_type</th>\n",
       "      <th>predicted_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52450</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52451</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52452</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52453</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52454</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52455 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted_activity  predicted_user  predicted_type  \\\n",
       "0         0                   0               1               1   \n",
       "1         0                   0               1               1   \n",
       "2         0                   0               0               0   \n",
       "3         0                   0               0               0   \n",
       "4         0                   0               0               0   \n",
       "...     ...                 ...             ...             ...   \n",
       "52450  3353                   0               0               0   \n",
       "52451  3353                   0               0               0   \n",
       "52452  3353                   0               0               0   \n",
       "52453  3353                   0               0               0   \n",
       "52454  3353                   0               0               0   \n",
       "\n",
       "       predicted_timestamp  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        1  \n",
       "4                        1  \n",
       "...                    ...  \n",
       "52450                    0  \n",
       "52451                    0  \n",
       "52452                    0  \n",
       "52453                    0  \n",
       "52454                    1  \n",
       "\n",
       "[52455 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted_activity'] = labels_activity\n",
    "mapping['predicted_user'] = labels_user\n",
    "mapping['predicted_type'] = labels_type\n",
    "mapping['predicted_timestamp'] = labels_timestamp\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "        ...  \n",
       "3349    False\n",
       "3350     True\n",
       "3351     True\n",
       "3352     True\n",
       "3353     True\n",
       "Length: 3354, dtype: bool"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean DataFrame where each value is True if the value is 1\n",
    "contains_one = (mapping[['predicted_activity', 'predicted_user', 'predicted_type', 'predicted_timestamp']] == 1)\n",
    "\n",
    "# Group by 'case' and check if there's at least one 'True' in any of the columns\n",
    "case_prediction = contains_one.groupby(mapping['case']).any().any(axis=1)\n",
    "case_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mert2/anaconda3/envs/jp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "python(7698) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "aligning log, completed variants :: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:31<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/MobisToBe.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(0)\n",
    "        else:\n",
    "            conformance_status.append(1)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3354 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              0          1\n",
       "1              1          1\n",
       "2              0          1\n",
       "3              1          1\n",
       "4              1          1\n",
       "...          ...        ...\n",
       "3349           0          0\n",
       "3350           1          1\n",
       "3351           0          1\n",
       "3352           0          1\n",
       "3353           0          1\n",
       "\n",
       "[3354 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Dev: 0.50\n"
     ]
    }
   ],
   "source": [
    "precision_dev = TP / (TP + FP)\n",
    "print(f\"Precision Dev: {precision_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Dev: 1.00\n"
     ]
    }
   ],
   "source": [
    "recall_dev = TP / (TP + FN)\n",
    "print(f\"Recall Dev: {recall_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision No Dev: 0.97\n"
     ]
    }
   ],
   "source": [
    "precision_no_dev = TN / (TN + FN)\n",
    "print(f\"Precision No Dev: {precision_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall No Dev: 0.02\n"
     ]
    }
   ],
   "source": [
    "recall_no_dev = TN / (TN + FP)\n",
    "print(f\"Recall No Dev: {recall_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
