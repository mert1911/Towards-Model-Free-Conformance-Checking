{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/BPIC12_Log_onlyO.csv', sep=',')\n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['lifecycle:transition'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])\n",
    "dataframe_log = dataframe_log.drop(columns=['@@index'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:AMOUNT_REQ'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:REG_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_SELECTED</td>\n",
       "      <td>2011-10-01 09:45:09.243000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_CREATED</td>\n",
       "      <td>2011-10-01 09:45:11.197000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>2011-10-01 09:45:11.380000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11049</td>\n",
       "      <td>O_SENT_BACK</td>\n",
       "      <td>2011-10-10 09:33:03.668000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10629</td>\n",
       "      <td>O_ACCEPTED</td>\n",
       "      <td>2011-10-13 08:37:29.226000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31239</th>\n",
       "      <td>11003</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>2012-03-02 09:14:43.303000+00:00</td>\n",
       "      <td>5013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31240</th>\n",
       "      <td>10789</td>\n",
       "      <td>O_SENT_BACK</td>\n",
       "      <td>2012-03-09 10:01:46.008000+00:00</td>\n",
       "      <td>5013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31241</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_SELECTED</td>\n",
       "      <td>2012-03-01 19:22:38.593000+00:00</td>\n",
       "      <td>5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31242</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_CREATED</td>\n",
       "      <td>2012-03-01 19:22:40.016000+00:00</td>\n",
       "      <td>5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31243</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>2012-03-01 19:22:40.043000+00:00</td>\n",
       "      <td>5014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31244 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource concept:name                   time:timestamp  \\\n",
       "0             10862   O_SELECTED 2011-10-01 09:45:09.243000+00:00   \n",
       "1             10862    O_CREATED 2011-10-01 09:45:11.197000+00:00   \n",
       "2             10862       O_SENT 2011-10-01 09:45:11.380000+00:00   \n",
       "3             11049  O_SENT_BACK 2011-10-10 09:33:03.668000+00:00   \n",
       "4             10629   O_ACCEPTED 2011-10-13 08:37:29.226000+00:00   \n",
       "...             ...          ...                              ...   \n",
       "31239         11003       O_SENT 2012-03-02 09:14:43.303000+00:00   \n",
       "31240         10789  O_SENT_BACK 2012-03-09 10:01:46.008000+00:00   \n",
       "31241         10933   O_SELECTED 2012-03-01 19:22:38.593000+00:00   \n",
       "31242         10933    O_CREATED 2012-03-01 19:22:40.016000+00:00   \n",
       "31243         10933       O_SENT 2012-03-01 19:22:40.043000+00:00   \n",
       "\n",
       "       @@case_index  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "31239          5013  \n",
       "31240          5013  \n",
       "31241          5014  \n",
       "31242          5014  \n",
       "31243          5014  \n",
       "\n",
       "[31244 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['org:resource'])\n",
    "dataframe_log['org:resource'] = codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['concept:name'])\n",
    "dataframe_log['concept:name'] = codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.299272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31239</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5013</td>\n",
       "      <td>-0.620265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31240</th>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>5013</td>\n",
       "      <td>0.099241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31241</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31242</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31243</th>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31244 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource  concept:name  @@case_index  standardized_elapsed_time\n",
       "0                 1             1             0                  -0.620650\n",
       "1                 1             2             0                  -0.620647\n",
       "2                 1             3             0                  -0.620647\n",
       "3                 2             4             0                   0.299272\n",
       "4                 3             5             0                   0.602251\n",
       "...             ...           ...           ...                        ...\n",
       "31239            52             3          5013                  -0.620265\n",
       "31240            26             4          5013                   0.099241\n",
       "31241            46             1          5014                  -0.620650\n",
       "31242            46             2          5014                  -0.620648\n",
       "31243            46             3          5014                  -0.620648\n",
       "\n",
       "[31244 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resource = dataframe_log[['org:resource', '@@case_index']]\n",
    "df_activity = dataframe_log[['concept:name', '@@case_index']]\n",
    "df_timestamp = dataframe_log[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:11:46.366653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_prefix_windows(df, case_id_column='@@case_index', max_len=None):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "    \n",
    "    for case_id in df[case_id_column].unique():\n",
    "        case_data = df[df[case_id_column] == case_id].drop(columns=[case_id_column]).to_numpy()\n",
    "        \n",
    "        # Optional: Make sure to sort the case data if there's an implicit order (e.g., by timestamps)\n",
    "        # case_data = case_data.sort_values(by='timestamp_column').to_numpy()  # Uncomment and adjust if needed\n",
    "        \n",
    "        for i in range(1, len(case_data)):\n",
    "            window = case_data[:i]\n",
    "            target = case_data[i]\n",
    "            windows.append(window)\n",
    "            targets.append(target)\n",
    "            case_indices.append(case_id)\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = max(len(window) for window in windows)\n",
    "    \n",
    "    # Pad sequences\n",
    "    windows_padded = pad_sequences(windows, maxlen=max_len, padding='post', dtype='float32')\n",
    "    \n",
    "    # Convert targets to numpy array\n",
    "    targets_array = np.array(targets, dtype='float32')\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_padded, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_resource, targets_resource, case_indices = generate_prefix_windows(df_resource)\n",
    "windows_activity, targets_activity, case_indices = generate_prefix_windows(df_activity)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_prefix_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the @@case_index column and count the rows in each group\n",
    "case_lengths = dataframe_log.groupby('@@case_index').size()\n",
    "\n",
    "# Find the maximum value among the case lengths\n",
    "E = case_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " activity_input (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " resource_input (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " activity_embedding (Embedd  (None, None, 50)             400       ['activity_input[0][0]']      \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " resource_embedding (Embedd  (None, None, 50)             3050      ['resource_input[0][0]']      \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " timestamp_input (InputLaye  [(None, None, 1)]            0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " activity_encoder (GRU)      (None, None, 60)             20160     ['activity_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " resource_encoder (GRU)      (None, None, 60)             20160     ['resource_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " timestamp_encoder (GRU)     (None, None, 60)             11340     ['timestamp_input[0][0]']     \n",
      "                                                                                                  \n",
      " bn_activity (BatchNormaliz  (None, None, 60)             240       ['activity_encoder[0][0]']    \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " bn_resource (BatchNormaliz  (None, None, 60)             240       ['resource_encoder[0][0]']    \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " bn_timestamp (BatchNormali  (None, None, 60)             240       ['timestamp_encoder[0][0]']   \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_encodings (Con  (None, None, 180)            0         ['bn_activity[0][0]',         \n",
      " catenate)                                                           'bn_resource[0][0]',         \n",
      "                                                                     'bn_timestamp[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)           (None, 60)                   43560     ['concatenate_encodings[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 60)                   0         ['decoder_gru[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 8)                    488       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_resource (Dense)     (None, 61)                   3721      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    61        ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 103660 (404.92 KB)\n",
      "Trainable params: 103300 (403.52 KB)\n",
      "Non-trainable params: 360 (1.41 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Dropout, Concatenate, BatchNormalization\n",
    "\n",
    "def create_binetv3(num_activities, num_resources, embedding_dim, gru_units, dropout_rate):\n",
    "    # Input layers for each attribute\n",
    "    input_activity = Input(shape=(None,), name='activity_input')\n",
    "    input_resource = Input(shape=(None,), name='resource_input')\n",
    "    input_timestamp = Input(shape=(None, 1), name='timestamp_input')\n",
    "\n",
    "    # Embedding layers for categorical attributes\n",
    "    embedding_activity = Embedding(input_dim=num_activities + 1, output_dim=embedding_dim, mask_zero=True, name='activity_embedding')(input_activity)\n",
    "    embedding_resource = Embedding(input_dim=num_resources + 1, output_dim=embedding_dim, mask_zero=True, name='resource_embedding')(input_resource)\n",
    "\n",
    "    # Encoder GRUs with Batch Normalization for categorical attributes\n",
    "    encoded_activity = GRU(units=gru_units, return_sequences=True, name='activity_encoder')(embedding_activity)\n",
    "    bn_activity = BatchNormalization(name='bn_activity')(encoded_activity)\n",
    "    encoded_resource = GRU(units=gru_units, return_sequences=True, name='resource_encoder')(embedding_resource)\n",
    "    bn_resource = BatchNormalization(name='bn_resource')(encoded_resource)\n",
    "\n",
    "    # Encoder GRU with Batch Normalization for continuous attribute\n",
    "    encoded_timestamp = GRU(units=gru_units, return_sequences=True, name='timestamp_encoder')(input_timestamp)\n",
    "    bn_timestamp = BatchNormalization(name='bn_timestamp')(encoded_timestamp)\n",
    "\n",
    "    # Concatenation of encoded outputs\n",
    "    concatenated = Concatenate(name='concatenate_encodings')([bn_activity, bn_resource, bn_timestamp])\n",
    "\n",
    "    # Decoder GRU\n",
    "    decoder_output = GRU(units=gru_units, return_sequences=False, name='decoder_gru')(concatenated)\n",
    "    dropout_layer = Dropout(rate=dropout_rate, name='dropout')(decoder_output)\n",
    "\n",
    "    # Output layers for predicting the next event's attributes\n",
    "    output_activity = Dense(num_activities + 1, activation='softmax', name='output_activity')(dropout_layer)\n",
    "    output_resource = Dense(num_resources + 1, activation='softmax', name='output_resource')(dropout_layer)\n",
    "    output_timestamp = Dense(1, activation='linear', name='output_timestamp')(dropout_layer)\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[input_activity, input_resource, input_timestamp], outputs=[output_activity, output_resource, output_timestamp])\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss={\n",
    "            'output_activity': 'categorical_crossentropy', \n",
    "            'output_resource': 'categorical_crossentropy',\n",
    "            'output_timestamp': 'mse'\n",
    "        },\n",
    "        metrics={\n",
    "            'output_activity': ['accuracy'], \n",
    "            'output_resource': ['accuracy'],\n",
    "            'output_timestamp': ['mse']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "gru_units = int(2 * E) \n",
    "num_activities = dataframe_log['concept:name'].max()\n",
    "num_resources = dataframe_log['org:resource'].max()\n",
    "embedding_dim = 50\n",
    "dropout_rate = 0.2\n",
    "model = create_binetv3(num_activities, num_resources, embedding_dim, gru_units, dropout_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for the resource attribute\n",
    "train_resource, test_resource, train_targets_resource, test_targets_resource = train_test_split(\n",
    "    windows_resource, targets_resource, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for the activity attribute\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for the timestamp attribute\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_targets_resource_cat = to_categorical(train_targets_resource, num_classes=num_resources + 1)\n",
    "test_targets_resource_cat = to_categorical(test_targets_resource, num_classes=num_resources + 1)\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities + 1)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37/37 [==============================] - 98s 1s/step - loss: 5.6232 - output_activity_loss: 1.3765 - output_resource_loss: 3.7835 - output_timestamp_loss: 0.4632 - output_activity_accuracy: 0.5596 - output_resource_accuracy: 0.1699 - output_timestamp_mse: 0.4632 - val_loss: 6.9350 - val_output_activity_loss: 1.9109 - val_output_resource_loss: 4.0265 - val_output_timestamp_loss: 0.9976 - val_output_activity_accuracy: 0.6049 - val_output_resource_accuracy: 0.1765 - val_output_timestamp_mse: 0.9976\n",
      "Epoch 2/20\n",
      "37/37 [==============================] - 27s 735ms/step - loss: 3.8572 - output_activity_loss: 0.6805 - output_resource_loss: 2.7913 - output_timestamp_loss: 0.3853 - output_activity_accuracy: 0.7742 - output_resource_accuracy: 0.4495 - output_timestamp_mse: 0.3853 - val_loss: 6.6451 - val_output_activity_loss: 1.7845 - val_output_resource_loss: 3.8684 - val_output_timestamp_loss: 0.9922 - val_output_activity_accuracy: 0.6222 - val_output_resource_accuracy: 0.4311 - val_output_timestamp_mse: 0.9922\n",
      "Epoch 3/20\n",
      "37/37 [==============================] - 30s 805ms/step - loss: 2.8767 - output_activity_loss: 0.5663 - output_resource_loss: 1.9061 - output_timestamp_loss: 0.4043 - output_activity_accuracy: 0.8034 - output_resource_accuracy: 0.5960 - output_timestamp_mse: 0.4043 - val_loss: 6.2556 - val_output_activity_loss: 1.6725 - val_output_resource_loss: 3.6554 - val_output_timestamp_loss: 0.9277 - val_output_activity_accuracy: 0.6560 - val_output_resource_accuracy: 0.5686 - val_output_timestamp_mse: 0.9277\n",
      "Epoch 4/20\n",
      "37/37 [==============================] - 30s 799ms/step - loss: 2.3766 - output_activity_loss: 0.5009 - output_resource_loss: 1.4783 - output_timestamp_loss: 0.3974 - output_activity_accuracy: 0.8099 - output_resource_accuracy: 0.6527 - output_timestamp_mse: 0.3974 - val_loss: 5.8212 - val_output_activity_loss: 1.5296 - val_output_resource_loss: 3.4204 - val_output_timestamp_loss: 0.8713 - val_output_activity_accuracy: 0.6611 - val_output_resource_accuracy: 0.6216 - val_output_timestamp_mse: 0.8713\n",
      "Epoch 5/20\n",
      "37/37 [==============================] - 32s 868ms/step - loss: 2.1528 - output_activity_loss: 0.4746 - output_resource_loss: 1.3099 - output_timestamp_loss: 0.3683 - output_activity_accuracy: 0.8113 - output_resource_accuracy: 0.6748 - output_timestamp_mse: 0.3683 - val_loss: 5.3314 - val_output_activity_loss: 1.3621 - val_output_resource_loss: 3.1529 - val_output_timestamp_loss: 0.8164 - val_output_activity_accuracy: 0.6856 - val_output_resource_accuracy: 0.6479 - val_output_timestamp_mse: 0.8164\n",
      "Epoch 6/20\n",
      "37/37 [==============================] - 28s 753ms/step - loss: 2.0310 - output_activity_loss: 0.4566 - output_resource_loss: 1.2256 - output_timestamp_loss: 0.3488 - output_activity_accuracy: 0.8145 - output_resource_accuracy: 0.6843 - output_timestamp_mse: 0.3488 - val_loss: 4.7764 - val_output_activity_loss: 1.1677 - val_output_resource_loss: 2.8559 - val_output_timestamp_loss: 0.7528 - val_output_activity_accuracy: 0.7017 - val_output_resource_accuracy: 0.6655 - val_output_timestamp_mse: 0.7528\n",
      "Epoch 7/20\n",
      "37/37 [==============================] - 27s 734ms/step - loss: 1.9587 - output_activity_loss: 0.4427 - output_resource_loss: 1.1822 - output_timestamp_loss: 0.3338 - output_activity_accuracy: 0.8168 - output_resource_accuracy: 0.6920 - output_timestamp_mse: 0.3338 - val_loss: 4.2056 - val_output_activity_loss: 0.9886 - val_output_resource_loss: 2.5386 - val_output_timestamp_loss: 0.6784 - val_output_activity_accuracy: 0.7189 - val_output_resource_accuracy: 0.6770 - val_output_timestamp_mse: 0.6784\n",
      "Epoch 8/20\n",
      "37/37 [==============================] - 30s 816ms/step - loss: 1.9136 - output_activity_loss: 0.4374 - output_resource_loss: 1.1483 - output_timestamp_loss: 0.3279 - output_activity_accuracy: 0.8173 - output_resource_accuracy: 0.6928 - output_timestamp_mse: 0.3279 - val_loss: 3.6656 - val_output_activity_loss: 0.8249 - val_output_resource_loss: 2.2154 - val_output_timestamp_loss: 0.6253 - val_output_activity_accuracy: 0.7242 - val_output_resource_accuracy: 0.6833 - val_output_timestamp_mse: 0.6253\n",
      "Epoch 9/20\n",
      "37/37 [==============================] - 26s 707ms/step - loss: 1.8841 - output_activity_loss: 0.4312 - output_resource_loss: 1.1291 - output_timestamp_loss: 0.3238 - output_activity_accuracy: 0.8186 - output_resource_accuracy: 0.6925 - output_timestamp_mse: 0.3238 - val_loss: 3.1471 - val_output_activity_loss: 0.6883 - val_output_resource_loss: 1.8949 - val_output_timestamp_loss: 0.5639 - val_output_activity_accuracy: 0.7306 - val_output_resource_accuracy: 0.6856 - val_output_timestamp_mse: 0.5639\n",
      "Epoch 10/20\n",
      "37/37 [==============================] - 28s 775ms/step - loss: 1.8494 - output_activity_loss: 0.4258 - output_resource_loss: 1.1139 - output_timestamp_loss: 0.3098 - output_activity_accuracy: 0.8163 - output_resource_accuracy: 0.6956 - output_timestamp_mse: 0.3098 - val_loss: 2.7110 - val_output_activity_loss: 0.5674 - val_output_resource_loss: 1.6207 - val_output_timestamp_loss: 0.5229 - val_output_activity_accuracy: 0.7821 - val_output_resource_accuracy: 0.6867 - val_output_timestamp_mse: 0.5229\n",
      "Epoch 11/20\n",
      "37/37 [==============================] - 24s 646ms/step - loss: 1.8266 - output_activity_loss: 0.4193 - output_resource_loss: 1.1003 - output_timestamp_loss: 0.3069 - output_activity_accuracy: 0.8195 - output_resource_accuracy: 0.6945 - output_timestamp_mse: 0.3069 - val_loss: 2.3984 - val_output_activity_loss: 0.5209 - val_output_resource_loss: 1.4160 - val_output_timestamp_loss: 0.4614 - val_output_activity_accuracy: 0.7546 - val_output_resource_accuracy: 0.6888 - val_output_timestamp_mse: 0.4614\n",
      "Epoch 12/20\n",
      "37/37 [==============================] - 29s 783ms/step - loss: 1.8181 - output_activity_loss: 0.4169 - output_resource_loss: 1.0936 - output_timestamp_loss: 0.3076 - output_activity_accuracy: 0.8197 - output_resource_accuracy: 0.6940 - output_timestamp_mse: 0.3076 - val_loss: 2.1552 - val_output_activity_loss: 0.4672 - val_output_resource_loss: 1.2682 - val_output_timestamp_loss: 0.4198 - val_output_activity_accuracy: 0.7876 - val_output_resource_accuracy: 0.6909 - val_output_timestamp_mse: 0.4198\n",
      "Epoch 13/20\n",
      "37/37 [==============================] - 30s 822ms/step - loss: 1.8066 - output_activity_loss: 0.4158 - output_resource_loss: 1.0877 - output_timestamp_loss: 0.3032 - output_activity_accuracy: 0.8204 - output_resource_accuracy: 0.6943 - output_timestamp_mse: 0.3032 - val_loss: 2.0334 - val_output_activity_loss: 0.4541 - val_output_resource_loss: 1.1985 - val_output_timestamp_loss: 0.3808 - val_output_activity_accuracy: 0.7875 - val_output_resource_accuracy: 0.6889 - val_output_timestamp_mse: 0.3808\n",
      "Epoch 14/20\n",
      "37/37 [==============================] - 25s 672ms/step - loss: 1.7969 - output_activity_loss: 0.4115 - output_resource_loss: 1.0798 - output_timestamp_loss: 0.3055 - output_activity_accuracy: 0.8223 - output_resource_accuracy: 0.6974 - output_timestamp_mse: 0.3055 - val_loss: 1.9507 - val_output_activity_loss: 0.4276 - val_output_resource_loss: 1.1320 - val_output_timestamp_loss: 0.3911 - val_output_activity_accuracy: 0.7934 - val_output_resource_accuracy: 0.6965 - val_output_timestamp_mse: 0.3911\n",
      "Epoch 15/20\n",
      "37/37 [==============================] - 26s 695ms/step - loss: 1.7949 - output_activity_loss: 0.4101 - output_resource_loss: 1.0790 - output_timestamp_loss: 0.3057 - output_activity_accuracy: 0.8193 - output_resource_accuracy: 0.6930 - output_timestamp_mse: 0.3057 - val_loss: 1.8696 - val_output_activity_loss: 0.4146 - val_output_resource_loss: 1.1059 - val_output_timestamp_loss: 0.3491 - val_output_activity_accuracy: 0.8109 - val_output_resource_accuracy: 0.6930 - val_output_timestamp_mse: 0.3491\n",
      "Epoch 16/20\n",
      "37/37 [==============================] - 30s 810ms/step - loss: 1.7857 - output_activity_loss: 0.4117 - output_resource_loss: 1.0721 - output_timestamp_loss: 0.3019 - output_activity_accuracy: 0.8191 - output_resource_accuracy: 0.6978 - output_timestamp_mse: 0.3019 - val_loss: 1.8250 - val_output_activity_loss: 0.4262 - val_output_resource_loss: 1.0916 - val_output_timestamp_loss: 0.3072 - val_output_activity_accuracy: 0.7946 - val_output_resource_accuracy: 0.6922 - val_output_timestamp_mse: 0.3072\n",
      "Epoch 17/20\n",
      "37/37 [==============================] - 24s 662ms/step - loss: 1.7752 - output_activity_loss: 0.4080 - output_resource_loss: 1.0679 - output_timestamp_loss: 0.2994 - output_activity_accuracy: 0.8215 - output_resource_accuracy: 0.6961 - output_timestamp_mse: 0.2994 - val_loss: 1.8075 - val_output_activity_loss: 0.4117 - val_output_resource_loss: 1.0816 - val_output_timestamp_loss: 0.3142 - val_output_activity_accuracy: 0.8075 - val_output_resource_accuracy: 0.6936 - val_output_timestamp_mse: 0.3142\n",
      "Epoch 18/20\n",
      "37/37 [==============================] - 23s 630ms/step - loss: 1.7645 - output_activity_loss: 0.4057 - output_resource_loss: 1.0610 - output_timestamp_loss: 0.2978 - output_activity_accuracy: 0.8225 - output_resource_accuracy: 0.6976 - output_timestamp_mse: 0.2978 - val_loss: 1.7794 - val_output_activity_loss: 0.4117 - val_output_resource_loss: 1.0809 - val_output_timestamp_loss: 0.2867 - val_output_activity_accuracy: 0.8058 - val_output_resource_accuracy: 0.6912 - val_output_timestamp_mse: 0.2867\n",
      "Epoch 19/20\n",
      "37/37 [==============================] - 28s 770ms/step - loss: 1.7538 - output_activity_loss: 0.4044 - output_resource_loss: 1.0549 - output_timestamp_loss: 0.2945 - output_activity_accuracy: 0.8224 - output_resource_accuracy: 0.6993 - output_timestamp_mse: 0.2945 - val_loss: 1.7606 - val_output_activity_loss: 0.4045 - val_output_resource_loss: 1.0778 - val_output_timestamp_loss: 0.2783 - val_output_activity_accuracy: 0.8222 - val_output_resource_accuracy: 0.6935 - val_output_timestamp_mse: 0.2783\n",
      "Epoch 20/20\n",
      "37/37 [==============================] - 24s 664ms/step - loss: 1.7545 - output_activity_loss: 0.4024 - output_resource_loss: 1.0542 - output_timestamp_loss: 0.2979 - output_activity_accuracy: 0.8227 - output_resource_accuracy: 0.6992 - output_timestamp_mse: 0.2979 - val_loss: 1.7554 - val_output_activity_loss: 0.4080 - val_output_resource_loss: 1.0760 - val_output_timestamp_loss: 0.2714 - val_output_activity_accuracy: 0.8167 - val_output_resource_accuracy: 0.6975 - val_output_timestamp_mse: 0.2714\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_activity, train_resource, train_timestamp],\n",
    "    [train_targets_activity_cat, train_targets_resource_cat, train_targets_timestamp],\n",
    "    validation_data=([test_activity, test_resource, test_timestamp], [test_targets_activity_cat, test_targets_resource_cat, test_targets_timestamp]),\n",
    "    epochs=20,\n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 6s 50ms/step - loss: 1.7554 - output_activity_loss: 0.4080 - output_resource_loss: 1.0760 - output_timestamp_loss: 0.2714 - output_activity_accuracy: 0.8167 - output_resource_accuracy: 0.6975 - output_timestamp_mse: 0.2714\n",
      "Validation Loss: 1.7553696632385254, Validation Accuracy: 0.4080291986465454\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "results = model.evaluate(\n",
    "    [test_activity, test_resource, test_timestamp],\n",
    "    [test_targets_activity_cat, test_targets_resource_cat, test_targets_timestamp],\n",
    "    batch_size=64\n",
    ")\n",
    "print(f\"Validation Loss: {results[0]}, Validation Accuracy: {results[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each event attribute, BINet's softmax layer outputs a probability distribution over possible values\n",
    "- The anomaly score for a specific attribute value v is calculated by summing all the probabilities from the softmax output that are greater than the probability assigned to v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 43s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_activity, windows_resource, windows_timestamp])\n",
    "\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_activity = predictions[0]\n",
    "predictions_resource = predictions[1]\n",
    "predictions_timestamp = predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_scores(predictions, targets):\n",
    "    scores = []\n",
    "    # Loop through each example in the predictions\n",
    "    for i in range(predictions.shape[0]):\n",
    "        actual_prob = predictions[i, targets[i]]  # Extract the probability of the true class using target index\n",
    "        # Calculate anomaly score as sum of probabilities greater than the probability of the actual value\n",
    "        anomaly_score = np.sum(predictions[i][predictions[i] > actual_prob])\n",
    "        scores.append(anomaly_score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_activity = calculate_anomaly_scores(predictions_activity, targets_activity.astype(int))\n",
    "anomaly_scores_resource = calculate_anomaly_scores(predictions_resource, targets_resource.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals):\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    max_diff = np.max(differences)\n",
    "    normalized_scores = differences / max_diff if max_diff != 0 else differences\n",
    "    \n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp)\n",
    "anomaly_scores_timestamp = anomaly_scores_timestamp.flatten()\n",
    "anomaly_scores_timestamp = anomaly_scores_timestamp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert missing scores for cases with less than 2 Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>score_resource</th>\n",
       "      <th>score_activity</th>\n",
       "      <th>score_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.175456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.565243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26224</th>\n",
       "      <td>5013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26225</th>\n",
       "      <td>5013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26226</th>\n",
       "      <td>5013</td>\n",
       "      <td>0.778130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26227</th>\n",
       "      <td>5014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26228</th>\n",
       "      <td>5014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26229 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  score_resource  score_activity  score_timestamp\n",
       "0         0        0.000000             0.0         0.011301\n",
       "1         0        0.000000             0.0         0.006965\n",
       "2         0        0.175456             0.0         0.009653\n",
       "3         0        0.565243             0.0         0.035876\n",
       "4         1        0.000000             0.0         0.011301\n",
       "...     ...             ...             ...              ...\n",
       "26224  5013        0.000000             0.0         0.020629\n",
       "26225  5013        0.000000             0.0         0.016130\n",
       "26226  5013        0.778130             0.0         0.071810\n",
       "26227  5014        0.000000             0.0         0.009156\n",
       "26228  5014        0.000000             0.0         0.016385\n",
       "\n",
       "[26229 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "score = pd.DataFrame({'case': case_indices})\n",
    "score['score_resource'] = anomaly_scores_resource\n",
    "score['score_activity'] = anomaly_scores_activity\n",
    "score['score_timestamp'] = anomaly_scores_timestamp\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the 'case' column contain all values between 0 and 5014? True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def contains_all_values(df, column, end):\n",
    "\n",
    "    # Generate the set of all values in the specified range\n",
    "    required_values = set(range(0, end + 1))\n",
    "    \n",
    "    # Get the unique values in the specified column\n",
    "    column_values = set(df[column].unique())\n",
    "    \n",
    "    # Find missing values\n",
    "    missing_values = required_values - column_values\n",
    "    \n",
    "    # Print missing values if any\n",
    "    if missing_values:\n",
    "        print(f\"Missing values: {sorted(missing_values)}\")\n",
    "    \n",
    "    # Check if all required values are in the column values\n",
    "    return required_values.issubset(column_values)\n",
    "\n",
    "end = 5014\n",
    "\n",
    "result = contains_all_values(score, 'case', end)\n",
    "print(f\"Does the 'case' column contain all values between 0 and {end}? {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold (lowest plateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_ratio(scores, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the anomaly ratio for a given threshold.\n",
    "    \"\"\"\n",
    "    return np.mean(scores > threshold)\n",
    "\n",
    "def find_plateaus(scores, epsilon=1e-4, min_plateau_length=10):\n",
    "    \"\"\"\n",
    "    Identify the lowest plateau in the anomaly ratio function and calculate the mean-centered threshold.\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)  # Convert scores to a NumPy array\n",
    "    sorted_scores = np.sort(scores)\n",
    "    \n",
    "    # Remove duplicate values\n",
    "    unique_thresholds, unique_indices = np.unique(sorted_scores, return_index=True)\n",
    "    anomaly_ratios = np.array([calculate_anomaly_ratio(scores, t) for t in unique_thresholds])\n",
    "    \n",
    "    # Calculate first and second derivatives\n",
    "    first_derivatives = np.diff(anomaly_ratios) / np.diff(unique_thresholds)\n",
    "    second_derivatives = np.diff(first_derivatives) / np.diff(unique_thresholds[:-1])\n",
    "    \n",
    "    # Identify plateaus where the first derivative is close to zero\n",
    "    plateau_indices = np.where(np.abs(first_derivatives) < epsilon)[0]\n",
    "    \n",
    "    # Group consecutive indices to identify continuous plateaus\n",
    "    grouped_plateaus = np.split(plateau_indices, np.where(np.diff(plateau_indices) != 1)[0] + 1)\n",
    "    \n",
    "    # Filter plateaus based on minimum length\n",
    "    long_plateaus = [g for g in grouped_plateaus if len(g) >= min_plateau_length]\n",
    "    \n",
    "    if long_plateaus:\n",
    "        # Take the first long plateau and find the mean threshold in this plateau\n",
    "        first_plateau = long_plateaus[0]\n",
    "        plateau_thresholds = unique_thresholds[first_plateau]\n",
    "        return np.mean(plateau_thresholds)\n",
    "    else:\n",
    "        # If no plateau is found, return a default value, e.g., the 90th percentile\n",
    "        percentile_90 = np.percentile(sorted_scores, 90)\n",
    "        if percentile_90 == 1.0:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return percentile_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_activity = find_plateaus(anomaly_scores_activity)\n",
    "threshold_resource = find_plateaus(anomaly_scores_resource)\n",
    "threshold_timestamp = find_plateaus(anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(anomaly_scores, threshold):\n",
    "    labels = [1 if score > threshold else 0 for score in anomaly_scores]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies based on the calculated anomaly scores and thresholds\n",
    "labels_resource = detect_anomalies(anomaly_scores_resource, threshold_resource)\n",
    "labels_activity = detect_anomalies(anomaly_scores_activity, threshold_activity)\n",
    "labels_timestamp = detect_anomalies(anomaly_scores_timestamp, threshold_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted_resource</th>\n",
       "      <th>predicted_activity</th>\n",
       "      <th>predicted_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26224</th>\n",
       "      <td>5013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26225</th>\n",
       "      <td>5013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26226</th>\n",
       "      <td>5013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26227</th>\n",
       "      <td>5014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26228</th>\n",
       "      <td>5014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26229 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted_resource  predicted_activity  predicted_timestamp\n",
       "0         0                   0                   0                    0\n",
       "1         0                   0                   0                    0\n",
       "2         0                   0                   0                    0\n",
       "3         0                   0                   0                    0\n",
       "4         1                   0                   0                    0\n",
       "...     ...                 ...                 ...                  ...\n",
       "26224  5013                   0                   0                    0\n",
       "26225  5013                   0                   0                    0\n",
       "26226  5013                   1                   0                    0\n",
       "26227  5014                   0                   0                    0\n",
       "26228  5014                   0                   0                    0\n",
       "\n",
       "[26229 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted_resource'] = labels_resource\n",
    "mapping['predicted_activity'] = labels_activity\n",
    "mapping['predicted_timestamp'] = labels_timestamp\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "        ...  \n",
       "5010    False\n",
       "5011    False\n",
       "5012     True\n",
       "5013     True\n",
       "5014    False\n",
       "Length: 5015, dtype: bool"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean DataFrame where each value is True if the value is 1\n",
    "contains_one = (mapping[['predicted_resource', 'predicted_activity', 'predicted_timestamp']] == 1)\n",
    "\n",
    "# Group by 'case' and check if there's at least one 'True' in any of the columns\n",
    "case_prediction = contains_one.groupby(mapping['case']).any().any(axis=1)\n",
    "case_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mert2/anaconda3/envs/jp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "aligning log, completed variants :: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:04<00:00, 41.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/Model_O.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(0)\n",
    "        else:\n",
    "            conformance_status.append(1)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Dev: 0.69\n"
     ]
    }
   ],
   "source": [
    "precision_dev = TP / (TP + FP)\n",
    "print(f\"Precision Dev: {precision_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Dev: 0.90\n"
     ]
    }
   ],
   "source": [
    "recall_dev = TP / (TP + FN)\n",
    "print(f\"Recall Dev: {recall_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision No Dev: 0.79\n"
     ]
    }
   ],
   "source": [
    "precision_no_dev = TN / (TN + FN)\n",
    "print(f\"Precision No Dev: {precision_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall No Dev: 0.48\n"
     ]
    }
   ],
   "source": [
    "recall_no_dev = TN / (TN + FP)\n",
    "print(f\"Recall No Dev: {recall_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
