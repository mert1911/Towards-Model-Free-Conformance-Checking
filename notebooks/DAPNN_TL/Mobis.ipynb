{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_11843/1672979557.py:15: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>travel_start</th>\n",
       "      <th>travel_end</th>\n",
       "      <th>case</th>\n",
       "      <th>cost</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:40:00+00:00</td>\n",
       "      <td>Travel Department</td>\n",
       "      <td>KS9688</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>2017-01-18 06:31:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>2017-11-22 06:50:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>55804</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>2017-11-22 13:06:00+00:00</td>\n",
       "      <td>Manager</td>\n",
       "      <td>AK7488</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>55805</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>2017-11-29 20:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>55806</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>2017-12-08 09:55:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>55807</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>2017-12-18 08:52:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>55808</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                activity  \\\n",
       "0                                    file travel request   \n",
       "1      check if travel request needs preliminary pric...   \n",
       "2                        decide on approval requirements   \n",
       "3                          check if booking is necessary   \n",
       "4                       check if expense documents exist   \n",
       "...                                                  ...   \n",
       "55804                      confirm travel expense report   \n",
       "55805                  decide on travel expense approval   \n",
       "55806                 send original documents to archive   \n",
       "55807                                 calculate payments   \n",
       "55808                                       pay expenses   \n",
       "\n",
       "                          start                       end               type  \\\n",
       "0     2017-01-17 11:17:00+00:00 2017-01-17 11:23:00+00:00           Employee   \n",
       "1     2017-01-17 11:23:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "2     2017-01-17 11:24:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "3     2017-01-17 11:24:00+00:00 2017-01-17 11:40:00+00:00  Travel Department   \n",
       "4     2017-01-18 05:59:00+00:00 2017-01-18 06:31:00+00:00           Employee   \n",
       "...                         ...                       ...                ...   \n",
       "55804 2017-11-22 06:48:00+00:00 2017-11-22 06:50:00+00:00           Employee   \n",
       "55805 2017-11-22 12:59:00+00:00 2017-11-22 13:06:00+00:00            Manager   \n",
       "55806 2017-11-29 20:12:00+00:00 2017-11-29 20:24:00+00:00           Employee   \n",
       "55807 2017-12-08 09:32:00+00:00 2017-12-08 09:55:00+00:00         Accounting   \n",
       "55808 2017-12-18 08:39:00+00:00 2017-12-18 08:52:00+00:00         Accounting   \n",
       "\n",
       "         user              travel_start                travel_end  case  cost  \\\n",
       "0      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "1      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "2      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "3      KS9688 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "4      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "...       ...                       ...                       ...   ...   ...   \n",
       "55804  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55805  AK7488 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55806  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55807  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55808  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "\n",
       "      case:concept:name                                       concept:name  \\\n",
       "0                   105                                file travel request   \n",
       "1                   105  check if travel request needs preliminary pric...   \n",
       "2                   105                    decide on approval requirements   \n",
       "3                   105                      check if booking is necessary   \n",
       "4                   105                   check if expense documents exist   \n",
       "...                 ...                                                ...   \n",
       "55804              6348                      confirm travel expense report   \n",
       "55805              6348                  decide on travel expense approval   \n",
       "55806              6348                 send original documents to archive   \n",
       "55807              6348                                 calculate payments   \n",
       "55808              6348                                       pay expenses   \n",
       "\n",
       "                 time:timestamp  @@index  @@case_index  \n",
       "0     2017-01-17 11:17:00+00:00        0             0  \n",
       "1     2017-01-17 11:23:00+00:00        1             0  \n",
       "2     2017-01-17 11:24:00+00:00        2             0  \n",
       "3     2017-01-17 11:24:00+00:00        3             0  \n",
       "4     2017-01-18 05:59:00+00:00        4             0  \n",
       "...                         ...      ...           ...  \n",
       "55804 2017-11-22 06:48:00+00:00    55804          3353  \n",
       "55805 2017-11-22 12:59:00+00:00    55805          3353  \n",
       "55806 2017-11-29 20:12:00+00:00    55806          3353  \n",
       "55807 2017-12-08 09:32:00+00:00    55807          3353  \n",
       "55808 2017-12-18 08:39:00+00:00    55808          3353  \n",
       "\n",
       "[55809 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/mobis.csv', sep=',')  \n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log, \n",
    "        case_id='case', \n",
    "        activity_key='activity', \n",
    "        timestamp_key='start'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)\n",
    "    \n",
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Start & End markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert start and end markers\n",
    "def add_markers(df):\n",
    "    # Identify unique case indices\n",
    "    case_indices = df['@@case_index'].unique()\n",
    "    \n",
    "    # Prepare a container for new DataFrame rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each case index to add start and end markers\n",
    "    for case_index in case_indices:\n",
    "        # Create a start marker row with all columns except @@case_index set to 'Start'\n",
    "        start_row = {col: 'Start' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Create an end marker row with all columns except @@case_index set to 'End'\n",
    "        end_row = {col: 'End' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Append start row, rows for the current case, and end row\n",
    "        new_rows.append(start_row)\n",
    "        new_rows.extend(df[df['@@case_index'] == case_index].to_dict('records'))\n",
    "        new_rows.append(end_row)\n",
    "    \n",
    "    # Convert the list of rows into a DataFrame\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Apply the function to add start and end markers to the dataframe\n",
    "modified_dataframe = add_markers(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['activity'])\n",
    "modified_dataframe['activity'] = codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for Cases with less then 5 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each unique value in '@@case_index'\n",
    "frequency = modified_dataframe['@@case_index'].value_counts()\n",
    "\n",
    "# Finding the minimum occurrence\n",
    "min_occurrence = frequency.min()\n",
    "\n",
    "min_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Assuming your dataframe might have different types, let\\'s create a generic function to add rows\\ndef add_rows(group, num_rows, case_index_value):\\n    # For each column, determine the appropriate \"zero\" value (int 0, string \\'\\', etc.)\\n    additional_rows = pd.DataFrame({\\n        column: 0 if pd.api.types.is_numeric_dtype(group[column]) else \\'\\' for column in group.columns\\n    }, index=range(num_rows))\\n    \\n    # Set the @@case_index column to the current case index value\\n    additional_rows[\\'@@case_index\\'] = case_index_value\\n    \\n    # Append the additional rows to the group\\n    return pd.concat([group, additional_rows], ignore_index=True)\\n\\n# Function to pad cases with less than 5 events\\ndef pad_cases(df):\\n    # Group by @@case_index\\n    groups = df.groupby(\\'@@case_index\\')\\n    \\n    # Placeholder for modified groups\\n    modified_groups = []\\n    \\n    for name, group in groups:\\n        # Calculate the number of events to add\\n        events_to_add = 5 - len(group)\\n        \\n        if events_to_add > 0:\\n            # Add the required number of rows\\n            group = add_rows(group, events_to_add, name)\\n        \\n        # Append the modified group to the list\\n        modified_groups.append(group)\\n    \\n    # Concatenate all modified groups back into a single DataFrame\\n    return pd.concat(modified_groups, ignore_index=True)  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Assuming your dataframe might have different types, let's create a generic function to add rows\n",
    "def add_rows(group, num_rows, case_index_value):\n",
    "    # For each column, determine the appropriate \"zero\" value (int 0, string '', etc.)\n",
    "    additional_rows = pd.DataFrame({\n",
    "        column: 0 if pd.api.types.is_numeric_dtype(group[column]) else '' for column in group.columns\n",
    "    }, index=range(num_rows))\n",
    "    \n",
    "    # Set the @@case_index column to the current case index value\n",
    "    additional_rows['@@case_index'] = case_index_value\n",
    "    \n",
    "    # Append the additional rows to the group\n",
    "    return pd.concat([group, additional_rows], ignore_index=True)\n",
    "\n",
    "# Function to pad cases with less than 5 events\n",
    "def pad_cases(df):\n",
    "    # Group by @@case_index\n",
    "    groups = df.groupby('@@case_index')\n",
    "    \n",
    "    # Placeholder for modified groups\n",
    "    modified_groups = []\n",
    "    \n",
    "    for name, group in groups:\n",
    "        # Calculate the number of events to add\n",
    "        events_to_add = 5 - len(group)\n",
    "        \n",
    "        if events_to_add > 0:\n",
    "            # Add the required number of rows\n",
    "            group = add_rows(group, events_to_add, name)\n",
    "        \n",
    "        # Append the modified group to the list\n",
    "        modified_groups.append(group)\n",
    "    \n",
    "    # Concatenate all modified groups back into a single DataFrame\n",
    "    return pd.concat(modified_groups, ignore_index=True)  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Apply the padding function\\nmodified_dataframe = pad_cases(modified_dataframe)\\nmodified_dataframe '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Apply the padding function\n",
    "modified_dataframe = pad_cases(modified_dataframe)\n",
    "modified_dataframe \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity = modified_dataframe[['activity', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df, case_id_column='@@case_index', window_size=5):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    # Iterate over each unique case\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        # Extract the case\n",
    "        case_data = df[df[case_id_column] == case_id]\n",
    "        \n",
    "        # Convert case_data to a NumPy array and drop the case_id_column\n",
    "        case_data_array = case_data.drop(columns=[case_id_column]).to_numpy()\n",
    "\n",
    "        # Adjusting the condition to correctly reflect window_size without needing an additional +1\n",
    "        # Now it correctly considers window_size as including the target event\n",
    "        if len(case_data_array) >= window_size:\n",
    "            # Adjust the loop to generate sliding windows of size window_size - 1 for the inputs and use the next event as the target\n",
    "            for i in range(len(case_data_array) - window_size + 1):\n",
    "                # window now has window_size - 1 events\n",
    "                window = case_data_array[i:i + window_size - 1]\n",
    "                # The target is the event immediately following the window\n",
    "                target = case_data_array[i + window_size - 1]\n",
    "                windows.append(window)\n",
    "                targets.append(target)\n",
    "                case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling and to ensure they are two-dimensional\n",
    "    windows_array = np.array(windows)\n",
    "    targets_array = np.array(targets)\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_array, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_activity, targets_activity, case_indices = generate_sliding_windows(df_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 21:53:58.143129: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_activity (InputLayer  [(None, 4)]               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 4, 50)             1400      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25)                7600      \n",
      "                                                                 \n",
      " output_activity (Dense)     (None, 28)                728       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9728 (38.00 KB)\n",
      "Trainable params: 9728 (38.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Assuming these values as placeholders, replace them with actual counts from your data\n",
    "num_activities = modified_dataframe['concept:name'].nunique()\n",
    "\n",
    "embedding_dim_activity = 50\n",
    "time_steps = 4\n",
    "\n",
    "# Input layer for activities\n",
    "input_activity = Input(shape=(time_steps,), name='input_activity')\n",
    "\n",
    "# Embedding layer for activities\n",
    "embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim_activity, input_length=time_steps)(input_activity)\n",
    "\n",
    "# LSTM layer for activities\n",
    "lstm_activity = LSTM(25, return_sequences=False)(embedding_activity)\n",
    "\n",
    "# Output layer for predicting the next activity\n",
    "output_activity = Dense(num_activities, activation='softmax', name='output_activity')(lstm_activity)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=input_activity, outputs=output_activity)\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the activity data\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def cyclic_lr(epoch, lr):\n",
    "    # Example function that modulates LR within a range for each epoch\n",
    "    # Customize this function based on your cyclic learning rate policy\n",
    "    max_lr = 0.01  # Maximum LR\n",
    "    base_lr = 0.001  # Base LR\n",
    "    step_size = 10  # Number of epochs for half a cycle\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "538/538 [==============================] - 4s 4ms/step - loss: 1.2329 - accuracy: 0.6959 - val_loss: 0.3810 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.3034 - accuracy: 0.8802 - val_loss: 0.2647 - val_accuracy: 0.8917 - lr: 0.0019\n",
      "Epoch 3/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2647 - accuracy: 0.8851 - val_loss: 0.2524 - val_accuracy: 0.8926 - lr: 0.0028\n",
      "Epoch 4/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2570 - accuracy: 0.8848 - val_loss: 0.2499 - val_accuracy: 0.8885 - lr: 0.0037\n",
      "Epoch 5/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2520 - accuracy: 0.8855 - val_loss: 0.2422 - val_accuracy: 0.8931 - lr: 0.0046\n",
      "Epoch 6/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2509 - accuracy: 0.8859 - val_loss: 0.2466 - val_accuracy: 0.8935 - lr: 0.0055\n",
      "Epoch 7/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2512 - accuracy: 0.8860 - val_loss: 0.2443 - val_accuracy: 0.8936 - lr: 0.0064\n",
      "Epoch 8/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2520 - accuracy: 0.8824 - val_loss: 0.2488 - val_accuracy: 0.8888 - lr: 0.0073\n",
      "Epoch 9/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2500 - accuracy: 0.8858 - val_loss: 0.2537 - val_accuracy: 0.8507 - lr: 0.0082\n",
      "Epoch 10/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2494 - accuracy: 0.8849 - val_loss: 0.2501 - val_accuracy: 0.8940 - lr: 0.0091\n",
      "Epoch 11/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2489 - accuracy: 0.8850 - val_loss: 0.2469 - val_accuracy: 0.8933 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2487 - accuracy: 0.8864 - val_loss: 0.2423 - val_accuracy: 0.8937 - lr: 0.0091\n",
      "Epoch 13/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2470 - accuracy: 0.8873 - val_loss: 0.2433 - val_accuracy: 0.8883 - lr: 0.0082\n",
      "Epoch 14/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2477 - accuracy: 0.8851 - val_loss: 0.2438 - val_accuracy: 0.8941 - lr: 0.0073\n",
      "Epoch 15/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2450 - accuracy: 0.8862 - val_loss: 0.2441 - val_accuracy: 0.8938 - lr: 0.0064\n",
      "Epoch 16/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2440 - accuracy: 0.8871 - val_loss: 0.2384 - val_accuracy: 0.8884 - lr: 0.0055\n",
      "Epoch 17/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2423 - accuracy: 0.8873 - val_loss: 0.2377 - val_accuracy: 0.8891 - lr: 0.0046\n",
      "Epoch 18/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2420 - accuracy: 0.8882 - val_loss: 0.2386 - val_accuracy: 0.8940 - lr: 0.0037\n",
      "Epoch 19/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2413 - accuracy: 0.8878 - val_loss: 0.2382 - val_accuracy: 0.8940 - lr: 0.0028\n",
      "Epoch 20/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2403 - accuracy: 0.8884 - val_loss: 0.2375 - val_accuracy: 0.8940 - lr: 0.0019\n",
      "Epoch 21/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2393 - accuracy: 0.8894 - val_loss: 0.2362 - val_accuracy: 0.8940 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2400 - accuracy: 0.8892 - val_loss: 0.2365 - val_accuracy: 0.8938 - lr: 0.0019\n",
      "Epoch 23/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2408 - accuracy: 0.8884 - val_loss: 0.2376 - val_accuracy: 0.8889 - lr: 0.0028\n",
      "Epoch 24/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2415 - accuracy: 0.8887 - val_loss: 0.2395 - val_accuracy: 0.8891 - lr: 0.0037\n",
      "Epoch 25/25\n",
      "538/538 [==============================] - 2s 3ms/step - loss: 0.2422 - accuracy: 0.8873 - val_loss: 0.2379 - val_accuracy: 0.8938 - lr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert activity targets to categorical\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_activity, \n",
    "                    train_targets_activity_cat,\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(test_activity, test_targets_activity_cat),\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_scheduler])  # Add other callbacks as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461/461 [==============================] - 1s 1ms/step - loss: 0.2379 - accuracy: 0.8938\n",
      "[0.2379106730222702, 0.893829345703125]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using only the activity-related data\n",
    "evaluation = model.evaluate(\n",
    "    test_activity,\n",
    "    test_targets_activity_cat\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535/1535 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions_activity = model.predict([windows_activity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute Anomaly Scores for categorical variables (Resources and Activities)\n",
    "\n",
    "def compute_anomaly_scores(predictions, actuals):\n",
    "    # For categorical predictions, convert actuals to one-hot for comparison\n",
    "    actuals_one_hot = to_categorical(actuals, num_classes=predictions.shape[-1])\n",
    "    \n",
    "    max_predictions = np.max(predictions, axis=-1)\n",
    "    actual_predictions = np.sum(predictions * actuals_one_hot, axis=-1)  # Extract the probability of the actual class\n",
    "    \n",
    "    anomaly_scores = (max_predictions - actual_predictions) / max_predictions\n",
    "    \n",
    "    return anomaly_scores\n",
    "\n",
    "anomaly_scores_activity = compute_anomaly_scores(predictions_activity, targets_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_activity, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_activity])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for user, activity, type, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True: anomaly, False: no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49096</th>\n",
       "      <td>3353</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49097</th>\n",
       "      <td>3353</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49098</th>\n",
       "      <td>3353</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49099</th>\n",
       "      <td>3353</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49100</th>\n",
       "      <td>3353</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49101 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted\n",
       "0         0      False\n",
       "1         0      False\n",
       "2         0      False\n",
       "3         0      False\n",
       "4         0      False\n",
       "...     ...        ...\n",
       "49096  3353      False\n",
       "49097  3353      False\n",
       "49098  3353      False\n",
       "49099  3353      False\n",
       "49100  3353      False\n",
       "\n",
       "[49101 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted'] = anomalous_cases\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "3349    False\n",
       "3350    False\n",
       "3351    False\n",
       "3352    False\n",
       "3353    False\n",
       "Name: predicted, Length: 3354, dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_prediction = mapping.groupby('case')['predicted'].any()\n",
    "case_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606c38156eeb46979bf7b957c17bded4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/MobisToBe.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(0)\n",
    "        else:\n",
    "            conformance_status.append(1)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3354 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              0          0\n",
       "1              1          0\n",
       "2              0          0\n",
       "3              1          0\n",
       "4              1          0\n",
       "...          ...        ...\n",
       "3349           0          0\n",
       "3350           1          0\n",
       "3351           0          0\n",
       "3352           0          0\n",
       "3353           0          0\n",
       "\n",
       "[3354 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Dev: 1.00\n"
     ]
    }
   ],
   "source": [
    "precision_dev = TP / (TP + FP)\n",
    "print(f\"Precision Dev: {precision_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Dev: 0.07\n"
     ]
    }
   ],
   "source": [
    "recall_dev = TP / (TP + FN)\n",
    "print(f\"Recall Dev: {recall_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision No Dev: 0.52\n"
     ]
    }
   ],
   "source": [
    "precision_no_dev = TN / (TN + FN)\n",
    "print(f\"Precision No Dev: {precision_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall No Dev: 1.00\n"
     ]
    }
   ],
   "source": [
    "recall_no_dev = TN / (TN + FP)\n",
    "print(f\"Recall No Dev: {recall_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
