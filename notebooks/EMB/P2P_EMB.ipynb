{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f1af5c",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab9ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_6609/2650004453.py:11: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv('../../data/logs/p2p_log.csv', sep=',')\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe = pm4py.format_dataframe(\n",
    "        dataframe,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba199329",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e8a99",
   "metadata": {},
   "source": [
    "## Integer Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9269f1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept:name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:label</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>activity_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01 00:00:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>Jamika</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Purchase SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01 00:01:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>Paul</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Approve SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01 00:02:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>Hugo</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Create PO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01 00:04:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>Ronald</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Release PO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01 00:05:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>Deloras</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43188</th>\n",
       "      <td>Approve PO 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-09-01 15:21:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>999</td>\n",
       "      <td>Doretta</td>\n",
       "      <td>43188</td>\n",
       "      <td>4999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43189</th>\n",
       "      <td>Release PO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-09-01 15:23:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>999</td>\n",
       "      <td>Deloras</td>\n",
       "      <td>43189</td>\n",
       "      <td>4999</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43190</th>\n",
       "      <td>Post GR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-09-01 15:24:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>999</td>\n",
       "      <td>Sherell</td>\n",
       "      <td>43190</td>\n",
       "      <td>4999</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43191</th>\n",
       "      <td>Post IR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-09-01 15:25:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>999</td>\n",
       "      <td>Alyce</td>\n",
       "      <td>43191</td>\n",
       "      <td>4999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43192</th>\n",
       "      <td>Pay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-09-01 15:27:00+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>999</td>\n",
       "      <td>Lannie</td>\n",
       "      <td>43192</td>\n",
       "      <td>4999</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43193 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       concept:name  timestamp            time:timestamp case:label  \\\n",
       "0         Create SC        NaN 2010-01-01 00:00:00+00:00     normal   \n",
       "1       Purchase SC        NaN 2010-01-01 00:01:00+00:00     normal   \n",
       "2        Approve SC        NaN 2010-01-01 00:02:00+00:00     normal   \n",
       "3         Create PO        NaN 2010-01-01 00:04:00+00:00     normal   \n",
       "4        Release PO        NaN 2010-01-01 00:05:00+00:00     normal   \n",
       "...             ...        ...                       ...        ...   \n",
       "43188  Approve PO 1        NaN 2010-09-01 15:21:00+00:00     normal   \n",
       "43189    Release PO        NaN 2010-09-01 15:23:00+00:00     normal   \n",
       "43190       Post GR        NaN 2010-09-01 15:24:00+00:00     normal   \n",
       "43191       Post IR        NaN 2010-09-01 15:25:00+00:00     normal   \n",
       "43192           Pay        NaN 2010-09-01 15:27:00+00:00     normal   \n",
       "\n",
       "      case:concept:name org:resource  @@index  @@case_index  activity_encoded  \n",
       "0                     1       Jamika        0             0                 6  \n",
       "1                     1         Paul        1             0                10  \n",
       "2                     1         Hugo        2             0                 3  \n",
       "3                     1       Ronald        3             0                 4  \n",
       "4                     1      Deloras        4             0                23  \n",
       "...                 ...          ...      ...           ...               ...  \n",
       "43188               999      Doretta    43188          4999                 0  \n",
       "43189               999      Deloras    43189          4999                23  \n",
       "43190               999      Sherell    43190          4999                 8  \n",
       "43191               999        Alyce    43191          4999                 9  \n",
       "43192               999       Lannie    43192          4999                 7  \n",
       "\n",
       "[43193 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "dataframe['activity_encoded'] = label_encoder.fit_transform(dataframe['concept:name'])\n",
    "\n",
    "# Now the 'dataframe' has a new column 'activity_encoded' with integer encoded values of the 'activity' column\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548730de",
   "metadata": {},
   "source": [
    "# Build Input X and output Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7b0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 21:23:20.795971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_prefix_windows(df, case_id_column='@@case_index', max_len=None):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    \n",
    "    for case_id in df[case_id_column].unique():\n",
    "        case_data = df[df[case_id_column] == case_id].drop(columns=[case_id_column]).to_numpy()\n",
    "        \n",
    "        # Optional: Make sure to sort the case data if there's an implicit order (e.g., by timestamps)\n",
    "        # case_data = case_data.sort_values(by='timestamp_column').to_numpy()  # Uncomment and adjust if needed\n",
    "        \n",
    "        for i in range(1, len(case_data)):\n",
    "            window = case_data[:i]\n",
    "            target = case_data[i]\n",
    "            windows.append(window)\n",
    "            targets.append(target)\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = max(len(window) for window in windows)\n",
    "    \n",
    "    # Pad sequences\n",
    "    windows_padded = pad_sequences(windows, maxlen=max_len, padding='post', dtype='float32')\n",
    "    \n",
    "    # Convert targets to numpy array\n",
    "    targets_array = np.array(targets, dtype='float32')\n",
    "    \n",
    "    return windows_padded, targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40b22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = dataframe[['activity_encoded', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2903a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_prefix_windows(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a83570",
   "metadata": {},
   "source": [
    "# LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8eb173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 25)          625       \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, None, 100),       50400     \n",
      "                              (None, 100),                       \n",
      "                              (None, 100)]                       \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 25)                2525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53550 (209.18 KB)\n",
      "Trainable params: 53550 (209.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Lambda\n",
    "\n",
    "# Define parameters\n",
    "input_dim = dataframe['concept:name'].nunique()  # Number of unique activities\n",
    "embedding_dim = len(dataframe['concept:name'].unique())  # Embedding dimension\n",
    "lstm_units = 100  # Number of units in the LSTM layer\n",
    "dropout_rate = 0.2  # Dropout rate to prevent overfitting\n",
    "output_dim = input_dim  # Output dimension (number of activities)\n",
    "\n",
    "# Define the input layer\n",
    "inputs = Input(shape=(None,))\n",
    "\n",
    "# Embedding layer\n",
    "embedding = Embedding(input_dim=input_dim, output_dim=embedding_dim)(inputs)\n",
    "\n",
    "# LSTM layer with return_sequences=True to get hidden states at each time step\n",
    "lstm_output, lstm_state_h, lstm_state_c = LSTM(units=lstm_units, return_sequences=True, return_state=True)(embedding)\n",
    "\n",
    "# Apply a Lambda layer to extract the last hidden state\n",
    "last_hidden_state = Lambda(lambda x: x[:, -1, :])(lstm_output)\n",
    "\n",
    "# Dropout layer\n",
    "dropout = Dropout(rate=dropout_rate)(last_hidden_state)\n",
    "\n",
    "# Dense output layer for next activity prediction\n",
    "outputs = Dense(units=output_dim, activation='softmax')(dropout)\n",
    "\n",
    "# Build the model for training (only outputs the prediction)\n",
    "training_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the training model\n",
    "training_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model for inference (outputs both predictions and hidden states)\n",
    "inference_model = Model(inputs=inputs, outputs=[outputs, lstm_output])\n",
    "\n",
    "# Display the model architecture\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66982278",
   "metadata": {},
   "source": [
    "# LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a8c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(Y, num_classes=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa90533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "77/77 [==============================] - 6s 45ms/step - loss: 2.4909 - accuracy: 0.1402\n",
      "Epoch 2/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5217 - accuracy: 0.5209\n",
      "Epoch 3/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.8232 - accuracy: 0.7984\n",
      "Epoch 4/25\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.6781 - accuracy: 0.8247\n",
      "Epoch 5/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.6232 - accuracy: 0.8349\n",
      "Epoch 6/25\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.6178 - accuracy: 0.8342\n",
      "Epoch 7/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5984 - accuracy: 0.8373\n",
      "Epoch 8/25\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.5610 - accuracy: 0.8450\n",
      "Epoch 9/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5475 - accuracy: 0.8476\n",
      "Epoch 10/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5391 - accuracy: 0.8486\n",
      "Epoch 11/25\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.5398 - accuracy: 0.8485\n",
      "Epoch 12/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5289 - accuracy: 0.8510\n",
      "Epoch 13/25\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.5244 - accuracy: 0.8512\n",
      "Epoch 14/25\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.5210 - accuracy: 0.8531\n",
      "Epoch 15/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5196 - accuracy: 0.8520\n",
      "Epoch 16/25\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.5155 - accuracy: 0.8531\n",
      "Epoch 17/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5086 - accuracy: 0.8542\n",
      "Epoch 18/25\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.5034 - accuracy: 0.8553\n",
      "Epoch 19/25\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.5031 - accuracy: 0.8553\n",
      "Epoch 20/25\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.4999 - accuracy: 0.8555\n",
      "Epoch 21/25\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.4966 - accuracy: 0.8560\n",
      "Epoch 22/25\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.4917 - accuracy: 0.8577\n",
      "Epoch 23/25\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.4884 - accuracy: 0.8574\n",
      "Epoch 24/25\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.4864 - accuracy: 0.8582\n",
      "Epoch 25/25\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.4848 - accuracy: 0.8586\n"
     ]
    }
   ],
   "source": [
    "history = training_model.fit(X, Y, epochs=25, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3b50f",
   "metadata": {},
   "source": [
    "# Extract Trace Representations Using LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e76f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194/1194 [==============================] - 3s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions, hidden_states = inference_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f445c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@@case_index\n",
       "0       [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "1       [Create PR, Release PR, Create PO, Release PO,...\n",
       "2       [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "3       [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "4       [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "                              ...                        \n",
       "4995    [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "4996    [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "4997    [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "4998    [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "4999    [Create SC, Purchase SC, Approve SC, Create PO...\n",
       "Name: concept:name, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by case index to get individual traces\n",
    "traces = dataframe.groupby('@@case_index')['concept:name'].apply(list)\n",
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e58c33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 8, ..., 7, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state_pos = traces.apply(len).values\n",
    "hidden_state_pos = hidden_state_pos - 1\n",
    "hidden_state_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8934670f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_representations = [hidden_states[i-1] for i in hidden_state_pos]\n",
    "trace_representations = np.array(trace_representations)\n",
    "len(trace_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a35e592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 13, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419acf27",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8791f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c9a48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1c97aae8c542c084290e278106109d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/p2p.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b508ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(0)\n",
    "        else:\n",
    "            conformance_status.append(1)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "326adc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = pd.DataFrame(conformance, columns=['actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171bf1e",
   "metadata": {},
   "source": [
    "# Input Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4fde334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group by case_index and concatenate the activities to form traces\n",
    "dataframe['trace'] = dataframe.groupby('@@case_index')['concept:name'].transform(lambda x: ', '.join(x))\n",
    "\n",
    "# Step 3: Count occurrences of each unique trace\n",
    "trace_counts = dataframe['trace'].value_counts()\n",
    "\n",
    "# Step 4: Convert to DataFrame and sort by occurrences\n",
    "trace_counts_df = trace_counts.reset_index()\n",
    "trace_counts_df.columns = ['Trace', 'Count']\n",
    "trace_counts_df = trace_counts_df.sort_values(by='Count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e723c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_traces = trace_counts_df.head(20)\n",
    "\n",
    "trace1 = top_3_traces.iloc[0]['Trace']\n",
    "trace1 = [event.strip() for event in trace1.split(',')]\n",
    "\n",
    "trace2 = top_3_traces.iloc[1]['Trace']\n",
    "trace2 = [event.strip() for event in trace2.split(',')]\n",
    "\n",
    "trace3 = top_3_traces.iloc[2]['Trace']\n",
    "trace3 = [event.strip() for event in trace3.split(',')]\n",
    "\n",
    "trace4 = top_3_traces.iloc[3]['Trace']\n",
    "trace4 = [event.strip() for event in trace4.split(',')]\n",
    "\n",
    "trace5 = top_3_traces.iloc[4]['Trace']\n",
    "trace5 = [event.strip() for event in trace5.split(',')]\n",
    "\n",
    "trace6 = top_3_traces.iloc[5]['Trace']\n",
    "trace6 = [event.strip() for event in trace6.split(',')]\n",
    "\n",
    "trace7 = top_3_traces.iloc[6]['Trace']\n",
    "trace7 = [event.strip() for event in trace7.split(',')]\n",
    "\n",
    "trace8 = top_3_traces.iloc[7]['Trace']\n",
    "trace8 = [event.strip() for event in trace8.split(',')]\n",
    "\n",
    "trace9 = top_3_traces.iloc[8]['Trace']\n",
    "trace9 = [event.strip() for event in trace9.split(',')]\n",
    "\n",
    "trace10 = top_3_traces.iloc[9]['Trace']\n",
    "trace10 = [event.strip() for event in trace10.split(',')]\n",
    "\n",
    "trace11 = top_3_traces.iloc[10]['Trace']\n",
    "trace11 = [event.strip() for event in trace11.split(',')]\n",
    "\n",
    "trace12 = top_3_traces.iloc[11]['Trace']\n",
    "trace12 = [event.strip() for event in trace12.split(',')]\n",
    "\n",
    "trace13 = top_3_traces.iloc[12]['Trace']\n",
    "trace13 = [event.strip() for event in trace13.split(',')]\n",
    "\n",
    "trace14 = top_3_traces.iloc[13]['Trace']\n",
    "trace14 = [event.strip() for event in trace14.split(',')]\n",
    "\n",
    "trace15 = top_3_traces.iloc[14]['Trace']\n",
    "trace15 = [event.strip() for event in trace15.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee60b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = dataframe.groupby('@@case_index')['concept:name'].apply(list).reset_index(name='trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3fc4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_happy_trace(row_trace):\n",
    "    predefined_traces = [trace1, trace2, trace3]\n",
    "    for trace in predefined_traces:\n",
    "        if row_trace == trace:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e286ba1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>trace</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Create PR, Release PR, Create PO, Release PO,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4995</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4996</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4997</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4998</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4999</td>\n",
       "      <td>[Create SC, Purchase SC, Approve SC, Create PO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      @@case_index                                              trace  happy\n",
       "0                0  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "1                1  [Create PR, Release PR, Create PO, Release PO,...      0\n",
       "2                2  [Create SC, Purchase SC, Approve SC, Create PO...      0\n",
       "3                3  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "4                4  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "...            ...                                                ...    ...\n",
       "4995          4995  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "4996          4996  [Create SC, Purchase SC, Approve SC, Create PO...      0\n",
       "4997          4997  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "4998          4998  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "4999          4999  [Create SC, Purchase SC, Approve SC, Create PO...      1\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped['happy'] = grouped['trace'].apply(is_happy_trace)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9521ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of the happy traces in the results dataframe\n",
    "happy_trace_indices = grouped[grouped['happy'] == 1].index.tolist()\n",
    "\n",
    "# Extract the corresponding coordinates from the trace_representations array\n",
    "happy_trace_coordinates = trace_representations[happy_trace_indices]\n",
    "\n",
    "# Extract unique coordinates\n",
    "unique_happy_trace_coordinates = np.unique(happy_trace_coordinates, axis=0)\n",
    "\n",
    "# Assuming the size of unique_happy_trace_coordinates is 3\n",
    "#happy_trace1, happy_trace2, happy_trace3 = unique_happy_trace_coordinates\n",
    "happy_trace1, happy_trace2 = unique_happy_trace_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb704ff",
   "metadata": {},
   "source": [
    "# Distance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ee5f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each trace representation (7, 100) into a 1D vector of size 700\n",
    "trace_representations = trace_representations.reshape(trace_representations.shape[0], -1)\n",
    "\n",
    "# Now flattened_trace_representations will have shape (13087, 700)\n",
    "happy_trace1 = happy_trace1.flatten()\n",
    "happy_trace2 = happy_trace2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cd15eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_trace1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71645897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_representations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce6f9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Calculate the distances to each of the happy traces for every trace representation\n",
    "distances_to_happy_traces = []\n",
    "\n",
    "for trace_representation in trace_representations:\n",
    "    distances = [\n",
    "        euclidean(trace_representation, happy_trace1),\n",
    "        euclidean(trace_representation, happy_trace2)\n",
    "    ]\n",
    "    distances_to_happy_traces.append(distances)\n",
    "\n",
    "# Calculate the average distance to the happy traces for each trace representation\n",
    "avg_distances = [np.mean(distances) for distances in distances_to_happy_traces]\n",
    "\n",
    "# Save the distances in a variable\n",
    "avg_distances_var = np.array(avg_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5910444",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification['distance'] = avg_distances_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba7067",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "364bd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Threshold\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Filter the DataFrame into conforming and non-conforming subsets\n",
    "conforming_distances = classification[classification['actual'] == 0]['distance']\n",
    "non_conforming_distances = classification[classification['actual'] == 1]['distance']\n",
    "\n",
    "# Determine common bin edges\n",
    "min_distance = min(classification['distance'])\n",
    "max_distance = max(classification['distance'])\n",
    "bin_edges = np.linspace(min_distance, max_distance, num=30)\n",
    "\n",
    "# Combine the data and reshape for k-means\n",
    "all_distances = avg_distances_var\n",
    "all_distances = np.array(all_distances)\n",
    "all_distances_reshaped = all_distances.reshape(-1, 1)\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(all_distances_reshaped)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Find the threshold as the average of the two cluster centers\n",
    "threshold_value = np.mean(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2052aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'classification' is a DataFrame and 'threshold_value' is already defined\n",
    "classification['predicted'] = classification['distance'].apply(lambda x: 1 if x > threshold_value else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fd070e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((classification['actual'] == 1) & (classification['predicted'] == 1)).sum()\n",
    "TN = ((classification['actual'] == 0) & (classification['predicted'] == 0)).sum()\n",
    "FP = ((classification['actual'] == 0) & (classification['predicted'] == 1)).sum()\n",
    "FN = ((classification['actual'] == 1) & (classification['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3f905e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Dev: 0.55\n"
     ]
    }
   ],
   "source": [
    "precision_dev = TP / (TP + FP)\n",
    "print(f\"Precision Dev: {precision_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff76d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Dev: 0.53\n"
     ]
    }
   ],
   "source": [
    "recall_dev = TP / (TP + FN)\n",
    "print(f\"Recall Dev: {recall_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2be910ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision No Dev: 0.85\n"
     ]
    }
   ],
   "source": [
    "precision_no_d = TN / (TN + FN)\n",
    "print(f\"Precision No Dev: {precision_no_d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e20e5e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall No Dev: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall\n",
    "recall_no_d = TN / (TN + FP)\n",
    "print(f\"Recall No Dev: {recall_no_d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f80181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_roc = roc_auc_score(classification['actual'], classification['predicted'])\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfe70c",
   "metadata": {},
   "source": [
    "# Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79372a48",
   "metadata": {},
   "source": [
    "### a) identify closest trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b71b0f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979c1b599a7f4721845f5cafc1793de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INPUT TRACE 1\n",
    "\n",
    "\n",
    "\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/input_traces/p2p_trace1.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_input_trace1 = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bef8e8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248ec0446f3c4dd29eb4a8ee69626bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INPUT TRACE 2\n",
    "\n",
    "\n",
    "\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/input_traces/p2p_trace2.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_input_trace2 = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41e779f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e6252c24e94f1a8fe326b5467606ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INPUT TRACE 3\n",
    "\n",
    "\n",
    "\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/input_traces/p2p_trace3.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_input_trace3 = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51efe467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance\n",
    "def euclidean_distance(arr1, arr2):\n",
    "    return np.linalg.norm(arr1 - arr2)\n",
    "\n",
    "# Prepare a list to store the results\n",
    "distance = []\n",
    "\n",
    "# Iterate through each subarray in one_hot_encoding\n",
    "for subarray in trace_representations:\n",
    "    dist_to_trace1 = euclidean_distance(subarray, happy_trace1)\n",
    "    \n",
    "    distances = [dist_to_trace1]\n",
    "    closest_trace_index = np.argmin(distances)\n",
    "    closest_trace = f'trace_{closest_trace_index + 1}'\n",
    "    \n",
    "    distance.append({\n",
    "        'Distance to Trace 1': dist_to_trace1,\n",
    "        'Closest Trace': closest_trace\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "closest_distance = pd.DataFrame(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1f415",
   "metadata": {},
   "source": [
    "### b) identify deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "524d8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the merged list\n",
    "aligned_input_trace = []\n",
    "\n",
    "# Iterate through each row of the fitness dataframe\n",
    "for index, row in closest_distance.iterrows():\n",
    "    closest_trace = row['Closest Trace']\n",
    "    \n",
    "    # Append the corresponding alignment to the merged list based on the closest trace\n",
    "    if closest_trace == 'trace_1':\n",
    "        aligned_input_trace.append(aligned_input_trace1[index])\n",
    "    elif closest_trace == 'trace_2':\n",
    "        aligned_input_trace.append(aligned_input_trace2[index])\n",
    "    elif closest_trace == 'trace_3':\n",
    "        aligned_input_trace.append(aligned_input_trace3[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3be04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitness values\n",
    "aligned_traces_fitness = [trace['fitness'] for trace in aligned_traces]\n",
    "aligned_input_traces_fitness = [trace['fitness'] for trace in aligned_input_trace]\n",
    "\n",
    "# Create DataFrame\n",
    "df_fitness = pd.DataFrame({\n",
    "    'ground_truth_fit': aligned_traces_fitness,\n",
    "    'predicted_fit': aligned_input_traces_fitness\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa000d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices where 'predicted' column has value 1\n",
    "indices_to_keep = classification[classification['actual'] == 1].index.tolist()\n",
    "\n",
    "# Filter the lists to keep only the indices where 'predicted' is 1\n",
    "aligned_input_trace = [aligned_input_trace[i] for i in indices_to_keep]\n",
    "aligned_traces = [aligned_traces[i] for i in indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "380d3013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Log Moves): 0.78\n",
      "Recall (Log Moves): 0.97\n",
      "\n",
      "Precision (Model Moves): 0.42\n",
      "Recall (Model Moves): 0.90\n"
     ]
    }
   ],
   "source": [
    "# Function to extract log and model moves excluding (None, >>) and (>>, None)\n",
    "def extract_moves(alignment):\n",
    "    log_moves = [move for move in alignment if move[1] == '>>' and move[0] is not None]\n",
    "    model_moves = [move for move in alignment if move[0] == '>>' and move[1] is not None]\n",
    "    return log_moves, model_moves\n",
    "\n",
    "# Initialize counts for moves\n",
    "total_log_moves = 0\n",
    "total_no_log_moves = 0\n",
    "total_model_moves = 0\n",
    "total_no_model_moves = 0\n",
    "\n",
    "# Initialize counts for TP, FP, FN, TN\n",
    "tp_log_moves = 0\n",
    "fp_log_moves = 0\n",
    "fn_log_moves = 0\n",
    "tn_log_moves = 0\n",
    "\n",
    "tp_model_moves = 0\n",
    "fp_model_moves = 0\n",
    "fn_model_moves = 0\n",
    "tn_model_moves = 0\n",
    "\n",
    "# Iterate through aligned traces and count moves\n",
    "for i, aligned_trace in enumerate(aligned_traces):\n",
    "    log_moves_gt, model_moves_gt = extract_moves(aligned_trace['alignment'])\n",
    "    total_log_moves += len(log_moves_gt)\n",
    "    total_no_log_moves += sum(1 for move in aligned_trace['alignment'] if move[1] != '>>' or move[0] is None)\n",
    "    total_model_moves += len(model_moves_gt)\n",
    "    total_no_model_moves += sum(1 for move in aligned_trace['alignment'] if move[0] != '>>' or move[1] is None)\n",
    "    \n",
    "    if i < len(aligned_input_trace):\n",
    "        log_moves_input, model_moves_input = extract_moves(aligned_input_trace[i]['alignment'])\n",
    "        \n",
    "        # Calculate TP, FP, FN, TN for log moves\n",
    "        tp_log_moves += sum(1 for move in log_moves_gt if move in log_moves_input)\n",
    "        fn_log_moves += sum(1 for move in log_moves_gt if move not in log_moves_input)\n",
    "        fp_log_moves += sum(1 for move in log_moves_input if move not in log_moves_gt)\n",
    "        tn_log_moves += sum(1 for move in aligned_trace['alignment'] if move not in log_moves_gt and move not in log_moves_input and move[1] != '>>' and move[0] != '>>')\n",
    "        \n",
    "        # Calculate TP, FP, FN, TN for model moves\n",
    "        tp_model_moves += sum(1 for move in model_moves_gt if move in model_moves_input)\n",
    "        fn_model_moves += sum(1 for move in model_moves_gt if move not in model_moves_input)\n",
    "        fp_model_moves += sum(1 for move in model_moves_input if move not in model_moves_gt)\n",
    "        tn_model_moves += sum(1 for move in aligned_trace['alignment'] if move not in model_moves_gt and move not in model_moves_input and move[1] != '>>' and move[0] != '>>')\n",
    "\n",
    "# Calculate recall, precision, F1 score for log moves\n",
    "recall_log_moves = tp_log_moves / (tp_log_moves + fn_log_moves) if (tp_log_moves + fn_log_moves) > 0 else 0\n",
    "precision_log_moves = tp_log_moves / (tp_log_moves + fp_log_moves) if (tp_log_moves + fp_log_moves) > 0 else 0\n",
    "f1_score_log_moves = 2 * (precision_log_moves * recall_log_moves) / (precision_log_moves + recall_log_moves) if (precision_log_moves + recall_log_moves) > 0 else 0\n",
    "\n",
    "# Calculate recall, precision, F1 score for model moves\n",
    "recall_model_moves = tp_model_moves / (tp_model_moves + fn_model_moves) if (tp_model_moves + fn_model_moves) > 0 else 0\n",
    "precision_model_moves = tp_model_moves / (tp_model_moves + fp_model_moves) if (tp_model_moves + fp_model_moves) > 0 else 0\n",
    "f1_score_model_moves = 2 * (precision_model_moves * recall_model_moves) / (precision_model_moves + recall_model_moves) if (precision_model_moves + recall_model_moves) > 0 else 0\n",
    "\n",
    "# Calculate dataset balance for log moves\n",
    "log_move_percentage = (total_log_moves / (total_log_moves + total_no_log_moves)) * 100 if (total_log_moves + total_no_log_moves) > 0 else 0\n",
    "no_log_move_percentage = (total_no_log_moves / (total_log_moves + total_no_log_moves)) * 100 if (total_log_moves + total_no_log_moves) > 0 else 0\n",
    "\n",
    "# Calculate dataset balance for model moves\n",
    "model_move_percentage = (total_model_moves / (total_model_moves + total_no_model_moves)) * 100 if (total_model_moves + total_no_model_moves) > 0 else 0\n",
    "no_model_move_percentage = (total_no_model_moves / (total_model_moves + total_no_model_moves)) * 100 if (total_model_moves + total_no_model_moves) > 0 else 0\n",
    "\n",
    "# Print results for log moves\n",
    "print(f\"Precision (Log Moves): {precision_log_moves:.2f}\")\n",
    "print(f\"Recall (Log Moves): {recall_log_moves:.2f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Print results for model moves\n",
    "print(f\"Precision (Model Moves): {precision_model_moves:.2f}\")\n",
    "print(f\"Recall (Model Moves): {recall_model_moves:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cc7afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the other dataframe using the indices_to_keep\n",
    "df_fitness = df_fitness.loc[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e645b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error (MSE) is: 0.0023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(df_fitness['ground_truth_fit'], df_fitness['predicted_fit'])\n",
    "\n",
    "# Print the MSE restricted to 4 decimal places\n",
    "print(f\"The Mean Squared Error (MSE) is: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bb1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
