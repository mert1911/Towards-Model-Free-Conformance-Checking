{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_10514/754145525.py:11: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/p2p_log.csv', sep=',')\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['@@index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Start & End markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert start and end markers\n",
    "def add_markers(df):\n",
    "    # Identify unique case indices\n",
    "    case_indices = df['@@case_index'].unique()\n",
    "    \n",
    "    # Prepare a container for new DataFrame rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each case index to add start and end markers\n",
    "    for case_index in case_indices:\n",
    "        # Create a start marker row with all columns except @@case_index set to 'Start'\n",
    "        start_row = {col: 'Start' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Create an end marker row with all columns except @@case_index set to 'End'\n",
    "        end_row = {col: 'End' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Append start row, rows for the current case, and end row\n",
    "        new_rows.append(start_row)\n",
    "        new_rows.extend(df[df['@@case_index'] == case_index].to_dict('records'))\n",
    "        new_rows.append(end_row)\n",
    "    \n",
    "    # Convert the list of rows into a DataFrame\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Apply the function to add start and end markers to the dataframe\n",
    "modified_dataframe = add_markers(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dataframe['standardized_elapsed_time'] = modified_dataframe['standardized_elapsed_time'].replace({'Start': 0, 'End': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['concept:name'])\n",
    "modified_dataframe['concept:name'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['org:resource'])\n",
    "modified_dataframe['org:resource'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept:name</th>\n",
       "      <th>case:label</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Start</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.027862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.027719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.027577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.027292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53188</th>\n",
       "      <td>5</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.026723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53189</th>\n",
       "      <td>6</td>\n",
       "      <td>normal</td>\n",
       "      <td>24</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.026580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53190</th>\n",
       "      <td>7</td>\n",
       "      <td>normal</td>\n",
       "      <td>25</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.026438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53191</th>\n",
       "      <td>8</td>\n",
       "      <td>normal</td>\n",
       "      <td>8</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.026153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53192</th>\n",
       "      <td>9</td>\n",
       "      <td>End</td>\n",
       "      <td>9</td>\n",
       "      <td>4999</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53193 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       concept:name case:label  org:resource  @@case_index  \\\n",
       "0                 0      Start             0             0   \n",
       "1                 1     normal             1             0   \n",
       "2                 2     normal             2             0   \n",
       "3                 3     normal             3             0   \n",
       "4                 4     normal             4             0   \n",
       "...             ...        ...           ...           ...   \n",
       "53188             5     normal             5          4999   \n",
       "53189             6     normal            24          4999   \n",
       "53190             7     normal            25          4999   \n",
       "53191             8     normal             8          4999   \n",
       "53192             9        End             9          4999   \n",
       "\n",
       "       standardized_elapsed_time  \n",
       "0                       0.000000  \n",
       "1                      -0.027862  \n",
       "2                      -0.027719  \n",
       "3                      -0.027577  \n",
       "4                      -0.027292  \n",
       "...                          ...  \n",
       "53188                  -0.026723  \n",
       "53189                  -0.026580  \n",
       "53190                  -0.026438  \n",
       "53191                  -0.026153  \n",
       "53192                   1.000000  \n",
       "\n",
       "[53193 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for Cases with less then 5 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each unique value in '@@case_index'\n",
    "frequency = modified_dataframe['@@case_index'].value_counts()\n",
    "\n",
    "# Finding the minimum occurrence\n",
    "min_occurrence = frequency.min()\n",
    "\n",
    "min_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Assuming your dataframe might have different types, let's create a generic function to add rows\n",
    "def add_rows(group, num_rows, case_index_value):\n",
    "    # For each column, determine the appropriate \"zero\" value (int 0, string '', etc.)\n",
    "    additional_rows = pd.DataFrame({\n",
    "        column: 0 if pd.api.types.is_numeric_dtype(group[column]) else '' for column in group.columns\n",
    "    }, index=range(num_rows))\n",
    "    \n",
    "    # Set the @@case_index column to the current case index value\n",
    "    additional_rows['@@case_index'] = case_index_value\n",
    "    \n",
    "    # Append the additional rows to the group\n",
    "    return pd.concat([group, additional_rows], ignore_index=True)\n",
    "\n",
    "# Function to pad cases with less than 5 events\n",
    "def pad_cases(df):\n",
    "    # Group by @@case_index\n",
    "    groups = df.groupby('@@case_index')\n",
    "    \n",
    "    # Placeholder for modified groups\n",
    "    modified_groups = []\n",
    "    \n",
    "    for name, group in groups:\n",
    "        # Calculate the number of events to add\n",
    "        events_to_add = 5 - len(group)\n",
    "        \n",
    "        if events_to_add > 0:\n",
    "            # Add the required number of rows\n",
    "            group = add_rows(group, events_to_add, name)\n",
    "        \n",
    "        # Append the modified group to the list\n",
    "        modified_groups.append(group)\n",
    "    \n",
    "    # Concatenate all modified groups back into a single DataFrame\n",
    "    return pd.concat(modified_groups, ignore_index=True)  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resource = modified_dataframe[['org:resource', '@@case_index']]\n",
    "df_activity = modified_dataframe[['concept:name', '@@case_index']]\n",
    "df_timestamp = modified_dataframe[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df, case_id_column='@@case_index', window_size=5):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    # Iterate over each unique case\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        # Extract the case\n",
    "        case_data = df[df[case_id_column] == case_id]\n",
    "        \n",
    "        # Convert case_data to a NumPy array and drop the case_id_column\n",
    "        case_data_array = case_data.drop(columns=[case_id_column]).to_numpy()\n",
    "\n",
    "        # Adjusting the condition to correctly reflect window_size without needing an additional +1\n",
    "        # Now it correctly considers window_size as including the target event\n",
    "        if len(case_data_array) >= window_size:\n",
    "            # Adjust the loop to generate sliding windows of size window_size - 1 for the inputs and use the next event as the target\n",
    "            for i in range(len(case_data_array) - window_size + 1):\n",
    "                # window now has window_size - 1 events\n",
    "                window = case_data_array[i:i + window_size - 1]\n",
    "                # The target is the event immediately following the window\n",
    "                target = case_data_array[i + window_size - 1]\n",
    "                windows.append(window)\n",
    "                targets.append(target)\n",
    "                case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling and to ensure they are two-dimensional\n",
    "    windows_array = np.array(windows)\n",
    "    targets_array = np.array(targets)\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_array, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_resource, targets_resource, case_indices = generate_sliding_windows(df_resource)\n",
    "windows_activity, targets_activity, case_indices = generate_sliding_windows(df_activity)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_sliding_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 15:00:08.378995: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_resource (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_activity (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 4, 50)                7050      ['input_resource[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 4, 50)                1350      ['input_activity[0][0]']      \n",
      "                                                                                                  \n",
      " input_timestamp (InputLaye  [(None, 4, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 25)                   7600      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 25)                   7600      ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 25)                   2700      ['input_timestamp[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 75)                   0         ['lstm[0][0]',                \n",
      "                                                                     'lstm_1[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_resource (Dense)     (None, 141)                  10716     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 27)                   2052      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    76        ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39144 (152.91 KB)\n",
      "Trainable params: 39144 (152.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding\n",
    "\n",
    "# Assuming these values as placeholders, replace them with actual counts from your data\n",
    "num_resources = modified_dataframe['org:resource'].nunique()\n",
    "num_activities = modified_dataframe['concept:name'].nunique()\n",
    "\n",
    "embedding_dim_resource = 50\n",
    "embedding_dim_activity = 50\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "# Input layers\n",
    "input_resource = Input(shape=(time_steps,), name='input_resource')\n",
    "input_activity = Input(shape=(time_steps,), name='input_activity')\n",
    "input_timestamp = Input(shape=(time_steps, 1), name='input_timestamp')\n",
    "\n",
    "# Embedding layers\n",
    "embedding_resource = Embedding(input_dim=num_resources, output_dim=embedding_dim_resource, input_length=time_steps)(input_resource)\n",
    "embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim_activity, input_length=time_steps)(input_activity)\n",
    "\n",
    "# LSTM layers\n",
    "lstm_resource = LSTM(25, return_sequences=False)(embedding_resource)\n",
    "lstm_activity = LSTM(25, return_sequences=False)(embedding_activity)\n",
    "lstm_timestamp = LSTM(25, return_sequences=False)(input_timestamp)\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate(axis=-1)([lstm_resource, lstm_activity, lstm_timestamp])\n",
    "\n",
    "# Output layers\n",
    "output_resource = Dense(num_resources, activation='softmax', name='output_resource')(concatenated)\n",
    "output_activity = Dense(num_activities, activation='softmax', name='output_activity')(concatenated)\n",
    "output_timestamp = Dense(1, activation='linear', name='output_timestamp')(concatenated)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[input_resource, input_activity, input_timestamp], \n",
    "              outputs=[output_resource, output_activity, output_timestamp])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'output_resource': 'categorical_crossentropy', \n",
    "                    'output_activity': 'categorical_crossentropy', \n",
    "                    'output_timestamp': 'mean_squared_error'},\n",
    "              metrics={'output_resource': 'accuracy', \n",
    "                       'output_activity': 'accuracy', \n",
    "                       'output_timestamp': 'mean_absolute_error'})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resource data\n",
    "train_resource, test_resource, train_targets_resource, test_targets_resource = train_test_split(\n",
    "    windows_resource, targets_resource, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the activity data\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the timestamp data\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def cyclic_lr(epoch, lr):\n",
    "    # Example function that modulates LR within a range for each epoch\n",
    "    # Customize this function based on your cyclic learning rate policy\n",
    "    max_lr = 0.01  # Maximum LR\n",
    "    base_lr = 0.001  # Base LR\n",
    "    step_size = 10  # Number of epochs for half a cycle\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "364/364 [==============================] - 9s 8ms/step - loss: 5.0126 - output_resource_loss: 2.5780 - output_activity_loss: 1.3412 - output_timestamp_loss: 1.0934 - output_resource_accuracy: 0.4080 - output_activity_accuracy: 0.7024 - output_timestamp_mean_absolute_error: 0.1672 - val_loss: 3.7066 - val_output_resource_loss: 1.6145 - val_output_activity_loss: 0.5171 - val_output_timestamp_loss: 1.5750 - val_output_resource_accuracy: 0.5571 - val_output_activity_accuracy: 0.9040 - val_output_timestamp_mean_absolute_error: 0.0940 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 2.6481 - output_resource_loss: 1.3310 - output_activity_loss: 0.3824 - output_timestamp_loss: 0.9346 - output_resource_accuracy: 0.6056 - output_activity_accuracy: 0.9248 - output_timestamp_mean_absolute_error: 0.1091 - val_loss: 2.9954 - val_output_resource_loss: 1.1946 - val_output_activity_loss: 0.3530 - val_output_timestamp_loss: 1.4478 - val_output_resource_accuracy: 0.6165 - val_output_activity_accuracy: 0.9312 - val_output_timestamp_mean_absolute_error: 0.0884 - lr: 0.0019\n",
      "Epoch 3/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 2.2761 - output_resource_loss: 1.1506 - output_activity_loss: 0.3171 - output_timestamp_loss: 0.8084 - output_resource_accuracy: 0.6316 - output_activity_accuracy: 0.9375 - output_timestamp_mean_absolute_error: 0.1134 - val_loss: 2.8254 - val_output_resource_loss: 1.1172 - val_output_activity_loss: 0.3193 - val_output_timestamp_loss: 1.3889 - val_output_resource_accuracy: 0.6474 - val_output_activity_accuracy: 0.9395 - val_output_timestamp_mean_absolute_error: 0.1672 - lr: 0.0028\n",
      "Epoch 4/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 2.0910 - output_resource_loss: 1.0903 - output_activity_loss: 0.2864 - output_timestamp_loss: 0.7144 - output_resource_accuracy: 0.6433 - output_activity_accuracy: 0.9453 - output_timestamp_mean_absolute_error: 0.1127 - val_loss: 2.7026 - val_output_resource_loss: 1.0936 - val_output_activity_loss: 0.2986 - val_output_timestamp_loss: 1.3104 - val_output_resource_accuracy: 0.6386 - val_output_activity_accuracy: 0.9436 - val_output_timestamp_mean_absolute_error: 0.1374 - lr: 0.0037\n",
      "Epoch 5/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.9598 - output_resource_loss: 1.0641 - output_activity_loss: 0.2682 - output_timestamp_loss: 0.6275 - output_resource_accuracy: 0.6494 - output_activity_accuracy: 0.9495 - output_timestamp_mean_absolute_error: 0.1259 - val_loss: 2.6086 - val_output_resource_loss: 1.0778 - val_output_activity_loss: 0.2893 - val_output_timestamp_loss: 1.2415 - val_output_resource_accuracy: 0.6502 - val_output_activity_accuracy: 0.9461 - val_output_timestamp_mean_absolute_error: 0.0981 - lr: 0.0046\n",
      "Epoch 6/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.8191 - output_resource_loss: 1.0455 - output_activity_loss: 0.2570 - output_timestamp_loss: 0.5167 - output_resource_accuracy: 0.6492 - output_activity_accuracy: 0.9523 - output_timestamp_mean_absolute_error: 0.1284 - val_loss: 2.5657 - val_output_resource_loss: 1.0829 - val_output_activity_loss: 0.2842 - val_output_timestamp_loss: 1.1986 - val_output_resource_accuracy: 0.6451 - val_output_activity_accuracy: 0.9468 - val_output_timestamp_mean_absolute_error: 0.1598 - lr: 0.0055\n",
      "Epoch 7/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.7102 - output_resource_loss: 1.0352 - output_activity_loss: 0.2507 - output_timestamp_loss: 0.4242 - output_resource_accuracy: 0.6485 - output_activity_accuracy: 0.9529 - output_timestamp_mean_absolute_error: 0.1252 - val_loss: 2.4913 - val_output_resource_loss: 1.0775 - val_output_activity_loss: 0.2739 - val_output_timestamp_loss: 1.1399 - val_output_resource_accuracy: 0.6432 - val_output_activity_accuracy: 0.9501 - val_output_timestamp_mean_absolute_error: 0.1112 - lr: 0.0064\n",
      "Epoch 8/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.6326 - output_resource_loss: 1.0300 - output_activity_loss: 0.2465 - output_timestamp_loss: 0.3562 - output_resource_accuracy: 0.6499 - output_activity_accuracy: 0.9542 - output_timestamp_mean_absolute_error: 0.1201 - val_loss: 2.4804 - val_output_resource_loss: 1.0877 - val_output_activity_loss: 0.2819 - val_output_timestamp_loss: 1.1108 - val_output_resource_accuracy: 0.6536 - val_output_activity_accuracy: 0.9475 - val_output_timestamp_mean_absolute_error: 0.1499 - lr: 0.0073\n",
      "Epoch 9/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.5604 - output_resource_loss: 1.0196 - output_activity_loss: 0.2418 - output_timestamp_loss: 0.2989 - output_resource_accuracy: 0.6474 - output_activity_accuracy: 0.9548 - output_timestamp_mean_absolute_error: 0.1203 - val_loss: 2.4715 - val_output_resource_loss: 1.0846 - val_output_activity_loss: 0.2774 - val_output_timestamp_loss: 1.1095 - val_output_resource_accuracy: 0.6505 - val_output_activity_accuracy: 0.9493 - val_output_timestamp_mean_absolute_error: 0.1591 - lr: 0.0082\n",
      "Epoch 10/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.5170 - output_resource_loss: 1.0192 - output_activity_loss: 0.2421 - output_timestamp_loss: 0.2557 - output_resource_accuracy: 0.6486 - output_activity_accuracy: 0.9548 - output_timestamp_mean_absolute_error: 0.1223 - val_loss: 2.4630 - val_output_resource_loss: 1.0954 - val_output_activity_loss: 0.2763 - val_output_timestamp_loss: 1.0913 - val_output_resource_accuracy: 0.6552 - val_output_activity_accuracy: 0.9501 - val_output_timestamp_mean_absolute_error: 0.1358 - lr: 0.0091\n",
      "Epoch 11/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.4890 - output_resource_loss: 1.0140 - output_activity_loss: 0.2369 - output_timestamp_loss: 0.2381 - output_resource_accuracy: 0.6498 - output_activity_accuracy: 0.9555 - output_timestamp_mean_absolute_error: 0.1189 - val_loss: 2.4870 - val_output_resource_loss: 1.1181 - val_output_activity_loss: 0.2841 - val_output_timestamp_loss: 1.0848 - val_output_resource_accuracy: 0.6399 - val_output_activity_accuracy: 0.9475 - val_output_timestamp_mean_absolute_error: 0.0984 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.4517 - output_resource_loss: 0.9977 - output_activity_loss: 0.2325 - output_timestamp_loss: 0.2215 - output_resource_accuracy: 0.6518 - output_activity_accuracy: 0.9575 - output_timestamp_mean_absolute_error: 0.1074 - val_loss: 2.4616 - val_output_resource_loss: 1.0894 - val_output_activity_loss: 0.2795 - val_output_timestamp_loss: 1.0927 - val_output_resource_accuracy: 0.6567 - val_output_activity_accuracy: 0.9498 - val_output_timestamp_mean_absolute_error: 0.0831 - lr: 0.0091\n",
      "Epoch 13/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 1.4239 - output_resource_loss: 0.9835 - output_activity_loss: 0.2271 - output_timestamp_loss: 0.2133 - output_resource_accuracy: 0.6564 - output_activity_accuracy: 0.9569 - output_timestamp_mean_absolute_error: 0.0979 - val_loss: 2.4781 - val_output_resource_loss: 1.1042 - val_output_activity_loss: 0.2716 - val_output_timestamp_loss: 1.1022 - val_output_resource_accuracy: 0.6287 - val_output_activity_accuracy: 0.9496 - val_output_timestamp_mean_absolute_error: 0.0838 - lr: 0.0082\n",
      "Epoch 14/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 1.3987 - output_resource_loss: 0.9685 - output_activity_loss: 0.2211 - output_timestamp_loss: 0.2091 - output_resource_accuracy: 0.6561 - output_activity_accuracy: 0.9580 - output_timestamp_mean_absolute_error: 0.0916 - val_loss: 2.4691 - val_output_resource_loss: 1.0885 - val_output_activity_loss: 0.2792 - val_output_timestamp_loss: 1.1014 - val_output_resource_accuracy: 0.6340 - val_output_activity_accuracy: 0.9491 - val_output_timestamp_mean_absolute_error: 0.1106 - lr: 0.0073\n",
      "Epoch 15/25\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 1.3777 - output_resource_loss: 0.9561 - output_activity_loss: 0.2182 - output_timestamp_loss: 0.2034 - output_resource_accuracy: 0.6608 - output_activity_accuracy: 0.9575 - output_timestamp_mean_absolute_error: 0.0927 - val_loss: 2.4661 - val_output_resource_loss: 1.0845 - val_output_activity_loss: 0.2780 - val_output_timestamp_loss: 1.1037 - val_output_resource_accuracy: 0.6489 - val_output_activity_accuracy: 0.9506 - val_output_timestamp_mean_absolute_error: 0.0983 - lr: 0.0064\n",
      "Epoch 16/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 1.3527 - output_resource_loss: 0.9404 - output_activity_loss: 0.2118 - output_timestamp_loss: 0.2006 - output_resource_accuracy: 0.6618 - output_activity_accuracy: 0.9596 - output_timestamp_mean_absolute_error: 0.0877 - val_loss: 2.4815 - val_output_resource_loss: 1.0922 - val_output_activity_loss: 0.2797 - val_output_timestamp_loss: 1.1095 - val_output_resource_accuracy: 0.6551 - val_output_activity_accuracy: 0.9480 - val_output_timestamp_mean_absolute_error: 0.1073 - lr: 0.0055\n",
      "Epoch 17/25\n",
      "364/364 [==============================] - 3s 7ms/step - loss: 1.3324 - output_resource_loss: 0.9276 - output_activity_loss: 0.2088 - output_timestamp_loss: 0.1960 - output_resource_accuracy: 0.6663 - output_activity_accuracy: 0.9594 - output_timestamp_mean_absolute_error: 0.0820 - val_loss: 2.4695 - val_output_resource_loss: 1.0835 - val_output_activity_loss: 0.2742 - val_output_timestamp_loss: 1.1118 - val_output_resource_accuracy: 0.6495 - val_output_activity_accuracy: 0.9513 - val_output_timestamp_mean_absolute_error: 0.1161 - lr: 0.0046\n",
      "Epoch 18/25\n",
      "364/364 [==============================] - 3s 8ms/step - loss: 1.3131 - output_resource_loss: 0.9152 - output_activity_loss: 0.2031 - output_timestamp_loss: 0.1949 - output_resource_accuracy: 0.6668 - output_activity_accuracy: 0.9610 - output_timestamp_mean_absolute_error: 0.0789 - val_loss: 2.4639 - val_output_resource_loss: 1.0871 - val_output_activity_loss: 0.2738 - val_output_timestamp_loss: 1.1029 - val_output_resource_accuracy: 0.6356 - val_output_activity_accuracy: 0.9501 - val_output_timestamp_mean_absolute_error: 0.0915 - lr: 0.0037\n",
      "Epoch 19/25\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 1.2942 - output_resource_loss: 0.9035 - output_activity_loss: 0.1996 - output_timestamp_loss: 0.1911 - output_resource_accuracy: 0.6706 - output_activity_accuracy: 0.9614 - output_timestamp_mean_absolute_error: 0.0741 - val_loss: 2.4620 - val_output_resource_loss: 1.0809 - val_output_activity_loss: 0.2741 - val_output_timestamp_loss: 1.1070 - val_output_resource_accuracy: 0.6497 - val_output_activity_accuracy: 0.9504 - val_output_timestamp_mean_absolute_error: 0.0971 - lr: 0.0028\n",
      "Epoch 20/25\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 1.2756 - output_resource_loss: 0.8924 - output_activity_loss: 0.1946 - output_timestamp_loss: 0.1886 - output_resource_accuracy: 0.6757 - output_activity_accuracy: 0.9624 - output_timestamp_mean_absolute_error: 0.0672 - val_loss: 2.4663 - val_output_resource_loss: 1.0818 - val_output_activity_loss: 0.2742 - val_output_timestamp_loss: 1.1102 - val_output_resource_accuracy: 0.6491 - val_output_activity_accuracy: 0.9496 - val_output_timestamp_mean_absolute_error: 0.0888 - lr: 0.0019\n",
      "Epoch 21/25\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 1.2581 - output_resource_loss: 0.8809 - output_activity_loss: 0.1916 - output_timestamp_loss: 0.1856 - output_resource_accuracy: 0.6791 - output_activity_accuracy: 0.9629 - output_timestamp_mean_absolute_error: 0.0627 - val_loss: 2.4628 - val_output_resource_loss: 1.0815 - val_output_activity_loss: 0.2736 - val_output_timestamp_loss: 1.1076 - val_output_resource_accuracy: 0.6515 - val_output_activity_accuracy: 0.9502 - val_output_timestamp_mean_absolute_error: 0.0770 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "364/364 [==============================] - 3s 8ms/step - loss: 1.2682 - output_resource_loss: 0.8866 - output_activity_loss: 0.1937 - output_timestamp_loss: 0.1879 - output_resource_accuracy: 0.6769 - output_activity_accuracy: 0.9628 - output_timestamp_mean_absolute_error: 0.0680 - val_loss: 2.4678 - val_output_resource_loss: 1.0839 - val_output_activity_loss: 0.2737 - val_output_timestamp_loss: 1.1101 - val_output_resource_accuracy: 0.6520 - val_output_activity_accuracy: 0.9509 - val_output_timestamp_mean_absolute_error: 0.0816 - lr: 0.0019\n",
      "Epoch 23/25\n",
      "364/364 [==============================] - 3s 7ms/step - loss: 1.2780 - output_resource_loss: 0.8930 - output_activity_loss: 0.1954 - output_timestamp_loss: 0.1896 - output_resource_accuracy: 0.6740 - output_activity_accuracy: 0.9622 - output_timestamp_mean_absolute_error: 0.0747 - val_loss: 2.4814 - val_output_resource_loss: 1.0971 - val_output_activity_loss: 0.2752 - val_output_timestamp_loss: 1.1091 - val_output_resource_accuracy: 0.6239 - val_output_activity_accuracy: 0.9505 - val_output_timestamp_mean_absolute_error: 0.1069 - lr: 0.0028\n",
      "Epoch 24/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 1.2894 - output_resource_loss: 0.8995 - output_activity_loss: 0.1979 - output_timestamp_loss: 0.1920 - output_resource_accuracy: 0.6717 - output_activity_accuracy: 0.9623 - output_timestamp_mean_absolute_error: 0.0750 - val_loss: 2.5034 - val_output_resource_loss: 1.0951 - val_output_activity_loss: 0.2781 - val_output_timestamp_loss: 1.1301 - val_output_resource_accuracy: 0.6501 - val_output_activity_accuracy: 0.9495 - val_output_timestamp_mean_absolute_error: 0.1696 - lr: 0.0037\n",
      "Epoch 25/25\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 1.2370 - output_resource_loss: 0.9024 - output_activity_loss: 0.2000 - output_timestamp_loss: 0.1347 - output_resource_accuracy: 0.6687 - output_activity_accuracy: 0.9611 - output_timestamp_mean_absolute_error: 0.0817 - val_loss: 2.4898 - val_output_resource_loss: 1.1043 - val_output_activity_loss: 0.2816 - val_output_timestamp_loss: 1.1038 - val_output_resource_accuracy: 0.6481 - val_output_activity_accuracy: 0.9497 - val_output_timestamp_mean_absolute_error: 0.1325 - lr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_targets_resource_cat = to_categorical(train_targets_resource, num_classes=num_resources)\n",
    "test_targets_resource_cat = to_categorical(test_targets_resource, num_classes=num_resources)\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "\n",
    "# Note: Assuming timestamp targets are continuous and don't need to be converted to categorical\n",
    "\n",
    "history = model.fit([train_resource, train_activity, train_timestamp], \n",
    "                    [train_targets_resource_cat, train_targets_activity_cat, train_targets_timestamp],\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=([test_resource, test_activity, test_timestamp], \n",
    "                                     [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]),\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_scheduler])  # Add other callbacks as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 1s 2ms/step - loss: 2.4898 - output_resource_loss: 1.1043 - output_activity_loss: 0.2816 - output_timestamp_loss: 1.1038 - output_resource_accuracy: 0.6481 - output_activity_accuracy: 0.9497 - output_timestamp_mean_absolute_error: 0.1325\n",
      "[2.4897677898406982, 1.104286789894104, 0.28163301944732666, 1.1038473844528198, 0.6481221318244934, 0.9496886730194092, 0.1324796825647354]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(\n",
    "    [test_resource, test_activity, test_timestamp],\n",
    "    [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038/1038 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_resource, windows_activity, windows_timestamp])\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_resource = predictions[0]        # ID predictions\n",
    "predictions_activity = predictions[1]  # Resource predictions\n",
    "\n",
    "# Extract predictions for numerical attribute (timestamp)\n",
    "predictions_timestamp = predictions[2] # Timestamp predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def compute_anomaly_scores(predictions, actuals):\n",
    "    # For categorical predictions, convert actuals to one-hot for comparison\n",
    "    actuals_one_hot = to_categorical(actuals, num_classes=predictions.shape[-1])\n",
    "    \n",
    "    max_predictions = np.max(predictions, axis=-1)\n",
    "    actual_predictions = np.sum(predictions * actuals_one_hot, axis=-1)  # Extract the probability of the actual class\n",
    "    \n",
    "    anomaly_scores = (max_predictions - actual_predictions) / max_predictions\n",
    "    \n",
    "    return anomaly_scores\n",
    "\n",
    "# Assuming targets_id, targets_resource, targets_activity, targets_role are the true values for these attributes\n",
    "anomaly_scores_resource = compute_anomaly_scores(predictions_resource, targets_resource)\n",
    "anomaly_scores_activity = compute_anomaly_scores(predictions_activity, targets_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals, normalization_factor):\n",
    "    \"\"\"\n",
    "    Compute anomaly scores for continuous attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predicted values.\n",
    "    - actuals: numpy array of actual values.\n",
    "    - normalization_factor: normalization factor (e.g., standard deviation of the attribute).\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of anomaly scores.\n",
    "    \"\"\"\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize the differences\n",
    "    anomaly_scores = differences / normalization_factor\n",
    "    \n",
    "    return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_factor = np.std(targets_timestamp)       # Example normalization factor (standard deviation)\n",
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp, normalization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_resource = np.array(anomaly_scores_resource).flatten()\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "    anomaly_scores_timestamp = np.array(anomaly_scores_timestamp).flatten()\n",
    "\n",
    "    # Check if all arrays have the same length\n",
    "    if not (len(anomaly_scores_resource) == len(anomaly_scores_activity) == len(anomaly_scores_timestamp)):\n",
    "        raise ValueError(\"All input anomaly scores must have the same length.\")\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_activity, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_activity])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True: anomaly, False: no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33188</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33189</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33190</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33191</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33192</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33193 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted\n",
       "0         0      False\n",
       "1         0      False\n",
       "2         0      False\n",
       "3         0      False\n",
       "4         0      False\n",
       "...     ...        ...\n",
       "33188  4999      False\n",
       "33189  4999      False\n",
       "33190  4999      False\n",
       "33191  4999      False\n",
       "33192  4999      False\n",
       "\n",
       "[33193 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted'] = anomalous_cases\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1       False\n",
       "2        True\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "4995    False\n",
       "4996    False\n",
       "4997    False\n",
       "4998    False\n",
       "4999    False\n",
       "Name: predicted, Length: 5000, dtype: bool"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_prediction = mapping.groupby('case')['predicted'].any()\n",
    "case_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491c8b789d3a4f10857d700762b5de93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/p2p.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(1)\n",
    "        else:\n",
    "            conformance_status.append(0)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              1          1\n",
       "1              1          1\n",
       "2              0          0\n",
       "3              1          1\n",
       "4              1          1\n",
       "...          ...        ...\n",
       "4995           1          1\n",
       "4996           1          1\n",
       "4997           1          1\n",
       "4998           1          1\n",
       "4999           1          1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth['predicted'] = 1 - ground_truth['predicted']\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.906\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.941\n"
     ]
    }
   ],
   "source": [
    "# Calculate f1\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.982\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = TN / (TN + FN)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.625\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = TN / (TN + FP)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.892\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.996\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.810496273464129"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming ground_truth is your DataFrame\n",
    "# Make sure 'conformity' contains actual labels (0 or 1)\n",
    "# and 'predicted' contains predicted probabilities or scores\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "auc_roc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
