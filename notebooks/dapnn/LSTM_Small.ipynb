{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_4962/3514865066.py:11: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/small_log.csv', sep=',')\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['@@index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept:name</th>\n",
       "      <th>case:label</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Activity A</td>\n",
       "      <td>normal</td>\n",
       "      <td>Sherlene</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.051301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Activity B</td>\n",
       "      <td>normal</td>\n",
       "      <td>Earl</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.050813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Activity C</td>\n",
       "      <td>normal</td>\n",
       "      <td>Brant</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.050326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Activity D</td>\n",
       "      <td>normal</td>\n",
       "      <td>Lourdes</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.049350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Activity E</td>\n",
       "      <td>normal</td>\n",
       "      <td>Hugo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.048862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43432</th>\n",
       "      <td>Activity K</td>\n",
       "      <td>Insert</td>\n",
       "      <td>Melany</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.046424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43433</th>\n",
       "      <td>Activity O</td>\n",
       "      <td>Insert</td>\n",
       "      <td>Hassie</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.045448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43434</th>\n",
       "      <td>Activity P</td>\n",
       "      <td>Insert</td>\n",
       "      <td>Sigrid</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.044961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43435</th>\n",
       "      <td>Activity L</td>\n",
       "      <td>Insert</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.043985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43436</th>\n",
       "      <td>Activity F</td>\n",
       "      <td>Insert</td>\n",
       "      <td>Emily</td>\n",
       "      <td>4999</td>\n",
       "      <td>-0.043497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43437 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      concept:name case:label org:resource  @@case_index  \\\n",
       "0       Activity A     normal     Sherlene             0   \n",
       "1       Activity B     normal         Earl             0   \n",
       "2       Activity C     normal        Brant             0   \n",
       "3       Activity D     normal      Lourdes             0   \n",
       "4       Activity E     normal         Hugo             0   \n",
       "...            ...        ...          ...           ...   \n",
       "43432   Activity K     Insert       Melany          4999   \n",
       "43433   Activity O     Insert       Hassie          4999   \n",
       "43434   Activity P     Insert       Sigrid          4999   \n",
       "43435   Activity L     Insert       Amanda          4999   \n",
       "43436   Activity F     Insert        Emily          4999   \n",
       "\n",
       "       standardized_elapsed_time  \n",
       "0                      -0.051301  \n",
       "1                      -0.050813  \n",
       "2                      -0.050326  \n",
       "3                      -0.049350  \n",
       "4                      -0.048862  \n",
       "...                          ...  \n",
       "43432                  -0.046424  \n",
       "43433                  -0.045448  \n",
       "43434                  -0.044961  \n",
       "43435                  -0.043985  \n",
       "43436                  -0.043497  \n",
       "\n",
       "[43437 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Start & End markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert start and end markers\n",
    "def add_markers(df):\n",
    "    # Identify unique case indices\n",
    "    case_indices = df['@@case_index'].unique()\n",
    "    \n",
    "    # Prepare a container for new DataFrame rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each case index to add start and end markers\n",
    "    for case_index in case_indices:\n",
    "        # Create a start marker row with all columns except @@case_index set to 'Start'\n",
    "        start_row = {col: 'Start' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Create an end marker row with all columns except @@case_index set to 'End'\n",
    "        end_row = {col: 'End' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Append start row, rows for the current case, and end row\n",
    "        new_rows.append(start_row)\n",
    "        new_rows.extend(df[df['@@case_index'] == case_index].to_dict('records'))\n",
    "        new_rows.append(end_row)\n",
    "    \n",
    "    # Convert the list of rows into a DataFrame\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Apply the function to add start and end markers to the dataframe\n",
    "modified_dataframe = add_markers(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dataframe['standardized_elapsed_time'] = modified_dataframe['standardized_elapsed_time'].replace({'Start': 0, 'End': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['org:resource'])\n",
    "modified_dataframe['org:resource'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['concept:name'])\n",
    "modified_dataframe['concept:name'] = codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for Cases with less then 5 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each unique value in '@@case_index'\n",
    "frequency = modified_dataframe['@@case_index'].value_counts()\n",
    "\n",
    "# Finding the minimum occurrence\n",
    "min_occurrence = frequency.min()\n",
    "\n",
    "min_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resource = modified_dataframe[['org:resource', '@@case_index']]\n",
    "df_activity = modified_dataframe[['concept:name', '@@case_index']]\n",
    "df_timestamp = modified_dataframe[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df, case_id_column='@@case_index', window_size=5):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    # Iterate over each unique case\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        # Extract the case\n",
    "        case_data = df[df[case_id_column] == case_id]\n",
    "        \n",
    "        # Convert case_data to a NumPy array and drop the case_id_column\n",
    "        case_data_array = case_data.drop(columns=[case_id_column]).to_numpy()\n",
    "\n",
    "        # Adjusting the condition to correctly reflect window_size without needing an additional +1\n",
    "        # Now it correctly considers window_size as including the target event\n",
    "        if len(case_data_array) >= window_size:\n",
    "            # Adjust the loop to generate sliding windows of size window_size - 1 for the inputs and use the next event as the target\n",
    "            for i in range(len(case_data_array) - window_size + 1):\n",
    "                # window now has window_size - 1 events\n",
    "                window = case_data_array[i:i + window_size - 1]\n",
    "                # The target is the event immediately following the window\n",
    "                target = case_data_array[i + window_size - 1]\n",
    "                windows.append(window)\n",
    "                targets.append(target)\n",
    "                case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling and to ensure they are two-dimensional\n",
    "    windows_array = np.array(windows)\n",
    "    targets_array = np.array(targets)\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_array, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_resource, targets_resource, case_indices = generate_sliding_windows(df_resource)\n",
    "windows_activity, targets_activity, case_indices = generate_sliding_windows(df_activity)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_sliding_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 17:22:36.069862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_resource (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_activity (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 4, 50)                7050      ['input_resource[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 4, 50)                2050      ['input_activity[0][0]']      \n",
      "                                                                                                  \n",
      " input_timestamp (InputLaye  [(None, 4, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 25)                   7600      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 25)                   7600      ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 25)                   2700      ['input_timestamp[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 75)                   0         ['lstm[0][0]',                \n",
      "                                                                     'lstm_1[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_resource (Dense)     (None, 141)                  10716     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 41)                   3116      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    76        ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40908 (159.80 KB)\n",
      "Trainable params: 40908 (159.80 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding\n",
    "\n",
    "# Assuming these values as placeholders, replace them with actual counts from your data\n",
    "num_resources = modified_dataframe['org:resource'].nunique()\n",
    "num_activities = modified_dataframe['concept:name'].nunique()\n",
    "\n",
    "embedding_dim_resource = 50\n",
    "embedding_dim_activity = 50\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "# Input layers\n",
    "input_resource = Input(shape=(time_steps,), name='input_resource')\n",
    "input_activity = Input(shape=(time_steps,), name='input_activity')\n",
    "input_timestamp = Input(shape=(time_steps, 1), name='input_timestamp')\n",
    "\n",
    "# Embedding layers\n",
    "embedding_resource = Embedding(input_dim=num_resources, output_dim=embedding_dim_resource, input_length=time_steps)(input_resource)\n",
    "embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim_activity, input_length=time_steps)(input_activity)\n",
    "\n",
    "# LSTM layers\n",
    "lstm_resource = LSTM(25, return_sequences=False)(embedding_resource)\n",
    "lstm_activity = LSTM(25, return_sequences=False)(embedding_activity)\n",
    "lstm_timestamp = LSTM(25, return_sequences=False)(input_timestamp)\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate(axis=-1)([lstm_resource, lstm_activity, lstm_timestamp])\n",
    "\n",
    "# Output layers\n",
    "output_resource = Dense(num_resources, activation='softmax', name='output_resource')(concatenated)\n",
    "output_activity = Dense(num_activities, activation='softmax', name='output_activity')(concatenated)\n",
    "output_timestamp = Dense(1, activation='linear', name='output_timestamp')(concatenated)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[input_resource, input_activity, input_timestamp], \n",
    "              outputs=[output_resource, output_activity, output_timestamp])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'output_resource': 'categorical_crossentropy', \n",
    "                    'output_activity': 'categorical_crossentropy', \n",
    "                    'output_timestamp': 'mean_squared_error'},\n",
    "              metrics={'output_resource': 'accuracy', \n",
    "                       'output_activity': 'accuracy', \n",
    "                       'output_timestamp': 'mean_absolute_error'})\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resource data\n",
    "train_resource, test_resource, train_targets_resource, test_targets_resource = train_test_split(\n",
    "    windows_resource, targets_resource, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the activity data\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the timestamp data\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def cyclic_lr(epoch, lr):\n",
    "    # Example function that modulates LR within a range for each epoch\n",
    "    # Customize this function based on your cyclic learning rate policy\n",
    "    max_lr = 0.01  # Maximum LR\n",
    "    base_lr = 0.001  # Base LR\n",
    "    step_size = 10  # Number of epochs for half a cycle\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "366/366 [==============================] - 9s 9ms/step - loss: 5.4642 - output_resource_loss: 2.5249 - output_activity_loss: 1.7076 - output_timestamp_loss: 1.2317 - output_resource_accuracy: 0.4449 - output_activity_accuracy: 0.6278 - output_timestamp_mean_absolute_error: 0.2301 - val_loss: 2.9886 - val_output_resource_loss: 1.4961 - val_output_activity_loss: 0.7921 - val_output_timestamp_loss: 0.7004 - val_output_resource_accuracy: 0.6430 - val_output_activity_accuracy: 0.8441 - val_output_timestamp_mean_absolute_error: 0.1623 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 2.4103 - output_resource_loss: 1.1368 - output_activity_loss: 0.5062 - output_timestamp_loss: 0.7673 - output_resource_accuracy: 0.6954 - output_activity_accuracy: 0.9027 - output_timestamp_mean_absolute_error: 0.1704 - val_loss: 1.7952 - val_output_resource_loss: 1.0309 - val_output_activity_loss: 0.4129 - val_output_timestamp_loss: 0.3514 - val_output_resource_accuracy: 0.6972 - val_output_activity_accuracy: 0.9183 - val_output_timestamp_mean_absolute_error: 0.1234 - lr: 0.0019\n",
      "Epoch 3/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.8792 - output_resource_loss: 0.9468 - output_activity_loss: 0.3688 - output_timestamp_loss: 0.5636 - output_resource_accuracy: 0.7169 - output_activity_accuracy: 0.9283 - output_timestamp_mean_absolute_error: 0.1622 - val_loss: 1.6153 - val_output_resource_loss: 0.9595 - val_output_activity_loss: 0.3601 - val_output_timestamp_loss: 0.2957 - val_output_resource_accuracy: 0.7198 - val_output_activity_accuracy: 0.9288 - val_output_timestamp_mean_absolute_error: 0.1284 - lr: 0.0028\n",
      "Epoch 4/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.7553 - output_resource_loss: 0.8966 - output_activity_loss: 0.3343 - output_timestamp_loss: 0.5244 - output_resource_accuracy: 0.7191 - output_activity_accuracy: 0.9336 - output_timestamp_mean_absolute_error: 0.1546 - val_loss: 1.5535 - val_output_resource_loss: 0.9300 - val_output_activity_loss: 0.3422 - val_output_timestamp_loss: 0.2813 - val_output_resource_accuracy: 0.7229 - val_output_activity_accuracy: 0.9333 - val_output_timestamp_mean_absolute_error: 0.0958 - lr: 0.0037\n",
      "Epoch 5/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.6975 - output_resource_loss: 0.8710 - output_activity_loss: 0.3131 - output_timestamp_loss: 0.5135 - output_resource_accuracy: 0.7239 - output_activity_accuracy: 0.9371 - output_timestamp_mean_absolute_error: 0.1438 - val_loss: 1.5466 - val_output_resource_loss: 0.9157 - val_output_activity_loss: 0.3242 - val_output_timestamp_loss: 0.3067 - val_output_resource_accuracy: 0.7248 - val_output_activity_accuracy: 0.9342 - val_output_timestamp_mean_absolute_error: 0.2037 - lr: 0.0046\n",
      "Epoch 6/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.6730 - output_resource_loss: 0.8576 - output_activity_loss: 0.3029 - output_timestamp_loss: 0.5126 - output_resource_accuracy: 0.7238 - output_activity_accuracy: 0.9386 - output_timestamp_mean_absolute_error: 0.1458 - val_loss: 1.5278 - val_output_resource_loss: 0.9156 - val_output_activity_loss: 0.3235 - val_output_timestamp_loss: 0.2887 - val_output_resource_accuracy: 0.7116 - val_output_activity_accuracy: 0.9360 - val_output_timestamp_mean_absolute_error: 0.1321 - lr: 0.0055\n",
      "Epoch 7/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.6519 - output_resource_loss: 0.8474 - output_activity_loss: 0.2953 - output_timestamp_loss: 0.5092 - output_resource_accuracy: 0.7244 - output_activity_accuracy: 0.9394 - output_timestamp_mean_absolute_error: 0.1412 - val_loss: 1.5079 - val_output_resource_loss: 0.9001 - val_output_activity_loss: 0.3189 - val_output_timestamp_loss: 0.2888 - val_output_resource_accuracy: 0.7234 - val_output_activity_accuracy: 0.9382 - val_output_timestamp_mean_absolute_error: 0.1317 - lr: 0.0064\n",
      "Epoch 8/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.6298 - output_resource_loss: 0.8345 - output_activity_loss: 0.2886 - output_timestamp_loss: 0.5068 - output_resource_accuracy: 0.7251 - output_activity_accuracy: 0.9412 - output_timestamp_mean_absolute_error: 0.1325 - val_loss: 1.5061 - val_output_resource_loss: 0.9056 - val_output_activity_loss: 0.3150 - val_output_timestamp_loss: 0.2854 - val_output_resource_accuracy: 0.7201 - val_output_activity_accuracy: 0.9378 - val_output_timestamp_mean_absolute_error: 0.1092 - lr: 0.0073\n",
      "Epoch 9/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.6213 - output_resource_loss: 0.8287 - output_activity_loss: 0.2850 - output_timestamp_loss: 0.5076 - output_resource_accuracy: 0.7269 - output_activity_accuracy: 0.9414 - output_timestamp_mean_absolute_error: 0.1324 - val_loss: 1.5075 - val_output_resource_loss: 0.9101 - val_output_activity_loss: 0.3171 - val_output_timestamp_loss: 0.2803 - val_output_resource_accuracy: 0.7182 - val_output_activity_accuracy: 0.9250 - val_output_timestamp_mean_absolute_error: 0.0765 - lr: 0.0082\n",
      "Epoch 10/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.5974 - output_resource_loss: 0.8189 - output_activity_loss: 0.2776 - output_timestamp_loss: 0.5009 - output_resource_accuracy: 0.7285 - output_activity_accuracy: 0.9423 - output_timestamp_mean_absolute_error: 0.1307 - val_loss: 1.5266 - val_output_resource_loss: 0.9178 - val_output_activity_loss: 0.3233 - val_output_timestamp_loss: 0.2854 - val_output_resource_accuracy: 0.7125 - val_output_activity_accuracy: 0.9340 - val_output_timestamp_mean_absolute_error: 0.1357 - lr: 0.0091\n",
      "Epoch 11/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.5970 - output_resource_loss: 0.8174 - output_activity_loss: 0.2769 - output_timestamp_loss: 0.5028 - output_resource_accuracy: 0.7241 - output_activity_accuracy: 0.9424 - output_timestamp_mean_absolute_error: 0.1345 - val_loss: 1.5170 - val_output_resource_loss: 0.9188 - val_output_activity_loss: 0.3210 - val_output_timestamp_loss: 0.2772 - val_output_resource_accuracy: 0.7232 - val_output_activity_accuracy: 0.9390 - val_output_timestamp_mean_absolute_error: 0.0705 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.5583 - output_resource_loss: 0.7997 - output_activity_loss: 0.2681 - output_timestamp_loss: 0.4905 - output_resource_accuracy: 0.7295 - output_activity_accuracy: 0.9427 - output_timestamp_mean_absolute_error: 0.1184 - val_loss: 1.5201 - val_output_resource_loss: 0.9157 - val_output_activity_loss: 0.3153 - val_output_timestamp_loss: 0.2892 - val_output_resource_accuracy: 0.7186 - val_output_activity_accuracy: 0.9374 - val_output_timestamp_mean_absolute_error: 0.1023 - lr: 0.0091\n",
      "Epoch 13/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.5435 - output_resource_loss: 0.7839 - output_activity_loss: 0.2595 - output_timestamp_loss: 0.5001 - output_resource_accuracy: 0.7327 - output_activity_accuracy: 0.9443 - output_timestamp_mean_absolute_error: 0.1316 - val_loss: 1.5088 - val_output_resource_loss: 0.9189 - val_output_activity_loss: 0.3133 - val_output_timestamp_loss: 0.2766 - val_output_resource_accuracy: 0.7263 - val_output_activity_accuracy: 0.9388 - val_output_timestamp_mean_absolute_error: 0.0826 - lr: 0.0082\n",
      "Epoch 14/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.5170 - output_resource_loss: 0.7672 - output_activity_loss: 0.2537 - output_timestamp_loss: 0.4961 - output_resource_accuracy: 0.7377 - output_activity_accuracy: 0.9462 - output_timestamp_mean_absolute_error: 0.1191 - val_loss: 1.5291 - val_output_resource_loss: 0.9295 - val_output_activity_loss: 0.3173 - val_output_timestamp_loss: 0.2823 - val_output_resource_accuracy: 0.7112 - val_output_activity_accuracy: 0.9369 - val_output_timestamp_mean_absolute_error: 0.1008 - lr: 0.0073\n",
      "Epoch 15/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.4898 - output_resource_loss: 0.7543 - output_activity_loss: 0.2471 - output_timestamp_loss: 0.4884 - output_resource_accuracy: 0.7385 - output_activity_accuracy: 0.9470 - output_timestamp_mean_absolute_error: 0.1176 - val_loss: 1.5303 - val_output_resource_loss: 0.9340 - val_output_activity_loss: 0.3172 - val_output_timestamp_loss: 0.2791 - val_output_resource_accuracy: 0.7121 - val_output_activity_accuracy: 0.9379 - val_output_timestamp_mean_absolute_error: 0.0807 - lr: 0.0064\n",
      "Epoch 16/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.4706 - output_resource_loss: 0.7415 - output_activity_loss: 0.2421 - output_timestamp_loss: 0.4870 - output_resource_accuracy: 0.7437 - output_activity_accuracy: 0.9479 - output_timestamp_mean_absolute_error: 0.1127 - val_loss: 1.5404 - val_output_resource_loss: 0.9368 - val_output_activity_loss: 0.3206 - val_output_timestamp_loss: 0.2830 - val_output_resource_accuracy: 0.7232 - val_output_activity_accuracy: 0.9384 - val_output_timestamp_mean_absolute_error: 0.1140 - lr: 0.0055\n",
      "Epoch 17/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.4518 - output_resource_loss: 0.7311 - output_activity_loss: 0.2367 - output_timestamp_loss: 0.4839 - output_resource_accuracy: 0.7462 - output_activity_accuracy: 0.9486 - output_timestamp_mean_absolute_error: 0.1136 - val_loss: 1.5246 - val_output_resource_loss: 0.9299 - val_output_activity_loss: 0.3156 - val_output_timestamp_loss: 0.2791 - val_output_resource_accuracy: 0.7141 - val_output_activity_accuracy: 0.9384 - val_output_timestamp_mean_absolute_error: 0.0723 - lr: 0.0046\n",
      "Epoch 18/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.4306 - output_resource_loss: 0.7194 - output_activity_loss: 0.2312 - output_timestamp_loss: 0.4800 - output_resource_accuracy: 0.7473 - output_activity_accuracy: 0.9500 - output_timestamp_mean_absolute_error: 0.1043 - val_loss: 1.5392 - val_output_resource_loss: 0.9336 - val_output_activity_loss: 0.3152 - val_output_timestamp_loss: 0.2905 - val_output_resource_accuracy: 0.7168 - val_output_activity_accuracy: 0.9354 - val_output_timestamp_mean_absolute_error: 0.1095 - lr: 0.0037\n",
      "Epoch 19/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.4162 - output_resource_loss: 0.7100 - output_activity_loss: 0.2275 - output_timestamp_loss: 0.4786 - output_resource_accuracy: 0.7509 - output_activity_accuracy: 0.9510 - output_timestamp_mean_absolute_error: 0.1014 - val_loss: 1.5280 - val_output_resource_loss: 0.9312 - val_output_activity_loss: 0.3156 - val_output_timestamp_loss: 0.2812 - val_output_resource_accuracy: 0.7166 - val_output_activity_accuracy: 0.9366 - val_output_timestamp_mean_absolute_error: 0.0802 - lr: 0.0028\n",
      "Epoch 20/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.4005 - output_resource_loss: 0.7005 - output_activity_loss: 0.2232 - output_timestamp_loss: 0.4768 - output_resource_accuracy: 0.7539 - output_activity_accuracy: 0.9520 - output_timestamp_mean_absolute_error: 0.1013 - val_loss: 1.5308 - val_output_resource_loss: 0.9331 - val_output_activity_loss: 0.3158 - val_output_timestamp_loss: 0.2819 - val_output_resource_accuracy: 0.7232 - val_output_activity_accuracy: 0.9376 - val_output_timestamp_mean_absolute_error: 0.0846 - lr: 0.0019\n",
      "Epoch 21/25\n",
      "366/366 [==============================] - 2s 5ms/step - loss: 1.3815 - output_resource_loss: 0.6906 - output_activity_loss: 0.2194 - output_timestamp_loss: 0.4714 - output_resource_accuracy: 0.7584 - output_activity_accuracy: 0.9524 - output_timestamp_mean_absolute_error: 0.0963 - val_loss: 1.5292 - val_output_resource_loss: 0.9346 - val_output_activity_loss: 0.3149 - val_output_timestamp_loss: 0.2797 - val_output_resource_accuracy: 0.7236 - val_output_activity_accuracy: 0.9371 - val_output_timestamp_mean_absolute_error: 0.0747 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "366/366 [==============================] - 2s 6ms/step - loss: 1.3910 - output_resource_loss: 0.6961 - output_activity_loss: 0.2220 - output_timestamp_loss: 0.4729 - output_resource_accuracy: 0.7577 - output_activity_accuracy: 0.9523 - output_timestamp_mean_absolute_error: 0.1036 - val_loss: 1.5340 - val_output_resource_loss: 0.9360 - val_output_activity_loss: 0.3165 - val_output_timestamp_loss: 0.2815 - val_output_resource_accuracy: 0.7230 - val_output_activity_accuracy: 0.9367 - val_output_timestamp_mean_absolute_error: 0.0775 - lr: 0.0019\n",
      "Epoch 23/25\n",
      "366/366 [==============================] - 2s 7ms/step - loss: 1.4026 - output_resource_loss: 0.7004 - output_activity_loss: 0.2249 - output_timestamp_loss: 0.4773 - output_resource_accuracy: 0.7541 - output_activity_accuracy: 0.9516 - output_timestamp_mean_absolute_error: 0.1067 - val_loss: 1.5535 - val_output_resource_loss: 0.9457 - val_output_activity_loss: 0.3186 - val_output_timestamp_loss: 0.2892 - val_output_resource_accuracy: 0.7156 - val_output_activity_accuracy: 0.9376 - val_output_timestamp_mean_absolute_error: 0.1230 - lr: 0.0028\n",
      "Epoch 24/25\n",
      "366/366 [==============================] - 2s 7ms/step - loss: 1.4121 - output_resource_loss: 0.7062 - output_activity_loss: 0.2270 - output_timestamp_loss: 0.4790 - output_resource_accuracy: 0.7512 - output_activity_accuracy: 0.9516 - output_timestamp_mean_absolute_error: 0.1148 - val_loss: 1.5683 - val_output_resource_loss: 0.9537 - val_output_activity_loss: 0.3240 - val_output_timestamp_loss: 0.2905 - val_output_resource_accuracy: 0.7175 - val_output_activity_accuracy: 0.9351 - val_output_timestamp_mean_absolute_error: 0.1253 - lr: 0.0037\n",
      "Epoch 25/25\n",
      "366/366 [==============================] - 3s 8ms/step - loss: 1.4220 - output_resource_loss: 0.7109 - output_activity_loss: 0.2299 - output_timestamp_loss: 0.4811 - output_resource_accuracy: 0.7486 - output_activity_accuracy: 0.9507 - output_timestamp_mean_absolute_error: 0.1177 - val_loss: 1.5638 - val_output_resource_loss: 0.9531 - val_output_activity_loss: 0.3246 - val_output_timestamp_loss: 0.2861 - val_output_resource_accuracy: 0.7149 - val_output_activity_accuracy: 0.9349 - val_output_timestamp_mean_absolute_error: 0.1114 - lr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_targets_resource_cat = to_categorical(train_targets_resource, num_classes=num_resources)\n",
    "test_targets_resource_cat = to_categorical(test_targets_resource, num_classes=num_resources)\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "\n",
    "# Note: Assuming timestamp targets are continuous and don't need to be converted to categorical\n",
    "\n",
    "history = model.fit([train_resource, train_activity, train_timestamp], \n",
    "                    [train_targets_resource_cat, train_targets_activity_cat, train_targets_timestamp],\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=([test_resource, test_activity, test_timestamp], \n",
    "                                     [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]),\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_scheduler])  # Add other callbacks as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 1s 3ms/step - loss: 1.5638 - output_resource_loss: 0.9531 - output_activity_loss: 0.3246 - output_timestamp_loss: 0.2861 - output_resource_accuracy: 0.7149 - output_activity_accuracy: 0.9349 - output_timestamp_mean_absolute_error: 0.1114\n",
      "[1.5637997388839722, 0.9531036615371704, 0.32456058263778687, 0.286135196685791, 0.7149122953414917, 0.9349082708358765, 0.11135195940732956]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(\n",
    "    [test_resource, test_activity, test_timestamp],\n",
    "    [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to an H5 file\n",
    "model.save('dapnn_small.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045/1045 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_resource, windows_activity, windows_timestamp])\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_resource = predictions[0]        # ID predictions\n",
    "predictions_activity = predictions[1]  # Resource predictions\n",
    "\n",
    "# Extract predictions for numerical attribute (timestamp)\n",
    "predictions_timestamp = predictions[2] # Timestamp predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def compute_anomaly_scores(predictions, actuals):\n",
    "    # For categorical predictions, convert actuals to one-hot for comparison\n",
    "    actuals_one_hot = to_categorical(actuals, num_classes=predictions.shape[-1])\n",
    "    \n",
    "    max_predictions = np.max(predictions, axis=-1)\n",
    "    actual_predictions = np.sum(predictions * actuals_one_hot, axis=-1)  # Extract the probability of the actual class\n",
    "    \n",
    "    anomaly_scores = (max_predictions - actual_predictions) / max_predictions\n",
    "    \n",
    "    return anomaly_scores\n",
    "\n",
    "# Assuming targets_id, targets_resource, targets_activity, targets_role are the true values for these attributes\n",
    "anomaly_scores_resource = compute_anomaly_scores(predictions_resource, targets_resource)\n",
    "anomaly_scores_activity = compute_anomaly_scores(predictions_activity, targets_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals, normalization_factor):\n",
    "    \"\"\"\n",
    "    Compute anomaly scores for continuous attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predicted values.\n",
    "    - actuals: numpy array of actual values.\n",
    "    - normalization_factor: normalization factor (e.g., standard deviation of the attribute).\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of anomaly scores.\n",
    "    \"\"\"\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize the differences\n",
    "    anomaly_scores = differences / normalization_factor\n",
    "    \n",
    "    return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_factor = np.std(targets_timestamp)       # Example normalization factor (standard deviation)\n",
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp, normalization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_resource = np.array(anomaly_scores_resource).flatten()\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "    anomaly_scores_timestamp = np.array(anomaly_scores_timestamp).flatten()\n",
    "\n",
    "    # Check if all arrays have the same length\n",
    "    if not (len(anomaly_scores_resource) == len(anomaly_scores_activity) == len(anomaly_scores_timestamp)):\n",
    "        raise ValueError(\"All input anomaly scores must have the same length.\")\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_activity, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_activity])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True: anomaly, False: no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33432</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33433</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33434</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33435</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33436</th>\n",
       "      <td>4999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted\n",
       "0         0      False\n",
       "1         0      False\n",
       "2         0      False\n",
       "3         0      False\n",
       "4         0      False\n",
       "...     ...        ...\n",
       "33432  4999      False\n",
       "33433  4999      False\n",
       "33434  4999      False\n",
       "33435  4999      False\n",
       "33436  4999      False\n",
       "\n",
       "[33437 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted'] = anomalous_cases\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1        True\n",
       "2        True\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "4995    False\n",
       "4996    False\n",
       "4997    False\n",
       "4998    False\n",
       "4999    False\n",
       "Name: predicted, Length: 5000, dtype: bool"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_prediction = mapping.groupby('case')['predicted'].any()\n",
    "case_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a01db8c9804446b4094cde2af72dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/small.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(1)\n",
    "        else:\n",
    "            conformance_status.append(0)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              1          1\n",
       "1              0          0\n",
       "2              0          0\n",
       "3              1          1\n",
       "4              1          1\n",
       "...          ...        ...\n",
       "4995           1          1\n",
       "4996           1          1\n",
       "4997           1          1\n",
       "4998           1          1\n",
       "4999           0          1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth['predicted'] = 1 - ground_truth['predicted']\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.935\n"
     ]
    }
   ],
   "source": [
    "# Calculate f1\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.914\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = TN / (TN + FN)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.595\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = TN / (TN + FP)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.891\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.983\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789341104953138"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming ground_truth is your DataFrame\n",
    "# Make sure 'conformity' contains actual labels (0 or 1)\n",
    "# and 'predicted' contains predicted probabilities or scores\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "auc_roc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
