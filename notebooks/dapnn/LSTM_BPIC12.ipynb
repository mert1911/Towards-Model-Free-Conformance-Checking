{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_2445/1717955873.py:14: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/Model_A.csv', sep=',')\n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:REG_DATE</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>case:AMOUNT_REQ</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>173688</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.880000+00:00</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>173688</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>2011-09-30 22:39:37.906000+00:00</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>173688</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10862</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_ACCEPTED</td>\n",
       "      <td>2011-10-01 09:42:43.308000+00:00</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>173688</td>\n",
       "      <td>20000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10862</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_FINALIZED</td>\n",
       "      <td>2011-10-01 09:45:09.243000+00:00</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>173688</td>\n",
       "      <td>20000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60844</th>\n",
       "      <td>10933</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_ACCEPTED</td>\n",
       "      <td>2012-03-01 19:17:22.457000+00:00</td>\n",
       "      <td>2012-02-29 22:43:09.766000+00:00</td>\n",
       "      <td>214373</td>\n",
       "      <td>8500</td>\n",
       "      <td>60844</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60845</th>\n",
       "      <td>10933</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_FINALIZED</td>\n",
       "      <td>2012-03-01 19:22:38.593000+00:00</td>\n",
       "      <td>2012-02-29 22:43:09.766000+00:00</td>\n",
       "      <td>214373</td>\n",
       "      <td>8500</td>\n",
       "      <td>60845</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60846</th>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2012-02-29 22:51:16.799000+00:00</td>\n",
       "      <td>2012-02-29 22:51:16.799000+00:00</td>\n",
       "      <td>214376</td>\n",
       "      <td>15000</td>\n",
       "      <td>60846</td>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60847</th>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2012-02-29 22:51:17.423000+00:00</td>\n",
       "      <td>2012-02-29 22:51:16.799000+00:00</td>\n",
       "      <td>214376</td>\n",
       "      <td>15000</td>\n",
       "      <td>60847</td>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60848</th>\n",
       "      <td>11169</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>2012-03-01 08:27:37.118000+00:00</td>\n",
       "      <td>2012-02-29 22:51:16.799000+00:00</td>\n",
       "      <td>214376</td>\n",
       "      <td>15000</td>\n",
       "      <td>60848</td>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60849 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource lifecycle:transition       concept:name  \\\n",
       "0               112             COMPLETE        A_SUBMITTED   \n",
       "1               112             COMPLETE  A_PARTLYSUBMITTED   \n",
       "2               112             COMPLETE      A_PREACCEPTED   \n",
       "3             10862             COMPLETE         A_ACCEPTED   \n",
       "4             10862             COMPLETE        A_FINALIZED   \n",
       "...             ...                  ...                ...   \n",
       "60844         10933             COMPLETE         A_ACCEPTED   \n",
       "60845         10933             COMPLETE        A_FINALIZED   \n",
       "60846           112             COMPLETE        A_SUBMITTED   \n",
       "60847           112             COMPLETE  A_PARTLYSUBMITTED   \n",
       "60848         11169             COMPLETE         A_DECLINED   \n",
       "\n",
       "                        time:timestamp                    case:REG_DATE  \\\n",
       "0     2011-09-30 22:38:44.546000+00:00 2011-09-30 22:38:44.546000+00:00   \n",
       "1     2011-09-30 22:38:44.880000+00:00 2011-09-30 22:38:44.546000+00:00   \n",
       "2     2011-09-30 22:39:37.906000+00:00 2011-09-30 22:38:44.546000+00:00   \n",
       "3     2011-10-01 09:42:43.308000+00:00 2011-09-30 22:38:44.546000+00:00   \n",
       "4     2011-10-01 09:45:09.243000+00:00 2011-09-30 22:38:44.546000+00:00   \n",
       "...                                ...                              ...   \n",
       "60844 2012-03-01 19:17:22.457000+00:00 2012-02-29 22:43:09.766000+00:00   \n",
       "60845 2012-03-01 19:22:38.593000+00:00 2012-02-29 22:43:09.766000+00:00   \n",
       "60846 2012-02-29 22:51:16.799000+00:00 2012-02-29 22:51:16.799000+00:00   \n",
       "60847 2012-02-29 22:51:17.423000+00:00 2012-02-29 22:51:16.799000+00:00   \n",
       "60848 2012-03-01 08:27:37.118000+00:00 2012-02-29 22:51:16.799000+00:00   \n",
       "\n",
       "      case:concept:name  case:AMOUNT_REQ  @@index  @@case_index  \n",
       "0                173688            20000        0             0  \n",
       "1                173688            20000        1             0  \n",
       "2                173688            20000        2             0  \n",
       "3                173688            20000        3             0  \n",
       "4                173688            20000        4             0  \n",
       "...                 ...              ...      ...           ...  \n",
       "60844            214373             8500    60844         13085  \n",
       "60845            214373             8500    60845         13085  \n",
       "60846            214376            15000    60846         13086  \n",
       "60847            214376            15000    60847         13086  \n",
       "60848            214376            15000    60848         13086  \n",
       "\n",
       "[60849 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['@@index'])\n",
    "dataframe_log = dataframe_log.drop(columns=['lifecycle:transition'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:REG_DATE'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])\n",
    "dataframe_log = dataframe_log.drop(columns=['case:AMOUNT_REQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.880000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>2011-09-30 22:39:37.906000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10862</td>\n",
       "      <td>A_ACCEPTED</td>\n",
       "      <td>2011-10-01 09:42:43.308000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10862</td>\n",
       "      <td>A_FINALIZED</td>\n",
       "      <td>2011-10-01 09:45:09.243000+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60844</th>\n",
       "      <td>10933</td>\n",
       "      <td>A_ACCEPTED</td>\n",
       "      <td>2012-03-01 19:17:22.457000+00:00</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60845</th>\n",
       "      <td>10933</td>\n",
       "      <td>A_FINALIZED</td>\n",
       "      <td>2012-03-01 19:22:38.593000+00:00</td>\n",
       "      <td>13085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60846</th>\n",
       "      <td>112</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2012-02-29 22:51:16.799000+00:00</td>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60847</th>\n",
       "      <td>112</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2012-02-29 22:51:17.423000+00:00</td>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60848</th>\n",
       "      <td>11169</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>2012-03-01 08:27:37.118000+00:00</td>\n",
       "      <td>13086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60849 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource       concept:name                   time:timestamp  \\\n",
       "0               112        A_SUBMITTED 2011-09-30 22:38:44.546000+00:00   \n",
       "1               112  A_PARTLYSUBMITTED 2011-09-30 22:38:44.880000+00:00   \n",
       "2               112      A_PREACCEPTED 2011-09-30 22:39:37.906000+00:00   \n",
       "3             10862         A_ACCEPTED 2011-10-01 09:42:43.308000+00:00   \n",
       "4             10862        A_FINALIZED 2011-10-01 09:45:09.243000+00:00   \n",
       "...             ...                ...                              ...   \n",
       "60844         10933         A_ACCEPTED 2012-03-01 19:17:22.457000+00:00   \n",
       "60845         10933        A_FINALIZED 2012-03-01 19:22:38.593000+00:00   \n",
       "60846           112        A_SUBMITTED 2012-02-29 22:51:16.799000+00:00   \n",
       "60847           112  A_PARTLYSUBMITTED 2012-02-29 22:51:17.423000+00:00   \n",
       "60848         11169         A_DECLINED 2012-03-01 08:27:37.118000+00:00   \n",
       "\n",
       "       @@case_index  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "60844         13085  \n",
       "60845         13085  \n",
       "60846         13086  \n",
       "60847         13086  \n",
       "60848         13086  \n",
       "\n",
       "[60849 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10862</td>\n",
       "      <td>A_ACCEPTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.335503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10862</td>\n",
       "      <td>A_FINALIZED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.335289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60844</th>\n",
       "      <td>10933</td>\n",
       "      <td>A_ACCEPTED</td>\n",
       "      <td>13085</td>\n",
       "      <td>-0.285350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60845</th>\n",
       "      <td>10933</td>\n",
       "      <td>A_FINALIZED</td>\n",
       "      <td>13085</td>\n",
       "      <td>-0.284887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60846</th>\n",
       "      <td>112</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>13086</td>\n",
       "      <td>-0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60847</th>\n",
       "      <td>112</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>13086</td>\n",
       "      <td>-0.393899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60848</th>\n",
       "      <td>11169</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>13086</td>\n",
       "      <td>-0.343211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60849 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource       concept:name  @@case_index  \\\n",
       "0               112        A_SUBMITTED             0   \n",
       "1               112  A_PARTLYSUBMITTED             0   \n",
       "2               112      A_PREACCEPTED             0   \n",
       "3             10862         A_ACCEPTED             0   \n",
       "4             10862        A_FINALIZED             0   \n",
       "...             ...                ...           ...   \n",
       "60844         10933         A_ACCEPTED         13085   \n",
       "60845         10933        A_FINALIZED         13085   \n",
       "60846           112        A_SUBMITTED         13086   \n",
       "60847           112  A_PARTLYSUBMITTED         13086   \n",
       "60848         11169         A_DECLINED         13086   \n",
       "\n",
       "       standardized_elapsed_time  \n",
       "0                      -0.393900  \n",
       "1                      -0.393899  \n",
       "2                      -0.393822  \n",
       "3                      -0.335503  \n",
       "4                      -0.335289  \n",
       "...                          ...  \n",
       "60844                  -0.285350  \n",
       "60845                  -0.284887  \n",
       "60846                  -0.393900  \n",
       "60847                  -0.393899  \n",
       "60848                  -0.343211  \n",
       "\n",
       "[60849 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Start & End markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert start and end markers\n",
    "def add_markers(df):\n",
    "    # Identify unique case indices\n",
    "    case_indices = df['@@case_index'].unique()\n",
    "    \n",
    "    # Prepare a container for new DataFrame rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each case index to add start and end markers\n",
    "    for case_index in case_indices:\n",
    "        # Create a start marker row with all columns except @@case_index set to 'Start'\n",
    "        start_row = {col: 'Start' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Create an end marker row with all columns except @@case_index set to 'End'\n",
    "        end_row = {col: 'End' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Append start row, rows for the current case, and end row\n",
    "        new_rows.append(start_row)\n",
    "        new_rows.extend(df[df['@@case_index'] == case_index].to_dict('records'))\n",
    "        new_rows.append(end_row)\n",
    "    \n",
    "    # Convert the list of rows into a DataFrame\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Apply the function to add start and end markers to the dataframe\n",
    "modified_dataframe = add_markers(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dataframe['org:resource'] = modified_dataframe['org:resource'].replace({'Start': 0, 'End': 1})\n",
    "modified_dataframe['standardized_elapsed_time'] = modified_dataframe['standardized_elapsed_time'].replace({'Start': 0, 'End': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['concept:name'])\n",
    "modified_dataframe['concept:name'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['org:resource'])\n",
    "modified_dataframe['org:resource'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.335503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13086</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87019</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13086</td>\n",
       "      <td>-0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87020</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13086</td>\n",
       "      <td>-0.393899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87021</th>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>13086</td>\n",
       "      <td>-0.343211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87022</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13086</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87023 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource  concept:name  @@case_index  standardized_elapsed_time\n",
       "0                 0             0             0                   0.000000\n",
       "1                 1             1             0                  -0.393900\n",
       "2                 1             2             0                  -0.393899\n",
       "3                 1             3             0                  -0.393822\n",
       "4                 2             4             0                  -0.335503\n",
       "...             ...           ...           ...                        ...\n",
       "87018             0             0         13086                   0.000000\n",
       "87019             1             1         13086                  -0.393900\n",
       "87020             1             2         13086                  -0.393899\n",
       "87021            31            10         13086                  -0.343211\n",
       "87022             4             9         13086                   1.000000\n",
       "\n",
       "[87023 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for Cases with less then 5 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each unique value in '@@case_index'\n",
    "frequency = modified_dataframe['@@case_index'].value_counts()\n",
    "\n",
    "# Finding the minimum occurrence\n",
    "min_occurrence = frequency.min()\n",
    "\n",
    "min_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Assuming your dataframe might have different types, let\\'s create a generic function to add rows\\ndef add_rows(group, num_rows, case_index_value):\\n    # For each column, determine the appropriate \"zero\" value (int 0, string \\'\\', etc.)\\n    additional_rows = pd.DataFrame({\\n        column: 0 if pd.api.types.is_numeric_dtype(group[column]) else \\'\\' for column in group.columns\\n    }, index=range(num_rows))\\n    \\n    # Set the @@case_index column to the current case index value\\n    additional_rows[\\'@@case_index\\'] = case_index_value\\n    \\n    # Append the additional rows to the group\\n    return pd.concat([group, additional_rows], ignore_index=True)\\n\\n# Function to pad cases with less than 5 events\\ndef pad_cases(df):\\n    # Group by @@case_index\\n    groups = df.groupby(\\'@@case_index\\')\\n    \\n    # Placeholder for modified groups\\n    modified_groups = []\\n    \\n    for name, group in groups:\\n        # Calculate the number of events to add\\n        events_to_add = 5 - len(group)\\n        \\n        if events_to_add > 0:\\n            # Add the required number of rows\\n            group = add_rows(group, events_to_add, name)\\n        \\n        # Append the modified group to the list\\n        modified_groups.append(group)\\n    \\n    # Concatenate all modified groups back into a single DataFrame\\n    return pd.concat(modified_groups, ignore_index=True)  '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Assuming your dataframe might have different types, let's create a generic function to add rows\n",
    "def add_rows(group, num_rows, case_index_value):\n",
    "    # For each column, determine the appropriate \"zero\" value (int 0, string '', etc.)\n",
    "    additional_rows = pd.DataFrame({\n",
    "        column: 0 if pd.api.types.is_numeric_dtype(group[column]) else '' for column in group.columns\n",
    "    }, index=range(num_rows))\n",
    "    \n",
    "    # Set the @@case_index column to the current case index value\n",
    "    additional_rows['@@case_index'] = case_index_value\n",
    "    \n",
    "    # Append the additional rows to the group\n",
    "    return pd.concat([group, additional_rows], ignore_index=True)\n",
    "\n",
    "# Function to pad cases with less than 5 events\n",
    "def pad_cases(df):\n",
    "    # Group by @@case_index\n",
    "    groups = df.groupby('@@case_index')\n",
    "    \n",
    "    # Placeholder for modified groups\n",
    "    modified_groups = []\n",
    "    \n",
    "    for name, group in groups:\n",
    "        # Calculate the number of events to add\n",
    "        events_to_add = 5 - len(group)\n",
    "        \n",
    "        if events_to_add > 0:\n",
    "            # Add the required number of rows\n",
    "            group = add_rows(group, events_to_add, name)\n",
    "        \n",
    "        # Append the modified group to the list\n",
    "        modified_groups.append(group)\n",
    "    \n",
    "    # Concatenate all modified groups back into a single DataFrame\n",
    "    return pd.concat(modified_groups, ignore_index=True)  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Apply the padding function\\nmodified_dataframe = pad_cases(modified_dataframe)\\nmodified_dataframe '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Apply the padding function\n",
    "modified_dataframe = pad_cases(modified_dataframe)\n",
    "modified_dataframe \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resource = modified_dataframe[['org:resource', '@@case_index']]\n",
    "df_activity = modified_dataframe[['concept:name', '@@case_index']]\n",
    "df_timestamp = modified_dataframe[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df, case_id_column='@@case_index', window_size=5):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    # Iterate over each unique case\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        # Extract the case\n",
    "        case_data = df[df[case_id_column] == case_id]\n",
    "        \n",
    "        # Convert case_data to a NumPy array and drop the case_id_column\n",
    "        case_data_array = case_data.drop(columns=[case_id_column]).to_numpy()\n",
    "\n",
    "        # Adjusting the condition to correctly reflect window_size without needing an additional +1\n",
    "        # Now it correctly considers window_size as including the target event\n",
    "        if len(case_data_array) >= window_size:\n",
    "            # Adjust the loop to generate sliding windows of size window_size - 1 for the inputs and use the next event as the target\n",
    "            for i in range(len(case_data_array) - window_size + 1):\n",
    "                # window now has window_size - 1 events\n",
    "                window = case_data_array[i:i + window_size - 1]\n",
    "                # The target is the event immediately following the window\n",
    "                target = case_data_array[i + window_size - 1]\n",
    "                windows.append(window)\n",
    "                targets.append(target)\n",
    "                case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling and to ensure they are two-dimensional\n",
    "    windows_array = np.array(windows)\n",
    "    targets_array = np.array(targets)\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_array, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_resource, targets_resource, case_indices = generate_sliding_windows(df_resource)\n",
    "windows_activity, targets_activity, case_indices = generate_sliding_windows(df_activity)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_sliding_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 13:36:47.602343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_resource (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_activity (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 4, 50)                3150      ['input_resource[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 4, 50)                600       ['input_activity[0][0]']      \n",
      "                                                                                                  \n",
      " input_timestamp (InputLaye  [(None, 4, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 25)                   7600      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 25)                   7600      ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 25)                   2700      ['input_timestamp[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 75)                   0         ['lstm[0][0]',                \n",
      "                                                                     'lstm_1[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_resource (Dense)     (None, 63)                   4788      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 12)                   912       ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    76        ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27426 (107.13 KB)\n",
      "Trainable params: 27426 (107.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding\n",
    "\n",
    "# Assuming these values as placeholders, replace them with actual counts from your data\n",
    "num_resources = modified_dataframe['org:resource'].nunique()\n",
    "num_activities = modified_dataframe['concept:name'].nunique()\n",
    "\n",
    "embedding_dim_resource = 50\n",
    "embedding_dim_activity = 50\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "# Input layers\n",
    "input_resource = Input(shape=(time_steps,), name='input_resource')\n",
    "input_activity = Input(shape=(time_steps,), name='input_activity')\n",
    "input_timestamp = Input(shape=(time_steps, 1), name='input_timestamp')\n",
    "\n",
    "# Embedding layers\n",
    "embedding_resource = Embedding(input_dim=num_resources, output_dim=embedding_dim_resource, input_length=time_steps)(input_resource)\n",
    "embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim_activity, input_length=time_steps)(input_activity)\n",
    "\n",
    "# LSTM layers\n",
    "lstm_resource = LSTM(25, return_sequences=False)(embedding_resource)\n",
    "lstm_activity = LSTM(25, return_sequences=False)(embedding_activity)\n",
    "lstm_timestamp = LSTM(25, return_sequences=False)(input_timestamp)\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate(axis=-1)([lstm_resource, lstm_activity, lstm_timestamp])\n",
    "\n",
    "# Output layers\n",
    "output_resource = Dense(num_resources, activation='softmax', name='output_resource')(concatenated)\n",
    "output_activity = Dense(num_activities, activation='softmax', name='output_activity')(concatenated)\n",
    "output_timestamp = Dense(1, activation='linear', name='output_timestamp')(concatenated)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[input_resource, input_activity, input_timestamp], \n",
    "              outputs=[output_resource, output_activity, output_timestamp])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'output_resource': 'categorical_crossentropy', \n",
    "                    'output_activity': 'categorical_crossentropy', \n",
    "                    'output_timestamp': 'mean_squared_error'},\n",
    "              metrics={'output_resource': 'accuracy', \n",
    "                       'output_activity': 'accuracy', \n",
    "                       'output_timestamp': 'mean_absolute_error'})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resource data\n",
    "train_resource, test_resource, train_targets_resource, test_targets_resource = train_test_split(\n",
    "    windows_resource, targets_resource, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the activity data\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the timestamp data\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def cyclic_lr(epoch, lr):\n",
    "    # Example function that modulates LR within a range for each epoch\n",
    "    # Customize this function based on your cyclic learning rate policy\n",
    "    max_lr = 0.01  # Maximum LR\n",
    "    base_lr = 0.001  # Base LR\n",
    "    step_size = 10  # Number of epochs for half a cycle\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "380/380 [==============================] - 9s 8ms/step - loss: 4.7635 - output_resource_loss: 2.6077 - output_activity_loss: 1.3087 - output_timestamp_loss: 0.8471 - output_resource_accuracy: 0.3760 - output_activity_accuracy: 0.5784 - output_timestamp_mean_absolute_error: 0.5700 - val_loss: 3.2562 - val_output_resource_loss: 1.9706 - val_output_activity_loss: 0.6578 - val_output_timestamp_loss: 0.6278 - val_output_resource_accuracy: 0.4905 - val_output_activity_accuracy: 0.7561 - val_output_timestamp_mean_absolute_error: 0.4205 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.9819 - output_resource_loss: 1.8017 - output_activity_loss: 0.5908 - output_timestamp_loss: 0.5895 - output_resource_accuracy: 0.5272 - output_activity_accuracy: 0.7729 - output_timestamp_mean_absolute_error: 0.4272 - val_loss: 2.8524 - val_output_resource_loss: 1.7474 - val_output_activity_loss: 0.5324 - val_output_timestamp_loss: 0.5726 - val_output_resource_accuracy: 0.5489 - val_output_activity_accuracy: 0.8003 - val_output_timestamp_mean_absolute_error: 0.3884 - lr: 0.0019\n",
      "Epoch 3/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.6994 - output_resource_loss: 1.6327 - output_activity_loss: 0.5149 - output_timestamp_loss: 0.5518 - output_resource_accuracy: 0.5881 - output_activity_accuracy: 0.8055 - output_timestamp_mean_absolute_error: 0.4007 - val_loss: 2.5881 - val_output_resource_loss: 1.5450 - val_output_activity_loss: 0.4915 - val_output_timestamp_loss: 0.5516 - val_output_resource_accuracy: 0.6281 - val_output_activity_accuracy: 0.8129 - val_output_timestamp_mean_absolute_error: 0.3936 - lr: 0.0028\n",
      "Epoch 4/25\n",
      "380/380 [==============================] - 2s 6ms/step - loss: 2.4534 - output_resource_loss: 1.4169 - output_activity_loss: 0.4933 - output_timestamp_loss: 0.5431 - output_resource_accuracy: 0.6569 - output_activity_accuracy: 0.8053 - output_timestamp_mean_absolute_error: 0.3950 - val_loss: 2.4049 - val_output_resource_loss: 1.3692 - val_output_activity_loss: 0.4842 - val_output_timestamp_loss: 0.5516 - val_output_resource_accuracy: 0.6649 - val_output_activity_accuracy: 0.8092 - val_output_timestamp_mean_absolute_error: 0.3788 - lr: 0.0037\n",
      "Epoch 5/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.3326 - output_resource_loss: 1.3068 - output_activity_loss: 0.4869 - output_timestamp_loss: 0.5389 - output_resource_accuracy: 0.6758 - output_activity_accuracy: 0.8050 - output_timestamp_mean_absolute_error: 0.3870 - val_loss: 2.3351 - val_output_resource_loss: 1.3089 - val_output_activity_loss: 0.4768 - val_output_timestamp_loss: 0.5494 - val_output_resource_accuracy: 0.6766 - val_output_activity_accuracy: 0.8127 - val_output_timestamp_mean_absolute_error: 0.3952 - lr: 0.0046\n",
      "Epoch 6/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2809 - output_resource_loss: 1.2639 - output_activity_loss: 0.4831 - output_timestamp_loss: 0.5338 - output_resource_accuracy: 0.6822 - output_activity_accuracy: 0.8083 - output_timestamp_mean_absolute_error: 0.3850 - val_loss: 2.3254 - val_output_resource_loss: 1.2800 - val_output_activity_loss: 0.4932 - val_output_timestamp_loss: 0.5522 - val_output_resource_accuracy: 0.6750 - val_output_activity_accuracy: 0.8054 - val_output_timestamp_mean_absolute_error: 0.3725 - lr: 0.0055\n",
      "Epoch 7/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2584 - output_resource_loss: 1.2417 - output_activity_loss: 0.4828 - output_timestamp_loss: 0.5339 - output_resource_accuracy: 0.6823 - output_activity_accuracy: 0.8066 - output_timestamp_mean_absolute_error: 0.3860 - val_loss: 2.3082 - val_output_resource_loss: 1.2771 - val_output_activity_loss: 0.4789 - val_output_timestamp_loss: 0.5522 - val_output_resource_accuracy: 0.6767 - val_output_activity_accuracy: 0.8079 - val_output_timestamp_mean_absolute_error: 0.4034 - lr: 0.0064\n",
      "Epoch 8/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2495 - output_resource_loss: 1.2334 - output_activity_loss: 0.4805 - output_timestamp_loss: 0.5356 - output_resource_accuracy: 0.6834 - output_activity_accuracy: 0.8071 - output_timestamp_mean_absolute_error: 0.3944 - val_loss: 2.2941 - val_output_resource_loss: 1.2566 - val_output_activity_loss: 0.4818 - val_output_timestamp_loss: 0.5557 - val_output_resource_accuracy: 0.6818 - val_output_activity_accuracy: 0.8064 - val_output_timestamp_mean_absolute_error: 0.3833 - lr: 0.0073\n",
      "Epoch 9/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2356 - output_resource_loss: 1.2258 - output_activity_loss: 0.4800 - output_timestamp_loss: 0.5298 - output_resource_accuracy: 0.6835 - output_activity_accuracy: 0.8074 - output_timestamp_mean_absolute_error: 0.3818 - val_loss: 2.2914 - val_output_resource_loss: 1.2606 - val_output_activity_loss: 0.4791 - val_output_timestamp_loss: 0.5517 - val_output_resource_accuracy: 0.6811 - val_output_activity_accuracy: 0.8108 - val_output_timestamp_mean_absolute_error: 0.3660 - lr: 0.0082\n",
      "Epoch 10/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2322 - output_resource_loss: 1.2222 - output_activity_loss: 0.4793 - output_timestamp_loss: 0.5306 - output_resource_accuracy: 0.6831 - output_activity_accuracy: 0.8071 - output_timestamp_mean_absolute_error: 0.3828 - val_loss: 2.2840 - val_output_resource_loss: 1.2595 - val_output_activity_loss: 0.4791 - val_output_timestamp_loss: 0.5455 - val_output_resource_accuracy: 0.6787 - val_output_activity_accuracy: 0.8086 - val_output_timestamp_mean_absolute_error: 0.3763 - lr: 0.0091\n",
      "Epoch 11/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2321 - output_resource_loss: 1.2200 - output_activity_loss: 0.4794 - output_timestamp_loss: 0.5327 - output_resource_accuracy: 0.6827 - output_activity_accuracy: 0.8067 - output_timestamp_mean_absolute_error: 0.3897 - val_loss: 2.3067 - val_output_resource_loss: 1.2673 - val_output_activity_loss: 0.4791 - val_output_timestamp_loss: 0.5603 - val_output_resource_accuracy: 0.6852 - val_output_activity_accuracy: 0.8036 - val_output_timestamp_mean_absolute_error: 0.4249 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.2145 - output_resource_loss: 1.2084 - output_activity_loss: 0.4782 - output_timestamp_loss: 0.5278 - output_resource_accuracy: 0.6851 - output_activity_accuracy: 0.8065 - output_timestamp_mean_absolute_error: 0.3839 - val_loss: 2.2862 - val_output_resource_loss: 1.2570 - val_output_activity_loss: 0.4795 - val_output_timestamp_loss: 0.5497 - val_output_resource_accuracy: 0.6836 - val_output_activity_accuracy: 0.8120 - val_output_timestamp_mean_absolute_error: 0.4133 - lr: 0.0091\n",
      "Epoch 13/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1983 - output_resource_loss: 1.1996 - output_activity_loss: 0.4755 - output_timestamp_loss: 0.5232 - output_resource_accuracy: 0.6862 - output_activity_accuracy: 0.8067 - output_timestamp_mean_absolute_error: 0.3799 - val_loss: 2.2865 - val_output_resource_loss: 1.2572 - val_output_activity_loss: 0.4765 - val_output_timestamp_loss: 0.5527 - val_output_resource_accuracy: 0.6836 - val_output_activity_accuracy: 0.8086 - val_output_timestamp_mean_absolute_error: 0.3817 - lr: 0.0082\n",
      "Epoch 14/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1841 - output_resource_loss: 1.1905 - output_activity_loss: 0.4728 - output_timestamp_loss: 0.5209 - output_resource_accuracy: 0.6890 - output_activity_accuracy: 0.8087 - output_timestamp_mean_absolute_error: 0.3767 - val_loss: 2.2837 - val_output_resource_loss: 1.2559 - val_output_activity_loss: 0.4801 - val_output_timestamp_loss: 0.5477 - val_output_resource_accuracy: 0.6811 - val_output_activity_accuracy: 0.8071 - val_output_timestamp_mean_absolute_error: 0.3898 - lr: 0.0073\n",
      "Epoch 15/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1762 - output_resource_loss: 1.1865 - output_activity_loss: 0.4718 - output_timestamp_loss: 0.5180 - output_resource_accuracy: 0.6869 - output_activity_accuracy: 0.8105 - output_timestamp_mean_absolute_error: 0.3731 - val_loss: 2.2747 - val_output_resource_loss: 1.2522 - val_output_activity_loss: 0.4740 - val_output_timestamp_loss: 0.5485 - val_output_resource_accuracy: 0.6843 - val_output_activity_accuracy: 0.8128 - val_output_timestamp_mean_absolute_error: 0.3962 - lr: 0.0064\n",
      "Epoch 16/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1595 - output_resource_loss: 1.1761 - output_activity_loss: 0.4688 - output_timestamp_loss: 0.5146 - output_resource_accuracy: 0.6880 - output_activity_accuracy: 0.8095 - output_timestamp_mean_absolute_error: 0.3683 - val_loss: 2.2807 - val_output_resource_loss: 1.2525 - val_output_activity_loss: 0.4751 - val_output_timestamp_loss: 0.5530 - val_output_resource_accuracy: 0.6811 - val_output_activity_accuracy: 0.8095 - val_output_timestamp_mean_absolute_error: 0.3608 - lr: 0.0055\n",
      "Epoch 17/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1491 - output_resource_loss: 1.1708 - output_activity_loss: 0.4667 - output_timestamp_loss: 0.5117 - output_resource_accuracy: 0.6891 - output_activity_accuracy: 0.8112 - output_timestamp_mean_absolute_error: 0.3650 - val_loss: 2.2673 - val_output_resource_loss: 1.2502 - val_output_activity_loss: 0.4743 - val_output_timestamp_loss: 0.5427 - val_output_resource_accuracy: 0.6816 - val_output_activity_accuracy: 0.8112 - val_output_timestamp_mean_absolute_error: 0.3678 - lr: 0.0046\n",
      "Epoch 18/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1367 - output_resource_loss: 1.1632 - output_activity_loss: 0.4643 - output_timestamp_loss: 0.5091 - output_resource_accuracy: 0.6899 - output_activity_accuracy: 0.8112 - output_timestamp_mean_absolute_error: 0.3626 - val_loss: 2.2657 - val_output_resource_loss: 1.2476 - val_output_activity_loss: 0.4732 - val_output_timestamp_loss: 0.5450 - val_output_resource_accuracy: 0.6836 - val_output_activity_accuracy: 0.8135 - val_output_timestamp_mean_absolute_error: 0.3747 - lr: 0.0037\n",
      "Epoch 19/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1252 - output_resource_loss: 1.1566 - output_activity_loss: 0.4623 - output_timestamp_loss: 0.5063 - output_resource_accuracy: 0.6913 - output_activity_accuracy: 0.8130 - output_timestamp_mean_absolute_error: 0.3588 - val_loss: 2.2641 - val_output_resource_loss: 1.2486 - val_output_activity_loss: 0.4773 - val_output_timestamp_loss: 0.5382 - val_output_resource_accuracy: 0.6824 - val_output_activity_accuracy: 0.8135 - val_output_timestamp_mean_absolute_error: 0.3648 - lr: 0.0028\n",
      "Epoch 20/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1134 - output_resource_loss: 1.1493 - output_activity_loss: 0.4605 - output_timestamp_loss: 0.5036 - output_resource_accuracy: 0.6918 - output_activity_accuracy: 0.8137 - output_timestamp_mean_absolute_error: 0.3544 - val_loss: 2.2598 - val_output_resource_loss: 1.2463 - val_output_activity_loss: 0.4741 - val_output_timestamp_loss: 0.5393 - val_output_resource_accuracy: 0.6837 - val_output_activity_accuracy: 0.8150 - val_output_timestamp_mean_absolute_error: 0.3673 - lr: 0.0019\n",
      "Epoch 21/25\n",
      "380/380 [==============================] - 2s 6ms/step - loss: 2.1015 - output_resource_loss: 1.1425 - output_activity_loss: 0.4582 - output_timestamp_loss: 0.5008 - output_resource_accuracy: 0.6950 - output_activity_accuracy: 0.8149 - output_timestamp_mean_absolute_error: 0.3521 - val_loss: 2.2596 - val_output_resource_loss: 1.2463 - val_output_activity_loss: 0.4736 - val_output_timestamp_loss: 0.5398 - val_output_resource_accuracy: 0.6850 - val_output_activity_accuracy: 0.8127 - val_output_timestamp_mean_absolute_error: 0.3636 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "380/380 [==============================] - 2s 6ms/step - loss: 2.1090 - output_resource_loss: 1.1469 - output_activity_loss: 0.4597 - output_timestamp_loss: 0.5024 - output_resource_accuracy: 0.6922 - output_activity_accuracy: 0.8152 - output_timestamp_mean_absolute_error: 0.3544 - val_loss: 2.2654 - val_output_resource_loss: 1.2488 - val_output_activity_loss: 0.4761 - val_output_timestamp_loss: 0.5406 - val_output_resource_accuracy: 0.6850 - val_output_activity_accuracy: 0.8120 - val_output_timestamp_mean_absolute_error: 0.3791 - lr: 0.0019\n",
      "Epoch 23/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1160 - output_resource_loss: 1.1506 - output_activity_loss: 0.4612 - output_timestamp_loss: 0.5042 - output_resource_accuracy: 0.6927 - output_activity_accuracy: 0.8139 - output_timestamp_mean_absolute_error: 0.3574 - val_loss: 2.2678 - val_output_resource_loss: 1.2504 - val_output_activity_loss: 0.4760 - val_output_timestamp_loss: 0.5413 - val_output_resource_accuracy: 0.6838 - val_output_activity_accuracy: 0.8099 - val_output_timestamp_mean_absolute_error: 0.3801 - lr: 0.0028\n",
      "Epoch 24/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1228 - output_resource_loss: 1.1541 - output_activity_loss: 0.4626 - output_timestamp_loss: 0.5061 - output_resource_accuracy: 0.6908 - output_activity_accuracy: 0.8125 - output_timestamp_mean_absolute_error: 0.3633 - val_loss: 2.2757 - val_output_resource_loss: 1.2533 - val_output_activity_loss: 0.4749 - val_output_timestamp_loss: 0.5475 - val_output_resource_accuracy: 0.6846 - val_output_activity_accuracy: 0.8107 - val_output_timestamp_mean_absolute_error: 0.3467 - lr: 0.0037\n",
      "Epoch 25/25\n",
      "380/380 [==============================] - 2s 5ms/step - loss: 2.1294 - output_resource_loss: 1.1580 - output_activity_loss: 0.4636 - output_timestamp_loss: 0.5079 - output_resource_accuracy: 0.6911 - output_activity_accuracy: 0.8113 - output_timestamp_mean_absolute_error: 0.3655 - val_loss: 2.2789 - val_output_resource_loss: 1.2562 - val_output_activity_loss: 0.4777 - val_output_timestamp_loss: 0.5450 - val_output_resource_accuracy: 0.6835 - val_output_activity_accuracy: 0.8082 - val_output_timestamp_mean_absolute_error: 0.3743 - lr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_targets_resource_cat = to_categorical(train_targets_resource, num_classes=num_resources)\n",
    "test_targets_resource_cat = to_categorical(test_targets_resource, num_classes=num_resources)\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "\n",
    "# Note: Assuming timestamp targets are continuous and don't need to be converted to categorical\n",
    "\n",
    "history = model.fit([train_resource, train_activity, train_timestamp], \n",
    "                    [train_targets_resource_cat, train_targets_activity_cat, train_targets_timestamp],\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=([test_resource, test_activity, test_timestamp], \n",
    "                                     [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]),\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_scheduler])  # Add other callbacks as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 1s 2ms/step - loss: 2.2789 - output_resource_loss: 1.2562 - output_activity_loss: 0.4777 - output_timestamp_loss: 0.5450 - output_resource_accuracy: 0.6835 - output_activity_accuracy: 0.8082 - output_timestamp_mean_absolute_error: 0.3743\n",
      "[2.278855562210083, 1.2561776638031006, 0.4776534140110016, 0.5450263023376465, 0.6834567189216614, 0.8082283735275269, 0.3742930293083191]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(\n",
    "    [test_resource, test_activity, test_timestamp],\n",
    "    [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to an H5 file\n",
    "model.save('dapnn_bpic12.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084/1084 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_resource, windows_activity, windows_timestamp])\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_resource = predictions[0]        # ID predictions\n",
    "predictions_activity = predictions[1]  # Resource predictions\n",
    "\n",
    "# Extract predictions for numerical attribute (timestamp)\n",
    "predictions_timestamp = predictions[2] # Timestamp predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def compute_anomaly_scores(predictions, actuals):\n",
    "    # For categorical predictions, convert actuals to one-hot for comparison\n",
    "    actuals_one_hot = to_categorical(actuals, num_classes=predictions.shape[-1])\n",
    "    \n",
    "    max_predictions = np.max(predictions, axis=-1)\n",
    "    actual_predictions = np.sum(predictions * actuals_one_hot, axis=-1)  # Extract the probability of the actual class\n",
    "    \n",
    "    anomaly_scores = (max_predictions - actual_predictions) / max_predictions\n",
    "    \n",
    "    return anomaly_scores\n",
    "\n",
    "# Assuming targets_id, targets_resource, targets_activity, targets_role are the true values for these attributes\n",
    "anomaly_scores_resource = compute_anomaly_scores(predictions_resource, targets_resource)\n",
    "anomaly_scores_activity = compute_anomaly_scores(predictions_activity, targets_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals, normalization_factor):\n",
    "    \"\"\"\n",
    "    Compute anomaly scores for continuous attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predicted values.\n",
    "    - actuals: numpy array of actual values.\n",
    "    - normalization_factor: normalization factor (e.g., standard deviation of the attribute).\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of anomaly scores.\n",
    "    \"\"\"\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize the differences\n",
    "    anomaly_scores = differences / normalization_factor\n",
    "    \n",
    "    return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_factor = np.std(targets_timestamp)       # Example normalization factor (standard deviation)\n",
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp, normalization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_resource = np.array(anomaly_scores_resource).flatten()\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "    anomaly_scores_timestamp = np.array(anomaly_scores_timestamp).flatten()\n",
    "\n",
    "    # Check if all arrays have the same length\n",
    "    if not (len(anomaly_scores_resource) == len(anomaly_scores_activity) == len(anomaly_scores_timestamp)):\n",
    "        raise ValueError(\"All input anomaly scores must have the same length.\")\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_activity, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_activity])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True: anomaly, False: no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34670</th>\n",
       "      <td>13084</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34671</th>\n",
       "      <td>13085</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34672</th>\n",
       "      <td>13085</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34673</th>\n",
       "      <td>13085</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34674</th>\n",
       "      <td>13086</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34675 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        case  predicted\n",
       "0          0      False\n",
       "1          0      False\n",
       "2          0      False\n",
       "3          0      False\n",
       "4          0      False\n",
       "...      ...        ...\n",
       "34670  13084      False\n",
       "34671  13085      False\n",
       "34672  13085      False\n",
       "34673  13085      False\n",
       "34674  13086      False\n",
       "\n",
       "[34675 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted'] = anomalous_cases\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0        False\n",
       "1         True\n",
       "2         True\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "13082    False\n",
       "13083    False\n",
       "13084    False\n",
       "13085    False\n",
       "13086    False\n",
       "Name: predicted, Length: 13087, dtype: bool"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_prediction = mapping.groupby('case')['predicted'].any()\n",
    "case_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866b5b0ab28245d29db5b0f841156f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/Model_A_corrected.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(1)\n",
    "        else:\n",
    "            conformance_status.append(0)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13082</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13083</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13084</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13085</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13086</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13087 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       conformity  predicted\n",
       "0               0          1\n",
       "1               1          0\n",
       "2               1          0\n",
       "3               1          1\n",
       "4               1          1\n",
       "...           ...        ...\n",
       "13082           0          1\n",
       "13083           1          1\n",
       "13084           1          1\n",
       "13085           0          1\n",
       "13086           1          1\n",
       "\n",
       "[13087 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth['predicted'] = 1 - ground_truth['predicted']\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.736\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.842\n"
     ]
    }
   ],
   "source": [
    "# Calculate f1\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.153\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = TN / (TN + FN)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.259\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = TN / (TN + FP)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.887\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.802\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5305774052077026"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming ground_truth is your DataFrame\n",
    "# Make sure 'conformity' contains actual labels (0 or 1)\n",
    "# and 'predicted' contains predicted probabilities or scores\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "auc_roc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
