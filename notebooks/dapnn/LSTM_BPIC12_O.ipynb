{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_4303/670060893.py:14: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/BPIC12_Log_onlyO.csv', sep=',')\n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['lifecycle:transition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['@@index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:AMOUNT_REQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:REG_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_SELECTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_CREATED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11049</td>\n",
       "      <td>O_SENT_BACK</td>\n",
       "      <td>0</td>\n",
       "      <td>0.299272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10629</td>\n",
       "      <td>O_ACCEPTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31239</th>\n",
       "      <td>11003</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>5013</td>\n",
       "      <td>-0.620265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31240</th>\n",
       "      <td>10789</td>\n",
       "      <td>O_SENT_BACK</td>\n",
       "      <td>5013</td>\n",
       "      <td>0.099241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31241</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_SELECTED</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31242</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_CREATED</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31243</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31244 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource concept:name  @@case_index  standardized_elapsed_time\n",
       "0             10862   O_SELECTED             0                  -0.620650\n",
       "1             10862    O_CREATED             0                  -0.620647\n",
       "2             10862       O_SENT             0                  -0.620647\n",
       "3             11049  O_SENT_BACK             0                   0.299272\n",
       "4             10629   O_ACCEPTED             0                   0.602251\n",
       "...             ...          ...           ...                        ...\n",
       "31239         11003       O_SENT          5013                  -0.620265\n",
       "31240         10789  O_SENT_BACK          5013                   0.099241\n",
       "31241         10933   O_SELECTED          5014                  -0.620650\n",
       "31242         10933    O_CREATED          5014                  -0.620648\n",
       "31243         10933       O_SENT          5014                  -0.620648\n",
       "\n",
       "[31244 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Start & End markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert start and end markers\n",
    "def add_markers(df):\n",
    "    # Identify unique case indices\n",
    "    case_indices = df['@@case_index'].unique()\n",
    "    \n",
    "    # Prepare a container for new DataFrame rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each case index to add start and end markers\n",
    "    for case_index in case_indices:\n",
    "        # Create a start marker row with all columns except @@case_index set to 'Start'\n",
    "        start_row = {col: 'Start' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Create an end marker row with all columns except @@case_index set to 'End'\n",
    "        end_row = {col: 'End' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Append start row, rows for the current case, and end row\n",
    "        new_rows.append(start_row)\n",
    "        new_rows.extend(df[df['@@case_index'] == case_index].to_dict('records'))\n",
    "        new_rows.append(end_row)\n",
    "    \n",
    "    # Convert the list of rows into a DataFrame\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Apply the function to add start and end markers to the dataframe\n",
    "modified_dataframe = add_markers(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dataframe['standardized_elapsed_time'] = modified_dataframe['standardized_elapsed_time'].replace({'Start': 0, 'End': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['org:resource'])\n",
    "modified_dataframe['org:resource'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['concept:name'])\n",
    "modified_dataframe['concept:name'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.299272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41269</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41270</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41271</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41272</th>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41273</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5014</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41274 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource  concept:name  @@case_index  standardized_elapsed_time\n",
       "0                 0             0             0                   0.000000\n",
       "1                 1             1             0                  -0.620650\n",
       "2                 1             2             0                  -0.620647\n",
       "3                 1             3             0                  -0.620647\n",
       "4                 2             4             0                   0.299272\n",
       "...             ...           ...           ...                        ...\n",
       "41269             0             0          5014                   0.000000\n",
       "41270            47             1          5014                  -0.620650\n",
       "41271            47             2          5014                  -0.620648\n",
       "41272            47             3          5014                  -0.620648\n",
       "41273             4             6          5014                   1.000000\n",
       "\n",
       "[41274 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for Cases with less then 5 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each unique value in '@@case_index'\n",
    "frequency = modified_dataframe['@@case_index'].value_counts()\n",
    "\n",
    "# Finding the minimum occurrence\n",
    "min_occurrence = frequency.min()\n",
    "\n",
    "min_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resource = modified_dataframe[['org:resource', '@@case_index']]\n",
    "df_activity = modified_dataframe[['concept:name', '@@case_index']]\n",
    "df_timestamp = modified_dataframe[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df, case_id_column='@@case_index', window_size=5):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    # Iterate over each unique case\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        # Extract the case\n",
    "        case_data = df[df[case_id_column] == case_id]\n",
    "        \n",
    "        # Convert case_data to a NumPy array and drop the case_id_column\n",
    "        case_data_array = case_data.drop(columns=[case_id_column]).to_numpy()\n",
    "\n",
    "        # Adjusting the condition to correctly reflect window_size without needing an additional +1\n",
    "        # Now it correctly considers window_size as including the target event\n",
    "        if len(case_data_array) >= window_size:\n",
    "            # Adjust the loop to generate sliding windows of size window_size - 1 for the inputs and use the next event as the target\n",
    "            for i in range(len(case_data_array) - window_size + 1):\n",
    "                # window now has window_size - 1 events\n",
    "                window = case_data_array[i:i + window_size - 1]\n",
    "                # The target is the event immediately following the window\n",
    "                target = case_data_array[i + window_size - 1]\n",
    "                windows.append(window)\n",
    "                targets.append(target)\n",
    "                case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling and to ensure they are two-dimensional\n",
    "    windows_array = np.array(windows)\n",
    "    targets_array = np.array(targets)\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_array, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_resource, targets_resource, case_indices = generate_sliding_windows(df_resource)\n",
    "windows_activity, targets_activity, case_indices = generate_sliding_windows(df_activity)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_sliding_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 16:43:47.902214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_resource (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_activity (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 4, 50)                3100      ['input_resource[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 4, 50)                450       ['input_activity[0][0]']      \n",
      "                                                                                                  \n",
      " input_timestamp (InputLaye  [(None, 4, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 25)                   7600      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 25)                   7600      ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 25)                   2700      ['input_timestamp[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 75)                   0         ['lstm[0][0]',                \n",
      "                                                                     'lstm_1[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_resource (Dense)     (None, 62)                   4712      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 9)                    684       ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    76        ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26922 (105.16 KB)\n",
      "Trainable params: 26922 (105.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding\n",
    "\n",
    "# Assuming these values as placeholders, replace them with actual counts from your data\n",
    "num_resources = modified_dataframe['org:resource'].nunique()\n",
    "num_activities = modified_dataframe['concept:name'].nunique()\n",
    "\n",
    "embedding_dim_resource = 50\n",
    "embedding_dim_activity = 50\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "# Input layers\n",
    "input_resource = Input(shape=(time_steps,), name='input_resource')\n",
    "input_activity = Input(shape=(time_steps,), name='input_activity')\n",
    "input_timestamp = Input(shape=(time_steps, 1), name='input_timestamp')\n",
    "\n",
    "# Embedding layers\n",
    "embedding_resource = Embedding(input_dim=num_resources, output_dim=embedding_dim_resource, input_length=time_steps)(input_resource)\n",
    "embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim_activity, input_length=time_steps)(input_activity)\n",
    "\n",
    "# LSTM layers\n",
    "lstm_resource = LSTM(25, return_sequences=False)(embedding_resource)\n",
    "lstm_activity = LSTM(25, return_sequences=False)(embedding_activity)\n",
    "lstm_timestamp = LSTM(25, return_sequences=False)(input_timestamp)\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate(axis=-1)([lstm_resource, lstm_activity, lstm_timestamp])\n",
    "\n",
    "# Output layers\n",
    "output_resource = Dense(num_resources, activation='softmax', name='output_resource')(concatenated)\n",
    "output_activity = Dense(num_activities, activation='softmax', name='output_activity')(concatenated)\n",
    "output_timestamp = Dense(1, activation='linear', name='output_timestamp')(concatenated)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[input_resource, input_activity, input_timestamp], \n",
    "              outputs=[output_resource, output_activity, output_timestamp])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'output_resource': 'categorical_crossentropy', \n",
    "                    'output_activity': 'categorical_crossentropy', \n",
    "                    'output_timestamp': 'mean_squared_error'},\n",
    "              metrics={'output_resource': 'accuracy', \n",
    "                       'output_activity': 'accuracy', \n",
    "                       'output_timestamp': 'mean_absolute_error'})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resource data\n",
    "train_resource, test_resource, train_targets_resource, test_targets_resource = train_test_split(\n",
    "    windows_resource, targets_resource, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the activity data\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the timestamp data\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def cyclic_lr(epoch, lr):\n",
    "    # Example function that modulates LR within a range for each epoch\n",
    "    # Customize this function based on your cyclic learning rate policy\n",
    "    max_lr = 0.01  # Maximum LR\n",
    "    base_lr = 0.001  # Base LR\n",
    "    step_size = 10  # Number of epochs for half a cycle\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "233/233 [==============================] - 10s 12ms/step - loss: 5.7891 - output_resource_loss: 3.3234 - output_activity_loss: 1.7319 - output_timestamp_loss: 0.7338 - output_resource_accuracy: 0.2276 - output_activity_accuracy: 0.3598 - output_timestamp_mean_absolute_error: 0.6250 - val_loss: 4.3681 - val_output_resource_loss: 2.7431 - val_output_activity_loss: 1.0648 - val_output_timestamp_loss: 0.5602 - val_output_resource_accuracy: 0.2809 - val_output_activity_accuracy: 0.7016 - val_output_timestamp_mean_absolute_error: 0.5329 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 3.5686 - output_resource_loss: 2.3369 - output_activity_loss: 0.7509 - output_timestamp_loss: 0.4809 - output_resource_accuracy: 0.3989 - output_activity_accuracy: 0.7200 - output_timestamp_mean_absolute_error: 0.4857 - val_loss: 3.2041 - val_output_resource_loss: 2.0661 - val_output_activity_loss: 0.6693 - val_output_timestamp_loss: 0.4688 - val_output_resource_accuracy: 0.4657 - val_output_activity_accuracy: 0.7263 - val_output_timestamp_mean_absolute_error: 0.4622 - lr: 0.0019\n",
      "Epoch 3/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.9460 - output_resource_loss: 1.8505 - output_activity_loss: 0.6470 - output_timestamp_loss: 0.4486 - output_resource_accuracy: 0.5221 - output_activity_accuracy: 0.7298 - output_timestamp_mean_absolute_error: 0.4562 - val_loss: 2.8068 - val_output_resource_loss: 1.7290 - val_output_activity_loss: 0.6321 - val_output_timestamp_loss: 0.4457 - val_output_resource_accuracy: 0.5447 - val_output_activity_accuracy: 0.7431 - val_output_timestamp_mean_absolute_error: 0.4501 - lr: 0.0028\n",
      "Epoch 4/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.6812 - output_resource_loss: 1.6198 - output_activity_loss: 0.6189 - output_timestamp_loss: 0.4425 - output_resource_accuracy: 0.5658 - output_activity_accuracy: 0.7390 - output_timestamp_mean_absolute_error: 0.4479 - val_loss: 2.6489 - val_output_resource_loss: 1.5881 - val_output_activity_loss: 0.6158 - val_output_timestamp_loss: 0.4450 - val_output_resource_accuracy: 0.5610 - val_output_activity_accuracy: 0.7417 - val_output_timestamp_mean_absolute_error: 0.4596 - lr: 0.0037\n",
      "Epoch 5/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.5825 - output_resource_loss: 1.5365 - output_activity_loss: 0.6122 - output_timestamp_loss: 0.4339 - output_resource_accuracy: 0.5769 - output_activity_accuracy: 0.7409 - output_timestamp_mean_absolute_error: 0.4402 - val_loss: 2.5873 - val_output_resource_loss: 1.5416 - val_output_activity_loss: 0.6068 - val_output_timestamp_loss: 0.4389 - val_output_resource_accuracy: 0.5679 - val_output_activity_accuracy: 0.7431 - val_output_timestamp_mean_absolute_error: 0.4285 - lr: 0.0046\n",
      "Epoch 6/25\n",
      "233/233 [==============================] - 2s 7ms/step - loss: 2.5302 - output_resource_loss: 1.4898 - output_activity_loss: 0.6027 - output_timestamp_loss: 0.4378 - output_resource_accuracy: 0.5828 - output_activity_accuracy: 0.7417 - output_timestamp_mean_absolute_error: 0.4416 - val_loss: 2.5913 - val_output_resource_loss: 1.5390 - val_output_activity_loss: 0.6107 - val_output_timestamp_loss: 0.4415 - val_output_resource_accuracy: 0.5712 - val_output_activity_accuracy: 0.7428 - val_output_timestamp_mean_absolute_error: 0.4342 - lr: 0.0055\n",
      "Epoch 7/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.5003 - output_resource_loss: 1.4700 - output_activity_loss: 0.5971 - output_timestamp_loss: 0.4332 - output_resource_accuracy: 0.5887 - output_activity_accuracy: 0.7419 - output_timestamp_mean_absolute_error: 0.4383 - val_loss: 2.6003 - val_output_resource_loss: 1.5272 - val_output_activity_loss: 0.6172 - val_output_timestamp_loss: 0.4559 - val_output_resource_accuracy: 0.5717 - val_output_activity_accuracy: 0.7464 - val_output_timestamp_mean_absolute_error: 0.4463 - lr: 0.0064\n",
      "Epoch 8/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.4846 - output_resource_loss: 1.4567 - output_activity_loss: 0.5965 - output_timestamp_loss: 0.4314 - output_resource_accuracy: 0.5894 - output_activity_accuracy: 0.7433 - output_timestamp_mean_absolute_error: 0.4361 - val_loss: 2.5674 - val_output_resource_loss: 1.5288 - val_output_activity_loss: 0.6060 - val_output_timestamp_loss: 0.4326 - val_output_resource_accuracy: 0.5694 - val_output_activity_accuracy: 0.7420 - val_output_timestamp_mean_absolute_error: 0.4409 - lr: 0.0073\n",
      "Epoch 9/25\n",
      "233/233 [==============================] - 2s 8ms/step - loss: 2.4651 - output_resource_loss: 1.4463 - output_activity_loss: 0.5944 - output_timestamp_loss: 0.4244 - output_resource_accuracy: 0.5900 - output_activity_accuracy: 0.7467 - output_timestamp_mean_absolute_error: 0.4287 - val_loss: 2.5851 - val_output_resource_loss: 1.5320 - val_output_activity_loss: 0.6124 - val_output_timestamp_loss: 0.4407 - val_output_resource_accuracy: 0.5728 - val_output_activity_accuracy: 0.7474 - val_output_timestamp_mean_absolute_error: 0.4277 - lr: 0.0082\n",
      "Epoch 10/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.4550 - output_resource_loss: 1.4433 - output_activity_loss: 0.5914 - output_timestamp_loss: 0.4203 - output_resource_accuracy: 0.5859 - output_activity_accuracy: 0.7441 - output_timestamp_mean_absolute_error: 0.4276 - val_loss: 2.5933 - val_output_resource_loss: 1.5303 - val_output_activity_loss: 0.6293 - val_output_timestamp_loss: 0.4337 - val_output_resource_accuracy: 0.5722 - val_output_activity_accuracy: 0.7447 - val_output_timestamp_mean_absolute_error: 0.4081 - lr: 0.0091\n",
      "Epoch 11/25\n",
      "233/233 [==============================] - 2s 7ms/step - loss: 2.4681 - output_resource_loss: 1.4489 - output_activity_loss: 0.5977 - output_timestamp_loss: 0.4215 - output_resource_accuracy: 0.5917 - output_activity_accuracy: 0.7439 - output_timestamp_mean_absolute_error: 0.4249 - val_loss: 2.5965 - val_output_resource_loss: 1.5412 - val_output_activity_loss: 0.6166 - val_output_timestamp_loss: 0.4387 - val_output_resource_accuracy: 0.5679 - val_output_activity_accuracy: 0.7191 - val_output_timestamp_mean_absolute_error: 0.4277 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "233/233 [==============================] - 2s 8ms/step - loss: 2.4478 - output_resource_loss: 1.4347 - output_activity_loss: 0.5933 - output_timestamp_loss: 0.4199 - output_resource_accuracy: 0.5905 - output_activity_accuracy: 0.7442 - output_timestamp_mean_absolute_error: 0.4234 - val_loss: 2.5812 - val_output_resource_loss: 1.5287 - val_output_activity_loss: 0.6058 - val_output_timestamp_loss: 0.4467 - val_output_resource_accuracy: 0.5725 - val_output_activity_accuracy: 0.7452 - val_output_timestamp_mean_absolute_error: 0.4540 - lr: 0.0091\n",
      "Epoch 13/25\n",
      "233/233 [==============================] - 2s 8ms/step - loss: 2.3996 - output_resource_loss: 1.4049 - output_activity_loss: 0.5825 - output_timestamp_loss: 0.4121 - output_resource_accuracy: 0.5964 - output_activity_accuracy: 0.7477 - output_timestamp_mean_absolute_error: 0.4155 - val_loss: 2.5739 - val_output_resource_loss: 1.5388 - val_output_activity_loss: 0.6098 - val_output_timestamp_loss: 0.4253 - val_output_resource_accuracy: 0.5697 - val_output_activity_accuracy: 0.7392 - val_output_timestamp_mean_absolute_error: 0.4145 - lr: 0.0082\n",
      "Epoch 14/25\n",
      "233/233 [==============================] - 3s 12ms/step - loss: 2.3872 - output_resource_loss: 1.3956 - output_activity_loss: 0.5814 - output_timestamp_loss: 0.4101 - output_resource_accuracy: 0.5961 - output_activity_accuracy: 0.7484 - output_timestamp_mean_absolute_error: 0.4125 - val_loss: 2.5506 - val_output_resource_loss: 1.5243 - val_output_activity_loss: 0.6088 - val_output_timestamp_loss: 0.4175 - val_output_resource_accuracy: 0.5687 - val_output_activity_accuracy: 0.7263 - val_output_timestamp_mean_absolute_error: 0.4196 - lr: 0.0073\n",
      "Epoch 15/25\n",
      "233/233 [==============================] - 3s 14ms/step - loss: 2.3600 - output_resource_loss: 1.3805 - output_activity_loss: 0.5748 - output_timestamp_loss: 0.4047 - output_resource_accuracy: 0.6001 - output_activity_accuracy: 0.7475 - output_timestamp_mean_absolute_error: 0.4104 - val_loss: 2.5519 - val_output_resource_loss: 1.5203 - val_output_activity_loss: 0.6099 - val_output_timestamp_loss: 0.4217 - val_output_resource_accuracy: 0.5805 - val_output_activity_accuracy: 0.7372 - val_output_timestamp_mean_absolute_error: 0.4288 - lr: 0.0064\n",
      "Epoch 16/25\n",
      "233/233 [==============================] - 4s 16ms/step - loss: 2.3452 - output_resource_loss: 1.3668 - output_activity_loss: 0.5722 - output_timestamp_loss: 0.4062 - output_resource_accuracy: 0.6019 - output_activity_accuracy: 0.7530 - output_timestamp_mean_absolute_error: 0.4108 - val_loss: 2.5484 - val_output_resource_loss: 1.5279 - val_output_activity_loss: 0.6067 - val_output_timestamp_loss: 0.4138 - val_output_resource_accuracy: 0.5714 - val_output_activity_accuracy: 0.7422 - val_output_timestamp_mean_absolute_error: 0.4065 - lr: 0.0055\n",
      "Epoch 17/25\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 2.3175 - output_resource_loss: 1.3517 - output_activity_loss: 0.5671 - output_timestamp_loss: 0.3986 - output_resource_accuracy: 0.6062 - output_activity_accuracy: 0.7530 - output_timestamp_mean_absolute_error: 0.4033 - val_loss: 2.5473 - val_output_resource_loss: 1.5291 - val_output_activity_loss: 0.6049 - val_output_timestamp_loss: 0.4133 - val_output_resource_accuracy: 0.5722 - val_output_activity_accuracy: 0.7467 - val_output_timestamp_mean_absolute_error: 0.4140 - lr: 0.0046\n",
      "Epoch 18/25\n",
      "233/233 [==============================] - 3s 13ms/step - loss: 2.3047 - output_resource_loss: 1.3401 - output_activity_loss: 0.5628 - output_timestamp_loss: 0.4018 - output_resource_accuracy: 0.6089 - output_activity_accuracy: 0.7571 - output_timestamp_mean_absolute_error: 0.4074 - val_loss: 2.5451 - val_output_resource_loss: 1.5238 - val_output_activity_loss: 0.6066 - val_output_timestamp_loss: 0.4147 - val_output_resource_accuracy: 0.5830 - val_output_activity_accuracy: 0.7422 - val_output_timestamp_mean_absolute_error: 0.4070 - lr: 0.0037\n",
      "Epoch 19/25\n",
      "233/233 [==============================] - 3s 11ms/step - loss: 2.2871 - output_resource_loss: 1.3293 - output_activity_loss: 0.5615 - output_timestamp_loss: 0.3963 - output_resource_accuracy: 0.6108 - output_activity_accuracy: 0.7565 - output_timestamp_mean_absolute_error: 0.3983 - val_loss: 2.5410 - val_output_resource_loss: 1.5254 - val_output_activity_loss: 0.6049 - val_output_timestamp_loss: 0.4107 - val_output_resource_accuracy: 0.5794 - val_output_activity_accuracy: 0.7431 - val_output_timestamp_mean_absolute_error: 0.4047 - lr: 0.0028\n",
      "Epoch 20/25\n",
      "233/233 [==============================] - 2s 8ms/step - loss: 2.2666 - output_resource_loss: 1.3165 - output_activity_loss: 0.5574 - output_timestamp_loss: 0.3927 - output_resource_accuracy: 0.6120 - output_activity_accuracy: 0.7592 - output_timestamp_mean_absolute_error: 0.3960 - val_loss: 2.5478 - val_output_resource_loss: 1.5288 - val_output_activity_loss: 0.6067 - val_output_timestamp_loss: 0.4123 - val_output_resource_accuracy: 0.5819 - val_output_activity_accuracy: 0.7431 - val_output_timestamp_mean_absolute_error: 0.4162 - lr: 0.0019\n",
      "Epoch 21/25\n",
      "233/233 [==============================] - 2s 7ms/step - loss: 2.2500 - output_resource_loss: 1.3058 - output_activity_loss: 0.5536 - output_timestamp_loss: 0.3905 - output_resource_accuracy: 0.6159 - output_activity_accuracy: 0.7599 - output_timestamp_mean_absolute_error: 0.3952 - val_loss: 2.5416 - val_output_resource_loss: 1.5267 - val_output_activity_loss: 0.6060 - val_output_timestamp_loss: 0.4089 - val_output_resource_accuracy: 0.5794 - val_output_activity_accuracy: 0.7411 - val_output_timestamp_mean_absolute_error: 0.4015 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "233/233 [==============================] - 2s 7ms/step - loss: 2.2625 - output_resource_loss: 1.3134 - output_activity_loss: 0.5559 - output_timestamp_loss: 0.3931 - output_resource_accuracy: 0.6126 - output_activity_accuracy: 0.7579 - output_timestamp_mean_absolute_error: 0.3972 - val_loss: 2.5501 - val_output_resource_loss: 1.5297 - val_output_activity_loss: 0.6091 - val_output_timestamp_loss: 0.4113 - val_output_resource_accuracy: 0.5782 - val_output_activity_accuracy: 0.7417 - val_output_timestamp_mean_absolute_error: 0.4033 - lr: 0.0019\n",
      "Epoch 23/25\n",
      "233/233 [==============================] - 2s 7ms/step - loss: 2.2774 - output_resource_loss: 1.3237 - output_activity_loss: 0.5589 - output_timestamp_loss: 0.3949 - output_resource_accuracy: 0.6130 - output_activity_accuracy: 0.7568 - output_timestamp_mean_absolute_error: 0.3993 - val_loss: 2.5513 - val_output_resource_loss: 1.5310 - val_output_activity_loss: 0.6101 - val_output_timestamp_loss: 0.4102 - val_output_resource_accuracy: 0.5780 - val_output_activity_accuracy: 0.7444 - val_output_timestamp_mean_absolute_error: 0.4007 - lr: 0.0028\n",
      "Epoch 24/25\n",
      "233/233 [==============================] - 2s 8ms/step - loss: 2.2851 - output_resource_loss: 1.3291 - output_activity_loss: 0.5608 - output_timestamp_loss: 0.3952 - output_resource_accuracy: 0.6097 - output_activity_accuracy: 0.7557 - output_timestamp_mean_absolute_error: 0.4000 - val_loss: 2.5620 - val_output_resource_loss: 1.5362 - val_output_activity_loss: 0.6096 - val_output_timestamp_loss: 0.4162 - val_output_resource_accuracy: 0.5739 - val_output_activity_accuracy: 0.7359 - val_output_timestamp_mean_absolute_error: 0.4119 - lr: 0.0037\n",
      "Epoch 25/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.3005 - output_resource_loss: 1.3397 - output_activity_loss: 0.5618 - output_timestamp_loss: 0.3991 - output_resource_accuracy: 0.6105 - output_activity_accuracy: 0.7551 - output_timestamp_mean_absolute_error: 0.4035 - val_loss: 2.5743 - val_output_resource_loss: 1.5405 - val_output_activity_loss: 0.6117 - val_output_timestamp_loss: 0.4221 - val_output_resource_accuracy: 0.5816 - val_output_activity_accuracy: 0.7351 - val_output_timestamp_mean_absolute_error: 0.4174 - lr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_targets_resource_cat = to_categorical(train_targets_resource, num_classes=num_resources)\n",
    "test_targets_resource_cat = to_categorical(test_targets_resource, num_classes=num_resources)\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "\n",
    "# Note: Assuming timestamp targets are continuous and don't need to be converted to categorical\n",
    "\n",
    "history = model.fit([train_resource, train_activity, train_timestamp], \n",
    "                    [train_targets_resource_cat, train_targets_activity_cat, train_targets_timestamp],\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=([test_resource, test_activity, test_timestamp], \n",
    "                                     [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]),\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_scheduler])  # Add other callbacks as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 0s 2ms/step - loss: 2.5743 - output_resource_loss: 1.5405 - output_activity_loss: 0.6117 - output_timestamp_loss: 0.4221 - output_resource_accuracy: 0.5816 - output_activity_accuracy: 0.7351 - output_timestamp_mean_absolute_error: 0.4174\n",
      "[2.574261426925659, 1.540480136871338, 0.6117017865180969, 0.42207974195480347, 0.5816182494163513, 0.7351139187812805, 0.41740697622299194]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(\n",
    "    [test_resource, test_activity, test_timestamp],\n",
    "    [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to an H5 file\n",
    "model.save('dapnn_bpicO.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663/663 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_resource, windows_activity, windows_timestamp])\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_resource = predictions[0]        # ID predictions\n",
    "predictions_activity = predictions[1]  # Resource predictions\n",
    "\n",
    "# Extract predictions for numerical attribute (timestamp)\n",
    "predictions_timestamp = predictions[2] # Timestamp predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def compute_anomaly_scores(predictions, actuals):\n",
    "    # For categorical predictions, convert actuals to one-hot for comparison\n",
    "    actuals_one_hot = to_categorical(actuals, num_classes=predictions.shape[-1])\n",
    "    \n",
    "    max_predictions = np.max(predictions, axis=-1)\n",
    "    actual_predictions = np.sum(predictions * actuals_one_hot, axis=-1)  # Extract the probability of the actual class\n",
    "    \n",
    "    anomaly_scores = (max_predictions - actual_predictions) / max_predictions\n",
    "    \n",
    "    return anomaly_scores\n",
    "\n",
    "# Assuming targets_id, targets_resource, targets_activity, targets_role are the true values for these attributes\n",
    "anomaly_scores_resource = compute_anomaly_scores(predictions_resource, targets_resource)\n",
    "anomaly_scores_activity = compute_anomaly_scores(predictions_activity, targets_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals, normalization_factor):\n",
    "    \"\"\"\n",
    "    Compute anomaly scores for continuous attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predicted values.\n",
    "    - actuals: numpy array of actual values.\n",
    "    - normalization_factor: normalization factor (e.g., standard deviation of the attribute).\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of anomaly scores.\n",
    "    \"\"\"\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize the differences\n",
    "    anomaly_scores = differences / normalization_factor\n",
    "    \n",
    "    return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_factor = np.std(targets_timestamp)       # Example normalization factor (standard deviation)\n",
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp, normalization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_resource = np.array(anomaly_scores_resource).flatten()\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "    anomaly_scores_timestamp = np.array(anomaly_scores_timestamp).flatten()\n",
    "\n",
    "    # Check if all arrays have the same length\n",
    "    if not (len(anomaly_scores_resource) == len(anomaly_scores_activity) == len(anomaly_scores_timestamp)):\n",
    "        raise ValueError(\"All input anomaly scores must have the same length.\")\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_activity, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_activity])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True: anomaly, False: no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21209</th>\n",
       "      <td>5013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21210</th>\n",
       "      <td>5013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21211</th>\n",
       "      <td>5013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21212</th>\n",
       "      <td>5013</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21213</th>\n",
       "      <td>5014</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21214 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted\n",
       "0         0      False\n",
       "1         0      False\n",
       "2         0      False\n",
       "3         1      False\n",
       "4         1      False\n",
       "...     ...        ...\n",
       "21209  5013      False\n",
       "21210  5013      False\n",
       "21211  5013      False\n",
       "21212  5013       True\n",
       "21213  5014      False\n",
       "\n",
       "[21214 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted'] = anomalous_cases\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "5010    False\n",
       "5011    False\n",
       "5012    False\n",
       "5013     True\n",
       "5014    False\n",
       "Name: predicted, Length: 5015, dtype: bool"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_prediction = mapping.groupby('case')['predicted'].any()\n",
    "case_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978e5f534e9f4fab9f7710b6c23e8f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/Model_O.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(1)\n",
    "        else:\n",
    "            conformance_status.append(0)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5015 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              1          1\n",
       "1              0          1\n",
       "2              0          1\n",
       "3              1          1\n",
       "4              0          1\n",
       "...          ...        ...\n",
       "5010           0          1\n",
       "5011           0          1\n",
       "5012           1          1\n",
       "5013           0          0\n",
       "5014           0          1\n",
       "\n",
       "[5015 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth['predicted'] = 1 - ground_truth['predicted']\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.454\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.617\n"
     ]
    }
   ],
   "source": [
    "# Calculate f1\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = TN / (TN + FN)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.027\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = TN / (TN + FP)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.446\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.513681592039801"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming ground_truth is your DataFrame\n",
    "# Make sure 'conformity' contains actual labels (0 or 1)\n",
    "# and 'predicted' contains predicted probabilities or scores\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "auc_roc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
