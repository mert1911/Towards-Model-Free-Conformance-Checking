{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_2365/539992039.py:15: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/mobis.csv', sep=',')  \n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log, \n",
    "        case_id='case', \n",
    "        activity_key='activity', \n",
    "        timestamp_key='start'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>travel_start</th>\n",
       "      <th>travel_end</th>\n",
       "      <th>case</th>\n",
       "      <th>cost</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:40:00+00:00</td>\n",
       "      <td>Travel Department</td>\n",
       "      <td>KS9688</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>2017-01-18 06:31:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>2017-11-22 06:50:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>55804</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>2017-11-22 13:06:00+00:00</td>\n",
       "      <td>Manager</td>\n",
       "      <td>AK7488</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>55805</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>2017-11-29 20:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>55806</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>2017-12-08 09:55:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>55807</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>2017-12-18 08:52:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>55808</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                activity  \\\n",
       "0                                    file travel request   \n",
       "1      check if travel request needs preliminary pric...   \n",
       "2                        decide on approval requirements   \n",
       "3                          check if booking is necessary   \n",
       "4                       check if expense documents exist   \n",
       "...                                                  ...   \n",
       "55804                      confirm travel expense report   \n",
       "55805                  decide on travel expense approval   \n",
       "55806                 send original documents to archive   \n",
       "55807                                 calculate payments   \n",
       "55808                                       pay expenses   \n",
       "\n",
       "                          start                       end               type  \\\n",
       "0     2017-01-17 11:17:00+00:00 2017-01-17 11:23:00+00:00           Employee   \n",
       "1     2017-01-17 11:23:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "2     2017-01-17 11:24:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "3     2017-01-17 11:24:00+00:00 2017-01-17 11:40:00+00:00  Travel Department   \n",
       "4     2017-01-18 05:59:00+00:00 2017-01-18 06:31:00+00:00           Employee   \n",
       "...                         ...                       ...                ...   \n",
       "55804 2017-11-22 06:48:00+00:00 2017-11-22 06:50:00+00:00           Employee   \n",
       "55805 2017-11-22 12:59:00+00:00 2017-11-22 13:06:00+00:00            Manager   \n",
       "55806 2017-11-29 20:12:00+00:00 2017-11-29 20:24:00+00:00           Employee   \n",
       "55807 2017-12-08 09:32:00+00:00 2017-12-08 09:55:00+00:00         Accounting   \n",
       "55808 2017-12-18 08:39:00+00:00 2017-12-18 08:52:00+00:00         Accounting   \n",
       "\n",
       "         user              travel_start                travel_end  case  cost  \\\n",
       "0      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "1      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "2      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "3      KS9688 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "4      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "...       ...                       ...                       ...   ...   ...   \n",
       "55804  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55805  AK7488 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55806  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55807  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55808  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "\n",
       "      case:concept:name                                       concept:name  \\\n",
       "0                   105                                file travel request   \n",
       "1                   105  check if travel request needs preliminary pric...   \n",
       "2                   105                    decide on approval requirements   \n",
       "3                   105                      check if booking is necessary   \n",
       "4                   105                   check if expense documents exist   \n",
       "...                 ...                                                ...   \n",
       "55804              6348                      confirm travel expense report   \n",
       "55805              6348                  decide on travel expense approval   \n",
       "55806              6348                 send original documents to archive   \n",
       "55807              6348                                 calculate payments   \n",
       "55808              6348                                       pay expenses   \n",
       "\n",
       "                 time:timestamp  @@index  @@case_index  \n",
       "0     2017-01-17 11:17:00+00:00        0             0  \n",
       "1     2017-01-17 11:23:00+00:00        1             0  \n",
       "2     2017-01-17 11:24:00+00:00        2             0  \n",
       "3     2017-01-17 11:24:00+00:00        3             0  \n",
       "4     2017-01-18 05:59:00+00:00        4             0  \n",
       "...                         ...      ...           ...  \n",
       "55804 2017-11-22 06:48:00+00:00    55804          3353  \n",
       "55805 2017-11-22 12:59:00+00:00    55805          3353  \n",
       "55806 2017-11-29 20:12:00+00:00    55806          3353  \n",
       "55807 2017-12-08 09:32:00+00:00    55807          3353  \n",
       "55808 2017-12-18 08:39:00+00:00    55808          3353  \n",
       "\n",
       "[55809 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['activity'])\n",
    "dataframe_log['activity'] = codes + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity = dataframe_log[['activity', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 12:16:02.311016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_prefix_windows(df, case_id_column='@@case_index', max_len=None):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "    \n",
    "    for case_id in df[case_id_column].unique():\n",
    "        case_data = df[df[case_id_column] == case_id].drop(columns=[case_id_column]).to_numpy()\n",
    "        \n",
    "        # Optional: Make sure to sort the case data if there's an implicit order (e.g., by timestamps)\n",
    "        # case_data = case_data.sort_values(by='timestamp_column').to_numpy()  # Uncomment and adjust if needed\n",
    "        \n",
    "        for i in range(1, len(case_data)):\n",
    "            window = case_data[:i]\n",
    "            target = case_data[i]\n",
    "            windows.append(window)\n",
    "            targets.append(target)\n",
    "            case_indices.append(case_id)\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = max(len(window) for window in windows)\n",
    "    \n",
    "    # Pad sequences\n",
    "    windows_padded = pad_sequences(windows, maxlen=max_len, padding='post', dtype='float32')\n",
    "    \n",
    "    # Convert targets to numpy array\n",
    "    targets_array = np.array(targets, dtype='float32')\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_padded, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_activity, targets_activity, case_indices = generate_prefix_windows(df_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate Inputs for Each Attribute\n",
    "- Each attribute is passed through an embedding layer\n",
    "- Each attribute has its corresponding GRU encoder\n",
    "- Selective Concatenation: After encoding, the outputs of these GRU layers are concatenated. However, this concatenation is selective, meaning it is structured in a way that prepares the data for effective synthesis without leaking information from the future (next event attributes)\n",
    "- Decoder GRUs: Integrated Decoding: Post-concatenation, the combined attributes are processed through decoder GRU layers. These layers are tasked with integrating the data from different attributes and preparing it for final prediction. This step is where BINet v3 distinguishes itself by effectively using the interdependencies between different attributes to enhance prediction accuracy.\n",
    "- Output Layer: Softmax Output for Each Attribute: For each attribute of the next event, a softmax layer predicts a probability distribution over all possible values. This allows the model to output the most likely next event and its attributes based on the learned dependencies and the history encoded by the GRUs.\n",
    "- E: maximum case length\n",
    "- We train BINet with a GRU size of 2E (two times the maximum case length)\n",
    "- on mini batches of size 500 for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the @@case_index column and count the rows in each group\n",
    "case_lengths = dataframe_log.groupby('@@case_index').size()\n",
    "\n",
    "# Find the maximum value among the case lengths\n",
    "E = case_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activity_input (InputLayer  [(None, None)]            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " activity_embedding (Embedd  (None, None, 50)          1350      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " activity_encoder (GRU)      (None, 98)                44100     \n",
      "                                                                 \n",
      " bn_activity (BatchNormaliz  (None, 98)                392       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 98)                0         \n",
      "                                                                 \n",
      " output_activity (Dense)     (None, 27)                2673      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48515 (189.51 KB)\n",
      "Trainable params: 48319 (188.75 KB)\n",
      "Non-trainable params: 196 (784.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Dropout, BatchNormalization\n",
    "\n",
    "def create_binetv3_for_activity(num_activities, embedding_dim, gru_units, dropout_rate):\n",
    "    # Input layers for activity attribute\n",
    "    input_activity = Input(shape=(None,), name='activity_input')\n",
    "\n",
    "    # Embedding layer for activity attribute\n",
    "    embedding_activity = Embedding(input_dim=num_activities + 1, output_dim=embedding_dim, mask_zero=True, name='activity_embedding')(input_activity)\n",
    "\n",
    "    # Encoder GRU with Batch Normalization for activity\n",
    "    encoded_activity = GRU(units=gru_units, return_sequences=False, name='activity_encoder')(embedding_activity)\n",
    "    bn_activity = BatchNormalization(name='bn_activity')(encoded_activity)\n",
    "\n",
    "    # Dropout layer\n",
    "    dropout_layer = Dropout(rate=dropout_rate, name='dropout')(bn_activity)\n",
    "\n",
    "    # Output layer for predicting the next activity\n",
    "    output_activity = Dense(num_activities + 1, activation='softmax', name='output_activity')(dropout_layer)\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=input_activity, outputs=output_activity)\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "gru_units = int(2 * E) \n",
    "num_activities = dataframe_log['activity'].max()\n",
    "embedding_dim = 50\n",
    "dropout_rate = 0.2\n",
    "model = create_binetv3_for_activity(num_activities, embedding_dim, gru_units, dropout_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for each attribute including timestamps\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities + 1)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "74/74 [==============================] - 17s 167ms/step - loss: 0.9473 - accuracy: 0.7779 - val_loss: 2.7828 - val_accuracy: 0.7552\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 12s 158ms/step - loss: 0.2957 - accuracy: 0.8783 - val_loss: 2.4665 - val_accuracy: 0.8882\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 12s 159ms/step - loss: 0.2750 - accuracy: 0.8809 - val_loss: 2.0526 - val_accuracy: 0.8882\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 13s 175ms/step - loss: 0.2694 - accuracy: 0.8814 - val_loss: 1.5436 - val_accuracy: 0.8763\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 13s 170ms/step - loss: 0.2661 - accuracy: 0.8832 - val_loss: 1.0091 - val_accuracy: 0.8758\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 13s 173ms/step - loss: 0.2635 - accuracy: 0.8833 - val_loss: 0.5922 - val_accuracy: 0.8891\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 12s 156ms/step - loss: 0.2607 - accuracy: 0.8848 - val_loss: 0.3789 - val_accuracy: 0.8864\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 13s 178ms/step - loss: 0.2600 - accuracy: 0.8853 - val_loss: 0.2878 - val_accuracy: 0.8843\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 14s 189ms/step - loss: 0.2590 - accuracy: 0.8860 - val_loss: 0.2623 - val_accuracy: 0.8865\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 12s 163ms/step - loss: 0.2573 - accuracy: 0.8863 - val_loss: 0.2552 - val_accuracy: 0.8898\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 12s 162ms/step - loss: 0.2555 - accuracy: 0.8865 - val_loss: 0.2540 - val_accuracy: 0.8893\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 12s 157ms/step - loss: 0.2556 - accuracy: 0.8858 - val_loss: 0.2552 - val_accuracy: 0.8897\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 12s 166ms/step - loss: 0.2552 - accuracy: 0.8868 - val_loss: 0.2591 - val_accuracy: 0.8885\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 12s 158ms/step - loss: 0.2543 - accuracy: 0.8872 - val_loss: 0.2538 - val_accuracy: 0.8899\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 12s 159ms/step - loss: 0.2547 - accuracy: 0.8873 - val_loss: 0.2527 - val_accuracy: 0.8899\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 12s 158ms/step - loss: 0.2543 - accuracy: 0.8861 - val_loss: 0.2527 - val_accuracy: 0.8899\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 12s 159ms/step - loss: 0.2537 - accuracy: 0.8862 - val_loss: 0.2540 - val_accuracy: 0.8901\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 13s 175ms/step - loss: 0.2540 - accuracy: 0.8870 - val_loss: 0.2537 - val_accuracy: 0.8871\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 13s 178ms/step - loss: 0.2531 - accuracy: 0.8867 - val_loss: 0.2516 - val_accuracy: 0.8894\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 12s 162ms/step - loss: 0.2521 - accuracy: 0.8873 - val_loss: 0.2558 - val_accuracy: 0.8901\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_activity,  # Only the activity input\n",
    "    train_targets_activity_cat,  # Only the activity targets\n",
    "    validation_data=(test_activity, test_targets_activity_cat),  # Only the activity input and targets for validation\n",
    "    epochs=20,\n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 2s 8ms/step - loss: 0.2558 - accuracy: 0.8901\n",
      "Validation Loss: 0.2557588815689087, Validation Accuracy: 0.890067994594574\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "results = model.evaluate(\n",
    "    test_activity,  # Only the activity input\n",
    "    test_targets_activity_cat,  # Only the activity targets\n",
    "    batch_size=64\n",
    ")\n",
    "print(f\"Validation Loss: {results[0]}, Validation Accuracy: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to an H5 file\n",
    "model.save('mobis.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each event attribute, BINet's softmax layer outputs a probability distribution over possible values\n",
    "- The anomaly score for a specific attribute value v is calculated by summing all the probabilities from the softmax output that are greater than the probability assigned to v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640/1640 [==============================] - 12s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions_activity = model.predict([windows_activity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_scores(predictions, targets):\n",
    "    scores = []\n",
    "    # Loop through each example in the predictions\n",
    "    for i in range(predictions.shape[0]):\n",
    "        actual_prob = predictions[i, targets[i]]  # Extract the probability of the true class using target index\n",
    "        # Calculate anomaly score as sum of probabilities greater than the probability of the actual value\n",
    "        anomaly_score = np.sum(predictions[i][predictions[i] > actual_prob])\n",
    "        scores.append(anomaly_score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_activity = calculate_anomaly_scores(predictions_activity, targets_activity.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold (lowest plateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_ratio(scores, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the anomaly ratio for a given threshold.\n",
    "    \"\"\"\n",
    "    return np.mean(scores > threshold)\n",
    "\n",
    "def find_plateaus(scores, epsilon=1e-4, min_plateau_length=10):\n",
    "    \"\"\"\n",
    "    Identify the lowest plateau in the anomaly ratio function and calculate the mean-centered threshold.\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)  # Convert scores to a NumPy array\n",
    "    sorted_scores = np.sort(scores)\n",
    "    \n",
    "    # Remove duplicate values\n",
    "    unique_thresholds, unique_indices = np.unique(sorted_scores, return_index=True)\n",
    "    anomaly_ratios = np.array([calculate_anomaly_ratio(scores, t) for t in unique_thresholds])\n",
    "    \n",
    "    # Calculate first and second derivatives\n",
    "    first_derivatives = np.diff(anomaly_ratios) / np.diff(unique_thresholds)\n",
    "    second_derivatives = np.diff(first_derivatives) / np.diff(unique_thresholds[:-1])\n",
    "    \n",
    "    # Identify plateaus where the first derivative is close to zero\n",
    "    plateau_indices = np.where(np.abs(first_derivatives) < epsilon)[0]\n",
    "    \n",
    "    # Group consecutive indices to identify continuous plateaus\n",
    "    grouped_plateaus = np.split(plateau_indices, np.where(np.diff(plateau_indices) != 1)[0] + 1)\n",
    "    \n",
    "    # Filter plateaus based on minimum length\n",
    "    long_plateaus = [g for g in grouped_plateaus if len(g) >= min_plateau_length]\n",
    "    \n",
    "    if long_plateaus:\n",
    "        # Take the first long plateau and find the mean threshold in this plateau\n",
    "        first_plateau = long_plateaus[0]\n",
    "        plateau_thresholds = unique_thresholds[first_plateau]\n",
    "        return np.mean(plateau_thresholds)\n",
    "    else:\n",
    "        # If no plateau is found, return a default value, e.g., the 90th percentile\n",
    "        percentile_90 = np.percentile(sorted_scores, 90)\n",
    "        if percentile_90 == 1.0:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return percentile_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_activity = find_plateaus(anomaly_scores_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(anomaly_scores, threshold):\n",
    "    labels = [1 if score > threshold else 0 for score in anomaly_scores]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies based on the calculated anomaly scores and thresholds\n",
    "labels_activity = detect_anomalies(anomaly_scores_activity, threshold_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52450</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52451</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52452</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52453</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52454</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted_activity\n",
       "0         0                   0\n",
       "1         0                   0\n",
       "2         0                   0\n",
       "3         0                   0\n",
       "4         0                   0\n",
       "...     ...                 ...\n",
       "52450  3353                   0\n",
       "52451  3353                   0\n",
       "52452  3353                   0\n",
       "52453  3353                   0\n",
       "52454  3353                   0\n",
       "\n",
       "[52455 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted_activity'] = labels_activity\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1        True\n",
       "2       False\n",
       "3        True\n",
       "4        True\n",
       "        ...  \n",
       "3349     True\n",
       "3350     True\n",
       "3351     True\n",
       "3352     True\n",
       "3353     True\n",
       "Length: 3354, dtype: bool"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean DataFrame where each value is True if the value is 1\n",
    "contains_one = (mapping[['predicted_activity']] == 1)\n",
    "\n",
    "# Group by 'case' and check if there's at least one 'True' in any of the columns\n",
    "case_prediction = contains_one.groupby(mapping['case']).any().any(axis=1)\n",
    "case_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15aac07c8b2a4f848b5ba0ab2f12a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/MobisToBe.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(0)\n",
    "        else:\n",
    "            conformance_status.append(1)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              0          0\n",
       "1              1          1\n",
       "2              0          0\n",
       "3              1          1\n",
       "4              1          1\n",
       "...          ...        ...\n",
       "3349           0          1\n",
       "3350           1          1\n",
       "3351           0          1\n",
       "3352           0          1\n",
       "3353           0          1\n",
       "\n",
       "[3354 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Dev: 0.72\n"
     ]
    }
   ],
   "source": [
    "precision_dev = TP / (TP + FP)\n",
    "print(f\"Precision Dev: {precision_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Dev: 1.00\n"
     ]
    }
   ],
   "source": [
    "recall_dev = TP / (TP + FN)\n",
    "print(f\"Recall Dev: {recall_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision No Dev: 1.00\n"
     ]
    }
   ],
   "source": [
    "precision_no_dev = TN / (TN + FN)\n",
    "print(f\"Precision No Dev: {precision_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall No Dev: 0.61\n"
     ]
    }
   ],
   "source": [
    "recall_no_dev = TN / (TN + FP)\n",
    "print(f\"Recall No Dev: {recall_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
