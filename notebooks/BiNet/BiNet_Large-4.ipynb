{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"     # Format the dataframe\\n    dataframe_log = pm4py.format_dataframe(\\n        dataframe_log,\\n        case_id='case:concept:name',\\n        activity_key='concept:name',\\n        timestamp_key='time:timestamp'\\n    )\\n\\n    # Convert the dataframe to event log\\n    log = log_converter.apply(dataframe_log) \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/extra_logs/large-0.3-4.csv', sep=',')\n",
    "\n",
    "\n",
    "\"\"\"     # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>trace_id</th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>day</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Activity A</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>Dontechi</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Della</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Activity Q</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>Blackzim</td>\n",
       "      <td>Grenada</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Roy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Activity R</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>Xx-holding</td>\n",
       "      <td>Sierra Leone</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Maryellen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Activity U</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>Silis</td>\n",
       "      <td>Mayotte</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Grayce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Activity V</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>Goodsilron</td>\n",
       "      <td>China</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Sherell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53380</th>\n",
       "      <td>Activity L</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>Plusstrip</td>\n",
       "      <td>India</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Micheline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53381</th>\n",
       "      <td>Activity J</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>Donquadtech</td>\n",
       "      <td>Guam</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53382</th>\n",
       "      <td>Activity E</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>Streethex</td>\n",
       "      <td>San Marino</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evelin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53383</th>\n",
       "      <td>Activity G</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>Ron-tech</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Pam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53384</th>\n",
       "      <td>Activity B</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>Plusstrip</td>\n",
       "      <td>Fiji</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Amanda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53385 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name       anomaly  trace_id      company       country      day  \\\n",
       "0      Activity A  SkipSequence         1     Dontechi       Nigeria   Friday   \n",
       "1      Activity Q  SkipSequence         1     Blackzim       Grenada   Monday   \n",
       "2      Activity R  SkipSequence         1   Xx-holding  Sierra Leone  Tuesday   \n",
       "3      Activity U  SkipSequence         1        Silis       Mayotte   Friday   \n",
       "4      Activity V  SkipSequence         1   Goodsilron         China   Friday   \n",
       "...           ...           ...       ...          ...           ...      ...   \n",
       "53380  Activity L          Late      5000    Plusstrip         India   Friday   \n",
       "53381  Activity J          Late      5000  Donquadtech          Guam   Monday   \n",
       "53382  Activity E          Late      5000    Streethex    San Marino   Friday   \n",
       "53383  Activity G          Late      5000     Ron-tech        Brazil   Monday   \n",
       "53384  Activity B          Late      5000    Plusstrip          Fiji  Tuesday   \n",
       "\n",
       "            user  \n",
       "0          Della  \n",
       "1            Roy  \n",
       "2      Maryellen  \n",
       "3         Grayce  \n",
       "4        Sherell  \n",
       "...          ...  \n",
       "53380  Micheline  \n",
       "53381       Chan  \n",
       "53382     Evelin  \n",
       "53383        Pam  \n",
       "53384     Amanda  \n",
       "\n",
       "[53385 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['timestamp_end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['name'])\n",
    "dataframe_log['name'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['day'])\n",
    "dataframe_log['day'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['user'])\n",
    "dataframe_log['user'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['country'])\n",
    "dataframe_log['country'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(dataframe_log['company'])\n",
    "dataframe_log['company'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>trace_id</th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>day</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SkipSequence</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53380</th>\n",
       "      <td>23</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53381</th>\n",
       "      <td>19</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>41</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53382</th>\n",
       "      <td>13</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>58</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53383</th>\n",
       "      <td>14</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53384</th>\n",
       "      <td>7</td>\n",
       "      <td>Late</td>\n",
       "      <td>5000</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53385 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name       anomaly  trace_id  company  country  day  user\n",
       "0         0  SkipSequence         1        0        0    0     0\n",
       "1         1  SkipSequence         1        1        1    1     1\n",
       "2         2  SkipSequence         1        2        2    2     2\n",
       "3         3  SkipSequence         1        3        3    0     3\n",
       "4         4  SkipSequence         1        4        4    0     4\n",
       "...     ...           ...       ...      ...      ...  ...   ...\n",
       "53380    23          Late      5000       17       37    0    18\n",
       "53381    19          Late      5000       41       50    1    22\n",
       "53382    13          Late      5000       58       83    0    28\n",
       "53383    14          Late      5000       23       32    1    29\n",
       "53384     7          Late      5000       17       20    2    30\n",
       "\n",
       "[53385 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity = dataframe_log[['name', 'trace_id']]\n",
    "df_day = dataframe_log[['day', 'trace_id']]\n",
    "df_user = dataframe_log[['user', 'trace_id']]\n",
    "df_country = dataframe_log[['country', 'trace_id']]\n",
    "df_company = dataframe_log[['company', 'trace_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 11:47:30.290149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_prefix_windows(df, case_id_column='trace_id', max_len=None, dtype='int32'):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        case_data = df[df[case_id_column] == case_id].drop(columns=[case_id_column]).to_numpy()\n",
    "        \n",
    "        # Optional: Make sure to sort the case data if there's an implicit order (e.g., by timestamps)\n",
    "        # case_data = case_data.sort_values(by='timestamp_column').to_numpy()  # Uncomment and adjust if needed\n",
    "        \n",
    "        for i in range(1, len(case_data)):\n",
    "            window = case_data[:i]\n",
    "            target = case_data[i]\n",
    "            windows.append(window.flatten())  # Flatten because we no longer want one-hot encoding\n",
    "            targets.append(target[0])  # Assume that the target is the first element (activity)\n",
    "            case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = max(len(window) for window in windows)\n",
    "    windows_padded = pad_sequences(windows, maxlen=max_len, padding='post', dtype=dtype)\n",
    "\n",
    "    # Ensure targets and case_indices are numpy arrays\n",
    "    targets_array = np.array(targets, dtype=dtype)\n",
    "    case_indices_array = np.array(case_indices, dtype=dtype)\n",
    "\n",
    "    # Check for length consistency\n",
    "    assert len(windows_padded) == len(targets_array) == len(case_indices_array), \\\n",
    "        \"Length of windows, targets, and case indices arrays must be equal.\"\n",
    "\n",
    "    return np.array(windows_padded), targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_activity, targets_activity, case_indices = generate_prefix_windows(df_activity)\n",
    "windows_day, targets_day, case_indices = generate_prefix_windows(df_day)\n",
    "windows_user, targets_user, case_indices = generate_prefix_windows(df_user)\n",
    "windows_country, targets_country, case_indices = generate_prefix_windows(df_country)\n",
    "windows_company, targets_company, case_indices = generate_prefix_windows(df_company)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate Inputs for Each Attribute\n",
    "- Each attribute is passed through an embedding layer\n",
    "- Each attribute has its corresponding GRU encoder\n",
    "- Selective Concatenation: After encoding, the outputs of these GRU layers are concatenated. However, this concatenation is selective, meaning it is structured in a way that prepares the data for effective synthesis without leaking information from the future (next event attributes)\n",
    "- Decoder GRUs: Integrated Decoding: Post-concatenation, the combined attributes are processed through decoder GRU layers. These layers are tasked with integrating the data from different attributes and preparing it for final prediction. This step is where BINet v3 distinguishes itself by effectively using the interdependencies between different attributes to enhance prediction accuracy.\n",
    "- Output Layer: Softmax Output for Each Attribute: For each attribute of the next event, a softmax layer predicts a probability distribution over all possible values. This allows the model to output the most likely next event and its attributes based on the learned dependencies and the history encoded by the GRUs.\n",
    "- E: maximum case length\n",
    "- We train BINet with a GRU size of 2E (two times the maximum case length)\n",
    "- on mini batches of size 500 for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the @@case_index column and count the rows in each group\n",
    "case_lengths = dataframe_log.groupby('trace_id').size()\n",
    "\n",
    "# Find the maximum value among the case lengths\n",
    "E = case_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " activity_input (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " day_input (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " user_input (InputLayer)     [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " country_input (InputLayer)  [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " company_input (InputLayer)  [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " activity_embedding (Embedd  (None, None, 50)             4150      ['activity_input[0][0]']      \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " day_embedding (Embedding)   (None, None, 50)             550       ['day_input[0][0]']           \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)  (None, None, 50)             6900      ['user_input[0][0]']          \n",
      "                                                                                                  \n",
      " country_embedding (Embeddi  (None, None, 50)             19400     ['country_input[0][0]']       \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " company_embedding (Embeddi  (None, None, 50)             7450      ['company_input[0][0]']       \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " activity_encoder (GRU)      (None, None, 30)             7380      ['activity_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " day_encoder (GRU)           (None, None, 30)             7380      ['day_embedding[0][0]']       \n",
      "                                                                                                  \n",
      " user_encoder (GRU)          (None, None, 30)             7380      ['user_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " country_encoder (GRU)       (None, None, 30)             7380      ['country_embedding[0][0]']   \n",
      "                                                                                                  \n",
      " company_encoder (GRU)       (None, None, 30)             7380      ['company_embedding[0][0]']   \n",
      "                                                                                                  \n",
      " bn_activity (BatchNormaliz  (None, None, 30)             120       ['activity_encoder[0][0]']    \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " bn_day (BatchNormalization  (None, None, 30)             120       ['day_encoder[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bn_user (BatchNormalizatio  (None, None, 30)             120       ['user_encoder[0][0]']        \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " bn_country (BatchNormaliza  (None, None, 30)             120       ['country_encoder[0][0]']     \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " bn_company (BatchNormaliza  (None, None, 30)             120       ['company_encoder[0][0]']     \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_encodings (Con  (None, None, 150)            0         ['bn_activity[0][0]',         \n",
      " catenate)                                                           'bn_day[0][0]',              \n",
      "                                                                     'bn_user[0][0]',             \n",
      "                                                                     'bn_country[0][0]',          \n",
      "                                                                     'bn_company[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)           (None, 30)                   16380     ['concatenate_encodings[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 30)                   0         ['decoder_gru[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 83)                   2573      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_day (Dense)          (None, 11)                   341       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_user (Dense)         (None, 138)                  4278      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_country (Dense)      (None, 388)                  12028     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " output_company (Dense)      (None, 149)                  4619      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 116169 (453.79 KB)\n",
      "Trainable params: 115869 (452.61 KB)\n",
      "Non-trainable params: 300 (1.17 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Dropout, Concatenate, BatchNormalization\n",
    "\n",
    "def create_binetv3(num_activities, num_days, num_users, num_countries, num_companies, embedding_dim, gru_units, dropout_rate):\n",
    "    # Input layers for each attribute\n",
    "    input_activity = Input(shape=(None,), name='activity_input')\n",
    "    input_day = Input(shape=(None,), name='day_input')\n",
    "    input_user = Input(shape=(None,), name='user_input')\n",
    "    input_country = Input(shape=(None,), name='country_input')\n",
    "    input_company = Input(shape=(None,), name='company_input')\n",
    "\n",
    "    # Embedding layers for categorical attributes\n",
    "    embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim, name='activity_embedding')(input_activity)\n",
    "    embedding_day = Embedding(input_dim=num_days, output_dim=embedding_dim, name='day_embedding')(input_day)\n",
    "    embedding_user = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(input_user)\n",
    "    embedding_country = Embedding(input_dim=num_countries, output_dim=embedding_dim, name='country_embedding')(input_country)\n",
    "    embedding_company = Embedding(input_dim=num_companies, output_dim=embedding_dim, name='company_embedding')(input_company)\n",
    "\n",
    "    # Encoder GRUs with Batch Normalization for categorical attributes\n",
    "    encoded_activity = GRU(units=gru_units, return_sequences=True, name='activity_encoder')(embedding_activity)\n",
    "    bn_activity = BatchNormalization(name='bn_activity')(encoded_activity)\n",
    "    encoded_day = GRU(units=gru_units, return_sequences=True, name='day_encoder')(embedding_day)\n",
    "    bn_day = BatchNormalization(name='bn_day')(encoded_day)\n",
    "    encoded_user = GRU(units=gru_units, return_sequences=True, name='user_encoder')(embedding_user)\n",
    "    bn_user = BatchNormalization(name='bn_user')(encoded_user)\n",
    "    encoded_country = GRU(units=gru_units, return_sequences=True, name='country_encoder')(embedding_country)\n",
    "    bn_country = BatchNormalization(name='bn_country')(encoded_country)\n",
    "    encoded_company = GRU(units=gru_units, return_sequences=True, name='company_encoder')(embedding_company)\n",
    "    bn_company = BatchNormalization(name='bn_company')(encoded_company)\n",
    "\n",
    "    # Concatenation of encoded outputs\n",
    "    concatenated = Concatenate(name='concatenate_encodings')([bn_activity, bn_day, bn_user, bn_country, bn_company])\n",
    "\n",
    "    # Decoder GRU\n",
    "    decoder_output = GRU(units=gru_units, return_sequences=False, name='decoder_gru')(concatenated)\n",
    "    dropout_layer = Dropout(rate=dropout_rate, name='dropout')(decoder_output)\n",
    "\n",
    "    # Output layers for predicting the next event's attributes\n",
    "    output_activity = Dense(num_activities, activation='softmax', name='output_activity')(dropout_layer)\n",
    "    output_day = Dense(num_days, activation='softmax', name='output_day')(dropout_layer)\n",
    "    output_user = Dense(num_users, activation='softmax', name='output_user')(dropout_layer)\n",
    "    output_country = Dense(num_countries, activation='softmax', name='output_country')(dropout_layer)\n",
    "    output_company = Dense(num_companies, activation='softmax', name='output_company')(dropout_layer)\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[input_activity, input_day, input_user, input_country, input_company], outputs=[output_activity, output_day, output_user, output_country, output_company])\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss={\n",
    "            'output_activity': 'categorical_crossentropy', \n",
    "            'output_day': 'categorical_crossentropy',\n",
    "            'output_user': 'categorical_crossentropy',\n",
    "            'output_country': 'categorical_crossentropy',\n",
    "            'output_company': 'categorical_crossentropy'\n",
    "        },\n",
    "        metrics={\n",
    "            'output_activity': ['accuracy'], \n",
    "            'output_day': ['accuracy'],\n",
    "            'output_user': ['accuracy'],\n",
    "            'output_country': ['accuracy'],\n",
    "            'output_company': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "gru_units = int(2 * E) \n",
    "num_activities = dataframe_log['name'].nunique()\n",
    "num_days = dataframe_log['day'].nunique()\n",
    "num_users = dataframe_log['user'].nunique()\n",
    "num_countries = dataframe_log['country'].nunique()\n",
    "num_companies = dataframe_log['company'].nunique()\n",
    "embedding_dim = 50\n",
    "dropout_rate = 0.2\n",
    "model = create_binetv3(num_activities, num_days, num_users, num_countries, num_companies, embedding_dim, gru_units, dropout_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for the activity attribute\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for the day attribute\n",
    "train_day, test_day, train_targets_day, test_targets_day = train_test_split(\n",
    "    windows_day, targets_day, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for the user attribute\n",
    "train_user, test_user, train_targets_user, test_targets_user = train_test_split(\n",
    "    windows_user, targets_user, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for the country attribute\n",
    "train_country, test_country, train_targets_country, test_targets_country = train_test_split(\n",
    "    windows_country, targets_country, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for the company attribute\n",
    "train_company, test_company, train_targets_company, test_targets_company = train_test_split(\n",
    "    windows_company, targets_company, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to categorical format\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "train_targets_day_cat = to_categorical(train_targets_day, num_classes=num_days)\n",
    "test_targets_day_cat = to_categorical(test_targets_day, num_classes=num_days)\n",
    "\n",
    "train_targets_user_cat = to_categorical(train_targets_user, num_classes=num_users)\n",
    "test_targets_user_cat = to_categorical(test_targets_user, num_classes=num_users)\n",
    "\n",
    "train_targets_country_cat = to_categorical(train_targets_country, num_classes=num_countries)\n",
    "test_targets_country_cat = to_categorical(test_targets_country, num_classes=num_countries)\n",
    "\n",
    "train_targets_company_cat = to_categorical(train_targets_company, num_classes=num_companies)\n",
    "test_targets_company_cat = to_categorical(test_targets_company, num_classes=num_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "68/68 [==============================] - 27s 183ms/step - loss: 19.1424 - output_activity_loss: 3.6758 - output_day_loss: 1.7834 - output_user_loss: 4.2934 - output_country_loss: 5.1365 - output_company_loss: 4.2533 - output_activity_accuracy: 0.1496 - output_day_accuracy: 0.2828 - output_user_accuracy: 0.0675 - output_country_accuracy: 0.0457 - output_company_accuracy: 0.0886 - val_loss: 18.5047 - val_output_activity_loss: 3.6165 - val_output_day_loss: 1.7323 - val_output_user_loss: 4.1329 - val_output_country_loss: 4.9058 - val_output_company_loss: 4.1172 - val_output_activity_accuracy: 0.2220 - val_output_day_accuracy: 0.2689 - val_output_user_accuracy: 0.0784 - val_output_country_accuracy: 0.0566 - val_output_company_accuracy: 0.0716\n",
      "Epoch 2/20\n",
      "68/68 [==============================] - 10s 142ms/step - loss: 14.6290 - output_activity_loss: 2.5120 - output_day_loss: 1.5416 - output_user_loss: 3.4249 - output_country_loss: 3.9106 - output_company_loss: 3.2398 - output_activity_accuracy: 0.3990 - output_day_accuracy: 0.3842 - output_user_accuracy: 0.1747 - output_country_accuracy: 0.2069 - output_company_accuracy: 0.2408 - val_loss: 16.0521 - val_output_activity_loss: 3.0391 - val_output_day_loss: 1.6004 - val_output_user_loss: 3.6497 - val_output_country_loss: 4.1903 - val_output_company_loss: 3.5726 - val_output_activity_accuracy: 0.3159 - val_output_day_accuracy: 0.2849 - val_output_user_accuracy: 0.1429 - val_output_country_accuracy: 0.1464 - val_output_company_accuracy: 0.2207\n",
      "Epoch 3/20\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 12.0392 - output_activity_loss: 1.7793 - output_day_loss: 1.4496 - output_user_loss: 2.9243 - output_country_loss: 3.2234 - output_company_loss: 2.6626 - output_activity_accuracy: 0.6074 - output_day_accuracy: 0.4406 - output_user_accuracy: 0.2930 - output_country_accuracy: 0.3363 - output_company_accuracy: 0.3905 - val_loss: 13.4829 - val_output_activity_loss: 2.2963 - val_output_day_loss: 1.4900 - val_output_user_loss: 3.1486 - val_output_country_loss: 3.5519 - val_output_company_loss: 2.9961 - val_output_activity_accuracy: 0.4945 - val_output_day_accuracy: 0.3653 - val_output_user_accuracy: 0.3211 - val_output_country_accuracy: 0.1928 - val_output_company_accuracy: 0.2840\n",
      "Epoch 4/20\n",
      "68/68 [==============================] - 14s 201ms/step - loss: 10.1231 - output_activity_loss: 1.2405 - output_day_loss: 1.3694 - output_user_loss: 2.5577 - output_country_loss: 2.7230 - output_company_loss: 2.2325 - output_activity_accuracy: 0.8026 - output_day_accuracy: 0.4833 - output_user_accuracy: 0.4060 - output_country_accuracy: 0.4326 - output_company_accuracy: 0.5018 - val_loss: 10.7538 - val_output_activity_loss: 1.5000 - val_output_day_loss: 1.3359 - val_output_user_loss: 2.6407 - val_output_country_loss: 2.8896 - val_output_company_loss: 2.3876 - val_output_activity_accuracy: 0.8046 - val_output_day_accuracy: 0.5074 - val_output_user_accuracy: 0.4217 - val_output_country_accuracy: 0.4048 - val_output_company_accuracy: 0.4655\n",
      "Epoch 5/20\n",
      "68/68 [==============================] - 14s 207ms/step - loss: 8.8037 - output_activity_loss: 0.9257 - output_day_loss: 1.3087 - output_user_loss: 2.2805 - output_country_loss: 2.3536 - output_company_loss: 1.9352 - output_activity_accuracy: 0.8774 - output_day_accuracy: 0.5174 - output_user_accuracy: 0.4716 - output_country_accuracy: 0.4950 - output_company_accuracy: 0.5668 - val_loss: 8.8373 - val_output_activity_loss: 0.9815 - val_output_day_loss: 1.2377 - val_output_user_loss: 2.2746 - val_output_country_loss: 2.3808 - val_output_company_loss: 1.9627 - val_output_activity_accuracy: 0.8972 - val_output_day_accuracy: 0.5678 - val_output_user_accuracy: 0.5032 - val_output_country_accuracy: 0.4792 - val_output_company_accuracy: 0.5744\n",
      "Epoch 6/20\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 7.9352 - output_activity_loss: 0.7404 - output_day_loss: 1.2664 - output_user_loss: 2.0821 - output_country_loss: 2.0965 - output_company_loss: 1.7499 - output_activity_accuracy: 0.9086 - output_day_accuracy: 0.5333 - output_user_accuracy: 0.5006 - output_country_accuracy: 0.5269 - output_company_accuracy: 0.5899 - val_loss: 7.6726 - val_output_activity_loss: 0.7030 - val_output_day_loss: 1.1880 - val_output_user_loss: 2.0282 - val_output_country_loss: 2.0451 - val_output_company_loss: 1.7083 - val_output_activity_accuracy: 0.9244 - val_output_day_accuracy: 0.5837 - val_output_user_accuracy: 0.5145 - val_output_country_accuracy: 0.5429 - val_output_company_accuracy: 0.6001\n",
      "Epoch 7/20\n",
      "68/68 [==============================] - 9s 126ms/step - loss: 7.3432 - output_activity_loss: 0.6248 - output_day_loss: 1.2316 - output_user_loss: 1.9473 - output_country_loss: 1.9113 - output_company_loss: 1.6282 - output_activity_accuracy: 0.9244 - output_day_accuracy: 0.5469 - output_user_accuracy: 0.5128 - output_country_accuracy: 0.5415 - output_company_accuracy: 0.5971 - val_loss: 6.9396 - val_output_activity_loss: 0.5537 - val_output_day_loss: 1.1481 - val_output_user_loss: 1.8587 - val_output_country_loss: 1.8237 - val_output_company_loss: 1.5554 - val_output_activity_accuracy: 0.9335 - val_output_day_accuracy: 0.5862 - val_output_user_accuracy: 0.5291 - val_output_country_accuracy: 0.5601 - val_output_company_accuracy: 0.6071\n",
      "Epoch 8/20\n",
      "68/68 [==============================] - 9s 140ms/step - loss: 6.9074 - output_activity_loss: 0.5473 - output_day_loss: 1.2034 - output_user_loss: 1.8482 - output_country_loss: 1.7788 - output_company_loss: 1.5296 - output_activity_accuracy: 0.9329 - output_day_accuracy: 0.5596 - output_user_accuracy: 0.5200 - output_country_accuracy: 0.5550 - output_company_accuracy: 0.6092 - val_loss: 6.4533 - val_output_activity_loss: 0.4678 - val_output_day_loss: 1.1154 - val_output_user_loss: 1.7395 - val_output_country_loss: 1.6824 - val_output_company_loss: 1.4483 - val_output_activity_accuracy: 0.9357 - val_output_day_accuracy: 0.5998 - val_output_user_accuracy: 0.5451 - val_output_country_accuracy: 0.5681 - val_output_company_accuracy: 0.6219\n",
      "Epoch 9/20\n",
      "68/68 [==============================] - 10s 140ms/step - loss: 6.5697 - output_activity_loss: 0.4875 - output_day_loss: 1.1812 - output_user_loss: 1.7646 - output_country_loss: 1.6803 - output_company_loss: 1.4561 - output_activity_accuracy: 0.9397 - output_day_accuracy: 0.5648 - output_user_accuracy: 0.5292 - output_country_accuracy: 0.5614 - output_company_accuracy: 0.6166 - val_loss: 6.1244 - val_output_activity_loss: 0.4147 - val_output_day_loss: 1.0972 - val_output_user_loss: 1.6558 - val_output_country_loss: 1.5830 - val_output_company_loss: 1.3736 - val_output_activity_accuracy: 0.9447 - val_output_day_accuracy: 0.6023 - val_output_user_accuracy: 0.5533 - val_output_country_accuracy: 0.5754 - val_output_company_accuracy: 0.6332\n",
      "Epoch 10/20\n",
      "68/68 [==============================] - 9s 136ms/step - loss: 6.3320 - output_activity_loss: 0.4514 - output_day_loss: 1.1574 - output_user_loss: 1.7064 - output_country_loss: 1.6105 - output_company_loss: 1.4063 - output_activity_accuracy: 0.9415 - output_day_accuracy: 0.5713 - output_user_accuracy: 0.5357 - output_country_accuracy: 0.5653 - output_company_accuracy: 0.6211 - val_loss: 5.8920 - val_output_activity_loss: 0.3837 - val_output_day_loss: 1.0749 - val_output_user_loss: 1.5998 - val_output_country_loss: 1.5153 - val_output_company_loss: 1.3183 - val_output_activity_accuracy: 0.9469 - val_output_day_accuracy: 0.6031 - val_output_user_accuracy: 0.5555 - val_output_country_accuracy: 0.5750 - val_output_company_accuracy: 0.6361\n",
      "Epoch 11/20\n",
      "68/68 [==============================] - 10s 148ms/step - loss: 6.1233 - output_activity_loss: 0.4202 - output_day_loss: 1.1433 - output_user_loss: 1.6549 - output_country_loss: 1.5464 - output_company_loss: 1.3585 - output_activity_accuracy: 0.9449 - output_day_accuracy: 0.5768 - output_user_accuracy: 0.5387 - output_country_accuracy: 0.5690 - output_company_accuracy: 0.6240 - val_loss: 5.7353 - val_output_activity_loss: 0.3643 - val_output_day_loss: 1.0647 - val_output_user_loss: 1.5539 - val_output_country_loss: 1.4657 - val_output_company_loss: 1.2866 - val_output_activity_accuracy: 0.9470 - val_output_day_accuracy: 0.6068 - val_output_user_accuracy: 0.5629 - val_output_country_accuracy: 0.5783 - val_output_company_accuracy: 0.6418\n",
      "Epoch 12/20\n",
      "68/68 [==============================] - 10s 143ms/step - loss: 5.9624 - output_activity_loss: 0.3979 - output_day_loss: 1.1262 - output_user_loss: 1.6154 - output_country_loss: 1.5011 - output_company_loss: 1.3219 - output_activity_accuracy: 0.9467 - output_day_accuracy: 0.5774 - output_user_accuracy: 0.5403 - output_country_accuracy: 0.5733 - output_company_accuracy: 0.6279 - val_loss: 5.6180 - val_output_activity_loss: 0.3547 - val_output_day_loss: 1.0525 - val_output_user_loss: 1.5216 - val_output_country_loss: 1.4306 - val_output_company_loss: 1.2587 - val_output_activity_accuracy: 0.9504 - val_output_day_accuracy: 0.6113 - val_output_user_accuracy: 0.5668 - val_output_country_accuracy: 0.5827 - val_output_company_accuracy: 0.6466\n",
      "Epoch 13/20\n",
      "68/68 [==============================] - 10s 140ms/step - loss: 5.8341 - output_activity_loss: 0.3820 - output_day_loss: 1.1127 - output_user_loss: 1.5789 - output_country_loss: 1.4616 - output_company_loss: 1.2989 - output_activity_accuracy: 0.9501 - output_day_accuracy: 0.5838 - output_user_accuracy: 0.5471 - output_country_accuracy: 0.5757 - output_company_accuracy: 0.6321 - val_loss: 5.4916 - val_output_activity_loss: 0.3378 - val_output_day_loss: 1.0404 - val_output_user_loss: 1.4857 - val_output_country_loss: 1.3968 - val_output_company_loss: 1.2308 - val_output_activity_accuracy: 0.9520 - val_output_day_accuracy: 0.6179 - val_output_user_accuracy: 0.5666 - val_output_country_accuracy: 0.5869 - val_output_company_accuracy: 0.6466\n",
      "Epoch 14/20\n",
      "68/68 [==============================] - 10s 154ms/step - loss: 5.7250 - output_activity_loss: 0.3673 - output_day_loss: 1.1031 - output_user_loss: 1.5490 - output_country_loss: 1.4313 - output_company_loss: 1.2743 - output_activity_accuracy: 0.9508 - output_day_accuracy: 0.5867 - output_user_accuracy: 0.5503 - output_country_accuracy: 0.5771 - output_company_accuracy: 0.6336 - val_loss: 5.4031 - val_output_activity_loss: 0.3286 - val_output_day_loss: 1.0333 - val_output_user_loss: 1.4583 - val_output_country_loss: 1.3707 - val_output_company_loss: 1.2123 - val_output_activity_accuracy: 0.9525 - val_output_day_accuracy: 0.6192 - val_output_user_accuracy: 0.5727 - val_output_country_accuracy: 0.5847 - val_output_company_accuracy: 0.6489\n",
      "Epoch 15/20\n",
      "68/68 [==============================] - 9s 136ms/step - loss: 5.6327 - output_activity_loss: 0.3572 - output_day_loss: 1.0951 - output_user_loss: 1.5247 - output_country_loss: 1.4053 - output_company_loss: 1.2504 - output_activity_accuracy: 0.9521 - output_day_accuracy: 0.5871 - output_user_accuracy: 0.5516 - output_country_accuracy: 0.5792 - output_company_accuracy: 0.6378 - val_loss: 5.3425 - val_output_activity_loss: 0.3269 - val_output_day_loss: 1.0231 - val_output_user_loss: 1.4395 - val_output_country_loss: 1.3544 - val_output_company_loss: 1.1985 - val_output_activity_accuracy: 0.9524 - val_output_day_accuracy: 0.6175 - val_output_user_accuracy: 0.5750 - val_output_country_accuracy: 0.5894 - val_output_company_accuracy: 0.6509\n",
      "Epoch 16/20\n",
      "68/68 [==============================] - 9s 140ms/step - loss: 5.5616 - output_activity_loss: 0.3479 - output_day_loss: 1.0853 - output_user_loss: 1.5069 - output_country_loss: 1.3837 - output_company_loss: 1.2378 - output_activity_accuracy: 0.9526 - output_day_accuracy: 0.5920 - output_user_accuracy: 0.5508 - output_country_accuracy: 0.5821 - output_company_accuracy: 0.6382 - val_loss: 5.2912 - val_output_activity_loss: 0.3237 - val_output_day_loss: 1.0199 - val_output_user_loss: 1.4263 - val_output_country_loss: 1.3399 - val_output_company_loss: 1.1814 - val_output_activity_accuracy: 0.9524 - val_output_day_accuracy: 0.6201 - val_output_user_accuracy: 0.5721 - val_output_country_accuracy: 0.5886 - val_output_company_accuracy: 0.6516\n",
      "Epoch 17/20\n",
      "68/68 [==============================] - 10s 146ms/step - loss: 5.4965 - output_activity_loss: 0.3400 - output_day_loss: 1.0792 - output_user_loss: 1.4912 - output_country_loss: 1.3660 - output_company_loss: 1.2201 - output_activity_accuracy: 0.9535 - output_day_accuracy: 0.5917 - output_user_accuracy: 0.5517 - output_country_accuracy: 0.5822 - output_company_accuracy: 0.6383 - val_loss: 5.2203 - val_output_activity_loss: 0.3146 - val_output_day_loss: 1.0140 - val_output_user_loss: 1.4033 - val_output_country_loss: 1.3205 - val_output_company_loss: 1.1680 - val_output_activity_accuracy: 0.9536 - val_output_day_accuracy: 0.6218 - val_output_user_accuracy: 0.5751 - val_output_country_accuracy: 0.5922 - val_output_company_accuracy: 0.6504\n",
      "Epoch 18/20\n",
      "68/68 [==============================] - 10s 140ms/step - loss: 5.4217 - output_activity_loss: 0.3315 - output_day_loss: 1.0691 - output_user_loss: 1.4669 - output_country_loss: 1.3480 - output_company_loss: 1.2062 - output_activity_accuracy: 0.9538 - output_day_accuracy: 0.5974 - output_user_accuracy: 0.5564 - output_country_accuracy: 0.5816 - output_company_accuracy: 0.6395 - val_loss: 5.2081 - val_output_activity_loss: 0.3147 - val_output_day_loss: 1.0178 - val_output_user_loss: 1.3977 - val_output_country_loss: 1.3133 - val_output_company_loss: 1.1646 - val_output_activity_accuracy: 0.9520 - val_output_day_accuracy: 0.6159 - val_output_user_accuracy: 0.5741 - val_output_country_accuracy: 0.5921 - val_output_company_accuracy: 0.6505\n",
      "Epoch 19/20\n",
      "68/68 [==============================] - 10s 143ms/step - loss: 5.3938 - output_activity_loss: 0.3282 - output_day_loss: 1.0716 - output_user_loss: 1.4617 - output_country_loss: 1.3358 - output_company_loss: 1.1965 - output_activity_accuracy: 0.9545 - output_day_accuracy: 0.5957 - output_user_accuracy: 0.5560 - output_country_accuracy: 0.5851 - output_company_accuracy: 0.6418 - val_loss: 5.1526 - val_output_activity_loss: 0.3109 - val_output_day_loss: 1.0097 - val_output_user_loss: 1.3805 - val_output_country_loss: 1.3005 - val_output_company_loss: 1.1511 - val_output_activity_accuracy: 0.9527 - val_output_day_accuracy: 0.6221 - val_output_user_accuracy: 0.5752 - val_output_country_accuracy: 0.5919 - val_output_company_accuracy: 0.6513\n",
      "Epoch 20/20\n",
      "68/68 [==============================] - 10s 154ms/step - loss: 5.3326 - output_activity_loss: 0.3188 - output_day_loss: 1.0675 - output_user_loss: 1.4449 - output_country_loss: 1.3202 - output_company_loss: 1.1812 - output_activity_accuracy: 0.9554 - output_day_accuracy: 0.5938 - output_user_accuracy: 0.5560 - output_country_accuracy: 0.5867 - output_company_accuracy: 0.6440 - val_loss: 5.1432 - val_output_activity_loss: 0.3110 - val_output_day_loss: 1.0077 - val_output_user_loss: 1.3786 - val_output_country_loss: 1.3008 - val_output_company_loss: 1.1452 - val_output_activity_accuracy: 0.9526 - val_output_day_accuracy: 0.6147 - val_output_user_accuracy: 0.5785 - val_output_country_accuracy: 0.5937 - val_output_company_accuracy: 0.6484\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_activity, train_day, train_user, train_country, train_company],\n",
    "    [train_targets_activity_cat, train_targets_day_cat, train_targets_user_cat, train_targets_country_cat, train_targets_company_cat],\n",
    "    validation_data=([test_activity, test_day, test_user, test_country, test_company], \n",
    "                     [test_targets_activity_cat, test_targets_day_cat, test_targets_user_cat, test_targets_country_cat, test_targets_company_cat]),\n",
    "    epochs=20,\n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 2s 7ms/step - loss: 5.1432 - output_activity_loss: 0.3110 - output_day_loss: 1.0077 - output_user_loss: 1.3786 - output_country_loss: 1.3008 - output_company_loss: 1.1452 - output_activity_accuracy: 0.9526 - output_day_accuracy: 0.6147 - output_user_accuracy: 0.5785 - output_country_accuracy: 0.5937 - output_company_accuracy: 0.6484\n",
      "Validation Loss: 5.14324426651001, Validation Accuracy: 0.3109837472438812\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "results = model.evaluate(\n",
    "    [test_activity, test_day, test_user, test_country, test_company],\n",
    "    [test_targets_activity_cat, test_targets_day_cat, test_targets_user_cat, test_targets_country_cat, test_targets_company_cat],\n",
    "    batch_size=64\n",
    ")\n",
    "print(f\"Validation Loss: {results[0]}, Validation Accuracy: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to an H5 file\n",
    "model.save('binetv3_Large-4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each event attribute, BINet's softmax layer outputs a probability distribution over possible values\n",
    "- The anomaly score for a specific attribute value v is calculated by summing all the probabilities from the softmax output that are greater than the probability assigned to v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513/1513 [==============================] - 7s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_activity, windows_day, windows_user, windows_country, windows_company])\n",
    "\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_activity = predictions[0]\n",
    "predictions_day = predictions[1]\n",
    "predictions_user = predictions[2]\n",
    "predictions_country = predictions[3]\n",
    "predictions_company = predictions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_scores(predictions, targets):\n",
    "    scores = []\n",
    "    # Loop through each example in the predictions\n",
    "    for i in range(predictions.shape[0]):\n",
    "        actual_prob = predictions[i, targets[i]]  # Extract the probability of the true class using target index\n",
    "        # Calculate anomaly score as sum of probabilities greater than the probability of the actual value\n",
    "        anomaly_score = np.sum(predictions[i][predictions[i] > actual_prob])\n",
    "        scores.append(anomaly_score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores for each attribute type\n",
    "anomaly_scores_activity = calculate_anomaly_scores(predictions_activity, targets_activity)\n",
    "anomaly_scores_day = calculate_anomaly_scores(predictions_day, targets_day)\n",
    "anomaly_scores_user = calculate_anomaly_scores(predictions_user, targets_user)\n",
    "anomaly_scores_country = calculate_anomaly_scores(predictions_country, targets_user)\n",
    "anomaly_scores_company = calculate_anomaly_scores(predictions_company, targets_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert missing scores for cases with less than 2 Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>score_day</th>\n",
       "      <th>score_activity</th>\n",
       "      <th>score_user</th>\n",
       "      <th>score_country</th>\n",
       "      <th>score_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.876759</td>\n",
       "      <td>0.987133</td>\n",
       "      <td>0.977750</td>\n",
       "      <td>0.985784</td>\n",
       "      <td>0.991848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.958875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669901</td>\n",
       "      <td>0.671877</td>\n",
       "      <td>0.373183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48380</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992675</td>\n",
       "      <td>0.914818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48381</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977162</td>\n",
       "      <td>0.926317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48382</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.800487</td>\n",
       "      <td>0.997735</td>\n",
       "      <td>0.989478</td>\n",
       "      <td>0.916523</td>\n",
       "      <td>0.933859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48383</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591363</td>\n",
       "      <td>0.556781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48384</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.461402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823916</td>\n",
       "      <td>0.995423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48385 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  score_day  score_activity  score_user  score_country  \\\n",
       "0         1   0.000000        0.000000    0.000000       0.000000   \n",
       "1         1   0.000000        0.000000    0.000000       0.000000   \n",
       "2         1   0.876759        0.987133    0.977750       0.985784   \n",
       "3         1   0.958875        0.000000    0.669901       0.671877   \n",
       "4         1   0.000000        0.000000    0.000000       0.000000   \n",
       "...     ...        ...             ...         ...            ...   \n",
       "48380  5000   0.000000        0.000000    0.000000       0.992675   \n",
       "48381  5000   0.000000        0.000000    0.000000       0.977162   \n",
       "48382  5000   0.800487        0.997735    0.989478       0.916523   \n",
       "48383  5000   0.000000        0.377368    0.000000       0.591363   \n",
       "48384  5000   0.461402        0.000000    0.000000       0.823916   \n",
       "\n",
       "       score_company  \n",
       "0           0.542855  \n",
       "1           0.000000  \n",
       "2           0.991848  \n",
       "3           0.373183  \n",
       "4           0.000000  \n",
       "...              ...  \n",
       "48380       0.914818  \n",
       "48381       0.926317  \n",
       "48382       0.933859  \n",
       "48383       0.556781  \n",
       "48384       0.995423  \n",
       "\n",
       "[48385 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "score = pd.DataFrame({'case': case_indices})\n",
    "score['score_day'] = anomaly_scores_day\n",
    "score['score_activity'] = anomaly_scores_activity\n",
    "score['score_user'] = anomaly_scores_user\n",
    "score['score_country'] = anomaly_scores_country\n",
    "score['score_company'] = anomaly_scores_company\n",
    "\n",
    "\n",
    "score['case'] = score['case'].astype(int)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the 'case' column contain all values between 0 and 5000? True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def contains_all_values(df, column, end):\n",
    "\n",
    "    # Generate the set of all values in the specified range\n",
    "    required_values = set(range(1, end + 1))\n",
    "    \n",
    "    # Get the unique values in the specified column\n",
    "    column_values = set(df[column].unique())\n",
    "    \n",
    "    # Find missing values\n",
    "    missing_values = required_values - column_values\n",
    "    \n",
    "    # Print missing values if any\n",
    "    if missing_values:\n",
    "        print(f\"Missing values: {sorted(missing_values)}\")\n",
    "    \n",
    "    # Check if all required values are in the column values\n",
    "    return required_values.issubset(column_values)\n",
    "\n",
    "end = 5000\n",
    "\n",
    "result = contains_all_values(score, 'case', end)\n",
    "print(f\"Does the 'case' column contain all values between 0 and {end}? {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold (lowest plateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_anomaly_ratio(scores, threshold):\n",
    "    return np.mean(scores > threshold)\n",
    "\n",
    "def find_plateaus(scores, epsilon=1e-4, min_plateau_length=10):\n",
    "    scores = np.array(scores)  # Convert scores to a NumPy array\n",
    "    sorted_scores = np.sort(scores)\n",
    "    thresholds = sorted_scores\n",
    "    \n",
    "    # Calculate anomaly ratios for all thresholds at once\n",
    "    scores_expanded = scores[:, np.newaxis]\n",
    "    thresholds_expanded = thresholds[np.newaxis, :]\n",
    "    anomaly_ratios = np.mean(scores_expanded > thresholds_expanded, axis=0)\n",
    "    \n",
    "    # Calculate first and second derivatives\n",
    "    first_derivatives = np.diff(anomaly_ratios) / np.diff(thresholds)\n",
    "    second_derivatives = np.diff(first_derivatives) / np.diff(thresholds[:-1])\n",
    "    \n",
    "    # Identify plateaus where the first derivative is close to zero\n",
    "    plateau_indices = np.where(np.abs(first_derivatives) < epsilon)[0]\n",
    "    \n",
    "    # Group consecutive indices to identify continuous plateaus\n",
    "    grouped_plateaus = np.split(plateau_indices, np.where(np.diff(plateau_indices) != 1)[0] + 1)\n",
    "    \n",
    "    # Filter plateaus based on minimum length\n",
    "    long_plateaus = [g for g in grouped_plateaus if len(g) >= min_plateau_length]\n",
    "    \n",
    "    if long_plateaus:\n",
    "        # Take the first long plateau and find the mean threshold in this plateau\n",
    "        first_plateau = long_plateaus[0]\n",
    "        plateau_thresholds = thresholds[first_plateau]\n",
    "        return np.mean(plateau_thresholds)\n",
    "    else:\n",
    "        # If no plateau is found, return a default value, e.g., the 90th percentile\n",
    "        return np.percentile(sorted_scores, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_1909/1116875279.py:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  first_derivatives = np.diff(anomaly_ratios) / np.diff(thresholds)\n"
     ]
    }
   ],
   "source": [
    "threshold_activity = find_plateaus(anomaly_scores_activity)\n",
    "threshold_day = find_plateaus(anomaly_scores_day)\n",
    "threshold_user = find_plateaus(anomaly_scores_user)\n",
    "threshold_country = find_plateaus(anomaly_scores_country)\n",
    "threshold_company = find_plateaus(anomaly_scores_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(anomaly_scores, threshold):\n",
    "    labels = [1 if score > threshold else 0 for score in anomaly_scores]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies based on the calculated anomaly scores and thresholds\n",
    "labels_activity = detect_anomalies(anomaly_scores_activity, threshold_activity)\n",
    "labels_day = detect_anomalies(anomaly_scores_day, threshold_day)\n",
    "labels_user = detect_anomalies(anomaly_scores_user, threshold_user)\n",
    "labels_country = detect_anomalies(anomaly_scores_country, threshold_country)\n",
    "labels_company = detect_anomalies(anomaly_scores_company, threshold_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted_activity</th>\n",
       "      <th>predicted_day</th>\n",
       "      <th>predicted_user</th>\n",
       "      <th>predicted_country</th>\n",
       "      <th>predicted_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48380</th>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48381</th>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48382</th>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48383</th>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48384</th>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48385 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted_activity  predicted_day  predicted_user  \\\n",
       "0         1                   0              0               0   \n",
       "1         1                   0              0               0   \n",
       "2         1                   1              1               1   \n",
       "3         1                   0              1               0   \n",
       "4         1                   0              0               0   \n",
       "...     ...                 ...            ...             ...   \n",
       "48380  5000                   0              0               0   \n",
       "48381  5000                   0              0               0   \n",
       "48382  5000                   1              1               1   \n",
       "48383  5000                   1              0               0   \n",
       "48384  5000                   0              0               0   \n",
       "\n",
       "       predicted_country  predicted_company  \n",
       "0                      0                  0  \n",
       "1                      0                  0  \n",
       "2                      0                  0  \n",
       "3                      0                  0  \n",
       "4                      0                  0  \n",
       "...                  ...                ...  \n",
       "48380                  0                  0  \n",
       "48381                  0                  0  \n",
       "48382                  0                  0  \n",
       "48383                  0                  0  \n",
       "48384                  0                  0  \n",
       "\n",
       "[48385 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted_activity'] = labels_activity\n",
    "mapping['predicted_day'] = labels_day\n",
    "mapping['predicted_user'] = labels_user\n",
    "mapping['predicted_country'] = labels_country\n",
    "mapping['predicted_company'] = labels_company\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "5        True\n",
       "        ...  \n",
       "4996    False\n",
       "4997     True\n",
       "4998     True\n",
       "4999     True\n",
       "5000     True\n",
       "Length: 5000, dtype: bool"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean DataFrame where each value is True if the value is 1\n",
    "contains_one = (mapping[['predicted_activity', 'predicted_day', 'predicted_user', 'predicted_country', 'predicted_company']] == 1)\n",
    "\n",
    "# Group by 'case' and check if there's at least one 'True' in any of the columns\n",
    "case_prediction = contains_one.groupby(mapping['case']).any().any(axis=1)\n",
    "case_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "5        True\n",
       "        ...  \n",
       "4996    False\n",
       "4997     True\n",
       "4998     True\n",
       "4999    False\n",
       "5000     True\n",
       "Length: 5000, dtype: bool"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean DataFrame where each value is True if the value is 1\n",
    "contains_one = (mapping[['predicted_activity', 'predicted_user']] == 1)\n",
    "\n",
    "# Group by 'case' and check if there's at least one 'True' in any of the columns\n",
    "case_prediction = contains_one.groupby(mapping['case']).any().any(axis=1)\n",
    "case_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SkipSequence' 'Insert' 'normal' 'Late' 'Attribute' 'Early' 'Rework']\n"
     ]
    }
   ],
   "source": [
    "unique_values = dataframe_log['anomaly'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1: conforming\n",
    "- 2: non-conforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of strings to check for anomalies\n",
    "anomaly_strings = ['SkipSequence', 'Insert', 'Early', 'Late', 'Rework']\n",
    "\n",
    "# Group by 'trace_id' and check if 'anomaly' contains any anomaly strings\n",
    "def is_anomalous(group):\n",
    "    return any(label in anomaly_strings for label in group['anomaly'])\n",
    "\n",
    "# Apply the function to each group and create a new dataframe\n",
    "anomaly_df = dataframe_log.groupby('trace_id').apply(is_anomalous).reset_index()\n",
    "anomaly_df.columns = ['trace_id', 'is_anomaly']\n",
    "\n",
    "# Convert boolean to integer (1 for conforming, 0 for anomaly)\n",
    "anomaly_df['is_anomaly'] = (~anomaly_df['is_anomaly']).astype(int)\n",
    "\n",
    "# Extract the conformity array\n",
    "conformity_array = anomaly_df['is_anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformity_array = conformity_array.reset_index(drop=True)\n",
    "case_prediction = case_prediction.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "    'conformity': conformity_array,\n",
    "    'predicted': case_prediction\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "ground_truth = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              0          0\n",
       "1              0          0\n",
       "2              1          0\n",
       "3              0          0\n",
       "4              0          0\n",
       "...          ...        ...\n",
       "4995           1          1\n",
       "4996           1          0\n",
       "4997           1          0\n",
       "4998           1          1\n",
       "4999           0          0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth['predicted'] = 1 - ground_truth['predicted']\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.646\n"
     ]
    }
   ],
   "source": [
    "# Calculate f1\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.391\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = TN / (TN + FN)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.966\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = TN / (TN + FP)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.976\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.483\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7241880742932729"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming ground_truth is your DataFrame\n",
    "# Make sure 'conformity' contains actual labels (0 or 1)\n",
    "# and 'predicted' contains predicted probabilities or scores\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace2Trace Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT TRACE 1\n",
    "\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/input_traces/large_trace1.bpmn\")\n",
    "\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "alignments = alignments_petri.apply(log, net, im, fm)\n",
    "\n",
    "fitness_trace_1 = [trace['fitness'] for trace in alignments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT TRACE 2\n",
    "\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/input_traces/large_trace2.bpmn\")\n",
    "\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "alignments = alignments_petri.apply(log, net, im, fm)\n",
    "\n",
    "fitness_trace_2 = [trace['fitness'] for trace in alignments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT TRACE 3\n",
    "\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/input_traces/large_trace3.bpmn\")\n",
    "\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "alignments = alignments_petri.apply(log, net, im, fm)\n",
    "\n",
    "fitness_trace_3 = [trace['fitness'] for trace in alignments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the lists\n",
    "data = {\n",
    "    'Trace 1': fitness_trace_1,\n",
    "    'Trace 2': fitness_trace_2,\n",
    "    'Trace 3': fitness_trace_3\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "fitness = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the trace with the highest value\n",
    "def highest_trace(row):\n",
    "    if row['Trace 1'] == max(row):\n",
    "        return 'trace_1'\n",
    "    elif row['Trace 2'] == max(row):\n",
    "        return 'trace_2'\n",
    "    else:\n",
    "        return 'trace_3'\n",
    "\n",
    "# Add a new column using the highest_trace function\n",
    "fitness['Closest Trace'] = fitness.apply(highest_trace, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify deviation:\n",
    "# – Skip: One or multiple events are skipped\n",
    "# – Insert: Random events are inserted\n",
    "# – Rework: Events are executed multiple times\n",
    "# – Late: Events are shifted forward\n",
    "# – Early: Events are shifted backward\n",
    "\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 1. Import the event log\n",
    "log = xes_importer.apply(\"../../data/logs/event_log.xes\")\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/large.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "# 4. Perform alignment-based conformance checking\n",
    "alignments = alignments_petri.apply(log, net, im, fm)\n",
    "\n",
    "# 5. Calculate and print diagnostics\n",
    "fit_traces = sum(1 for trace in alignments if trace['fitness'] == 1.0)\n",
    "\n",
    "print(f\"Total traces: {len(log)}\")\n",
    "print(f\"Conform traces: {fit_traces}\")\n",
    "print(f\"Non-Conform traces: {len(log) - fit_traces}\")\n",
    "\n",
    "# 6. Document deviations for each trace\n",
    "deviations = []\n",
    "\n",
    "for idx, trace in enumerate(alignments):\n",
    "    trace_deviations = {\n",
    "        \"trace_index\": idx,\n",
    "        \"skip\": [],\n",
    "        \"insert\": [],\n",
    "        \"rework\": [],\n",
    "        \"late\": [],\n",
    "        \"early\": []\n",
    "    }\n",
    "    visited_activities = set()\n",
    "    alignment_steps = trace['alignment']\n",
    "    for i, step in enumerate(alignment_steps):\n",
    "        if step[0] == \">>\" and step[1] != \">>\":\n",
    "            trace_deviations[\"skip\"].append(step[1])\n",
    "        elif step[1] == \">>\" and step[0] != \">>\":\n",
    "            trace_deviations[\"insert\"].append(step[0])\n",
    "        elif step[0] == step[1]:\n",
    "            if step[0] in visited_activities:\n",
    "                trace_deviations[\"rework\"].append(step[0])\n",
    "            visited_activities.add(step[0])\n",
    "        if step[0] != \">>\" and step[1] != \">>\" and step[0] != step[1]:\n",
    "            if alignment_steps[i-1][0] == step[1] or alignment_steps[i-1][1] == step[0]:\n",
    "                trace_deviations[\"early\"].append(step[0])\n",
    "            else:\n",
    "                trace_deviations[\"late\"].append(step[0])\n",
    "    deviations.append(trace_deviations)\n",
    "\n",
    "# Print or save the deviations\n",
    "for dev in deviations:\n",
    "    print(f\"Trace {dev['trace_index']}:\")\n",
    "    print(f\"  Skipped Activities: {dev['skip']}\")\n",
    "    print(f\"  Inserted Activities: {dev['insert']}\")\n",
    "    print(f\"  Rework Activities: {dev['rework']}\")\n",
    "    print(f\"  Late Activities: {dev['late']}\")\n",
    "    print(f\"  Early Activities: {dev['early']}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
