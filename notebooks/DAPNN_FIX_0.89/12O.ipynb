{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ch_w_j2d0sqf6dbdc0_224m40000gq/T/ipykernel_11980/670060893.py:14: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe_log = pm4py.format_dataframe(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe_log = pd.read_csv('../../data/logs/BPIC12_Log_onlyO.csv', sep=',')\n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe_log = dataframe_log.drop(dataframe_log.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe_log = pm4py.format_dataframe(\n",
    "        dataframe_log,\n",
    "        case_id='case:concept:name',\n",
    "        activity_key='concept:name',\n",
    "        timestamp_key='time:timestamp'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['lifecycle:transition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:concept:name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['@@index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:AMOUNT_REQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_log = dataframe_log.drop(columns=['case:REG_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to datetime format\n",
    "dataframe_log['time:timestamp'] = pd.to_datetime(dataframe_log['time:timestamp'])\n",
    "\n",
    "# Calculate elapsed time since the start of each case\n",
    "dataframe_log['start_time'] = dataframe_log.groupby('@@case_index')['time:timestamp'].transform('min')\n",
    "dataframe_log['elapsed_time'] = (dataframe_log['time:timestamp'] - dataframe_log['start_time']).dt.total_seconds()\n",
    "\n",
    "# Normalize the elapsed time in minutes\n",
    "scaler = StandardScaler()\n",
    "dataframe_log['standardized_elapsed_time'] = scaler.fit_transform(dataframe_log[['elapsed_time']])\n",
    "\n",
    "dataframe_log = dataframe_log.drop(columns=['start_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['elapsed_time'])\n",
    "dataframe_log = dataframe_log.drop(columns=['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_SELECTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_CREATED</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10862</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11049</td>\n",
       "      <td>O_SENT_BACK</td>\n",
       "      <td>0</td>\n",
       "      <td>0.299272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10629</td>\n",
       "      <td>O_ACCEPTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31239</th>\n",
       "      <td>11003</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>5013</td>\n",
       "      <td>-0.620265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31240</th>\n",
       "      <td>10789</td>\n",
       "      <td>O_SENT_BACK</td>\n",
       "      <td>5013</td>\n",
       "      <td>0.099241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31241</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_SELECTED</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31242</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_CREATED</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31243</th>\n",
       "      <td>10933</td>\n",
       "      <td>O_SENT</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31244 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource concept:name  @@case_index  standardized_elapsed_time\n",
       "0             10862   O_SELECTED             0                  -0.620650\n",
       "1             10862    O_CREATED             0                  -0.620647\n",
       "2             10862       O_SENT             0                  -0.620647\n",
       "3             11049  O_SENT_BACK             0                   0.299272\n",
       "4             10629   O_ACCEPTED             0                   0.602251\n",
       "...             ...          ...           ...                        ...\n",
       "31239         11003       O_SENT          5013                  -0.620265\n",
       "31240         10789  O_SENT_BACK          5013                   0.099241\n",
       "31241         10933   O_SELECTED          5014                  -0.620650\n",
       "31242         10933    O_CREATED          5014                  -0.620648\n",
       "31243         10933       O_SENT          5014                  -0.620648\n",
       "\n",
       "[31244 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Start & End markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert start and end markers\n",
    "def add_markers(df):\n",
    "    # Identify unique case indices\n",
    "    case_indices = df['@@case_index'].unique()\n",
    "    \n",
    "    # Prepare a container for new DataFrame rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each case index to add start and end markers\n",
    "    for case_index in case_indices:\n",
    "        # Create a start marker row with all columns except @@case_index set to 'Start'\n",
    "        start_row = {col: 'Start' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Create an end marker row with all columns except @@case_index set to 'End'\n",
    "        end_row = {col: 'End' if col != '@@case_index' else case_index for col in df.columns}\n",
    "        \n",
    "        # Append start row, rows for the current case, and end row\n",
    "        new_rows.append(start_row)\n",
    "        new_rows.extend(df[df['@@case_index'] == case_index].to_dict('records'))\n",
    "        new_rows.append(end_row)\n",
    "    \n",
    "    # Convert the list of rows into a DataFrame\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Apply the function to add start and end markers to the dataframe\n",
    "modified_dataframe = add_markers(dataframe_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dataframe['standardized_elapsed_time'] = modified_dataframe['standardized_elapsed_time'].replace({'Start': 0, 'End': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['org:resource'])\n",
    "modified_dataframe['org:resource'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(modified_dataframe['concept:name'])\n",
    "modified_dataframe['concept:name'] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>standardized_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.620647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.299272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41269</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41270</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41271</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41272</th>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>5014</td>\n",
       "      <td>-0.620648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41273</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5014</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41274 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       org:resource  concept:name  @@case_index  standardized_elapsed_time\n",
       "0                 0             0             0                   0.000000\n",
       "1                 1             1             0                  -0.620650\n",
       "2                 1             2             0                  -0.620647\n",
       "3                 1             3             0                  -0.620647\n",
       "4                 2             4             0                   0.299272\n",
       "...             ...           ...           ...                        ...\n",
       "41269             0             0          5014                   0.000000\n",
       "41270            47             1          5014                  -0.620650\n",
       "41271            47             2          5014                  -0.620648\n",
       "41272            47             3          5014                  -0.620648\n",
       "41273             4             6          5014                   1.000000\n",
       "\n",
       "[41274 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for Cases with less then 5 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each unique value in '@@case_index'\n",
    "frequency = modified_dataframe['@@case_index'].value_counts()\n",
    "\n",
    "# Finding the minimum occurrence\n",
    "min_occurrence = frequency.min()\n",
    "\n",
    "min_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resource = modified_dataframe[['org:resource', '@@case_index']]\n",
    "df_activity = modified_dataframe[['concept:name', '@@case_index']]\n",
    "df_timestamp = modified_dataframe[['standardized_elapsed_time', '@@case_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(df, case_id_column='@@case_index', window_size=5):\n",
    "    windows = []\n",
    "    targets = []\n",
    "    case_indices = []\n",
    "\n",
    "    # Iterate over each unique case\n",
    "    for case_id in df[case_id_column].unique():\n",
    "        # Extract the case\n",
    "        case_data = df[df[case_id_column] == case_id]\n",
    "        \n",
    "        # Convert case_data to a NumPy array and drop the case_id_column\n",
    "        case_data_array = case_data.drop(columns=[case_id_column]).to_numpy()\n",
    "\n",
    "        # Adjusting the condition to correctly reflect window_size without needing an additional +1\n",
    "        # Now it correctly considers window_size as including the target event\n",
    "        if len(case_data_array) >= window_size:\n",
    "            # Adjust the loop to generate sliding windows of size window_size - 1 for the inputs and use the next event as the target\n",
    "            for i in range(len(case_data_array) - window_size + 1):\n",
    "                # window now has window_size - 1 events\n",
    "                window = case_data_array[i:i + window_size - 1]\n",
    "                # The target is the event immediately following the window\n",
    "                target = case_data_array[i + window_size - 1]\n",
    "                windows.append(window)\n",
    "                targets.append(target)\n",
    "                case_indices.append(case_id)  # Store the case_id corresponding to the window\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling and to ensure they are two-dimensional\n",
    "    windows_array = np.array(windows)\n",
    "    targets_array = np.array(targets)\n",
    "    case_indices_array = np.array(case_indices)\n",
    "    \n",
    "    return windows_array, targets_array, case_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_resource, targets_resource, case_indices = generate_sliding_windows(df_resource)\n",
    "windows_activity, targets_activity, case_indices = generate_sliding_windows(df_activity)\n",
    "windows_timestamp, targets_timestamp, case_indices = generate_sliding_windows(df_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 21:57:53.791977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_resource (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_activity (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 4, 50)                3100      ['input_resource[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 4, 50)                450       ['input_activity[0][0]']      \n",
      "                                                                                                  \n",
      " input_timestamp (InputLaye  [(None, 4, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 25)                   7600      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 25)                   7600      ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 25)                   2700      ['input_timestamp[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 75)                   0         ['lstm[0][0]',                \n",
      "                                                                     'lstm_1[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_resource (Dense)     (None, 62)                   4712      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_activity (Dense)     (None, 9)                    684       ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_timestamp (Dense)    (None, 1)                    76        ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26922 (105.16 KB)\n",
      "Trainable params: 26922 (105.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding\n",
    "\n",
    "# Assuming these values as placeholders, replace them with actual counts from your data\n",
    "num_resources = modified_dataframe['org:resource'].nunique()\n",
    "num_activities = modified_dataframe['concept:name'].nunique()\n",
    "\n",
    "embedding_dim_resource = 50\n",
    "embedding_dim_activity = 50\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "# Input layers\n",
    "input_resource = Input(shape=(time_steps,), name='input_resource')\n",
    "input_activity = Input(shape=(time_steps,), name='input_activity')\n",
    "input_timestamp = Input(shape=(time_steps, 1), name='input_timestamp')\n",
    "\n",
    "# Embedding layers\n",
    "embedding_resource = Embedding(input_dim=num_resources, output_dim=embedding_dim_resource, input_length=time_steps)(input_resource)\n",
    "embedding_activity = Embedding(input_dim=num_activities, output_dim=embedding_dim_activity, input_length=time_steps)(input_activity)\n",
    "\n",
    "# LSTM layers\n",
    "lstm_resource = LSTM(25, return_sequences=False)(embedding_resource)\n",
    "lstm_activity = LSTM(25, return_sequences=False)(embedding_activity)\n",
    "lstm_timestamp = LSTM(25, return_sequences=False)(input_timestamp)\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate(axis=-1)([lstm_resource, lstm_activity, lstm_timestamp])\n",
    "\n",
    "# Output layers\n",
    "output_resource = Dense(num_resources, activation='softmax', name='output_resource')(concatenated)\n",
    "output_activity = Dense(num_activities, activation='softmax', name='output_activity')(concatenated)\n",
    "output_timestamp = Dense(1, activation='linear', name='output_timestamp')(concatenated)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=[input_resource, input_activity, input_timestamp], \n",
    "              outputs=[output_resource, output_activity, output_timestamp])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'output_resource': 'categorical_crossentropy', \n",
    "                    'output_activity': 'categorical_crossentropy', \n",
    "                    'output_timestamp': 'mean_squared_error'},\n",
    "              metrics={'output_resource': 'accuracy', \n",
    "                       'output_activity': 'accuracy', \n",
    "                       'output_timestamp': 'mean_absolute_error'})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resource data\n",
    "train_resource, test_resource, train_targets_resource, test_targets_resource = train_test_split(\n",
    "    windows_resource, targets_resource, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the activity data\n",
    "train_activity, test_activity, train_targets_activity, test_targets_activity = train_test_split(\n",
    "    windows_activity, targets_activity, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the timestamp data\n",
    "train_timestamp, test_timestamp, train_targets_timestamp, test_targets_timestamp = train_test_split(\n",
    "    windows_timestamp, targets_timestamp, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def cyclic_lr(epoch, lr):\n",
    "    # Example function that modulates LR within a range for each epoch\n",
    "    # Customize this function based on your cyclic learning rate policy\n",
    "    max_lr = 0.01  # Maximum LR\n",
    "    base_lr = 0.001  # Base LR\n",
    "    step_size = 10  # Number of epochs for half a cycle\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "233/233 [==============================] - 9s 11ms/step - loss: 5.6934 - output_resource_loss: 3.3251 - output_activity_loss: 1.6928 - output_timestamp_loss: 0.6754 - output_resource_accuracy: 0.2360 - output_activity_accuracy: 0.4091 - output_timestamp_mean_absolute_error: 0.5995 - val_loss: 4.3719 - val_output_resource_loss: 2.7702 - val_output_activity_loss: 1.0507 - val_output_timestamp_loss: 0.5510 - val_output_resource_accuracy: 0.2718 - val_output_activity_accuracy: 0.7018 - val_output_timestamp_mean_absolute_error: 0.5244 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 3.5363 - output_resource_loss: 2.3206 - output_activity_loss: 0.7361 - output_timestamp_loss: 0.4797 - output_resource_accuracy: 0.4111 - output_activity_accuracy: 0.7241 - output_timestamp_mean_absolute_error: 0.4855 - val_loss: 3.1230 - val_output_resource_loss: 2.0071 - val_output_activity_loss: 0.6530 - val_output_timestamp_loss: 0.4630 - val_output_resource_accuracy: 0.4809 - val_output_activity_accuracy: 0.7317 - val_output_timestamp_mean_absolute_error: 0.4684 - lr: 0.0019\n",
      "Epoch 3/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.9079 - output_resource_loss: 1.8287 - output_activity_loss: 0.6360 - output_timestamp_loss: 0.4432 - output_resource_accuracy: 0.5284 - output_activity_accuracy: 0.7343 - output_timestamp_mean_absolute_error: 0.4523 - val_loss: 2.8340 - val_output_resource_loss: 1.7356 - val_output_activity_loss: 0.6263 - val_output_timestamp_loss: 0.4721 - val_output_resource_accuracy: 0.5474 - val_output_activity_accuracy: 0.7444 - val_output_timestamp_mean_absolute_error: 0.4666 - lr: 0.0028\n",
      "Epoch 4/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.6928 - output_resource_loss: 1.6350 - output_activity_loss: 0.6181 - output_timestamp_loss: 0.4397 - output_resource_accuracy: 0.5697 - output_activity_accuracy: 0.7361 - output_timestamp_mean_absolute_error: 0.4445 - val_loss: 2.6804 - val_output_resource_loss: 1.6241 - val_output_activity_loss: 0.6171 - val_output_timestamp_loss: 0.4391 - val_output_resource_accuracy: 0.5650 - val_output_activity_accuracy: 0.7343 - val_output_timestamp_mean_absolute_error: 0.4353 - lr: 0.0037\n",
      "Epoch 5/25\n",
      "233/233 [==============================] - 2s 8ms/step - loss: 2.6049 - output_resource_loss: 1.5566 - output_activity_loss: 0.6116 - output_timestamp_loss: 0.4368 - output_resource_accuracy: 0.5817 - output_activity_accuracy: 0.7377 - output_timestamp_mean_absolute_error: 0.4399 - val_loss: 2.6272 - val_output_resource_loss: 1.5713 - val_output_activity_loss: 0.6179 - val_output_timestamp_loss: 0.4380 - val_output_resource_accuracy: 0.5709 - val_output_activity_accuracy: 0.7340 - val_output_timestamp_mean_absolute_error: 0.4490 - lr: 0.0046\n",
      "Epoch 6/25\n",
      "233/233 [==============================] - 2s 7ms/step - loss: 2.5617 - output_resource_loss: 1.5226 - output_activity_loss: 0.6031 - output_timestamp_loss: 0.4360 - output_resource_accuracy: 0.5851 - output_activity_accuracy: 0.7407 - output_timestamp_mean_absolute_error: 0.4401 - val_loss: 2.5910 - val_output_resource_loss: 1.5472 - val_output_activity_loss: 0.6096 - val_output_timestamp_loss: 0.4342 - val_output_resource_accuracy: 0.5789 - val_output_activity_accuracy: 0.7412 - val_output_timestamp_mean_absolute_error: 0.4482 - lr: 0.0055\n",
      "Epoch 7/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.5225 - output_resource_loss: 1.4890 - output_activity_loss: 0.5998 - output_timestamp_loss: 0.4337 - output_resource_accuracy: 0.5903 - output_activity_accuracy: 0.7392 - output_timestamp_mean_absolute_error: 0.4386 - val_loss: 2.5941 - val_output_resource_loss: 1.5407 - val_output_activity_loss: 0.6098 - val_output_timestamp_loss: 0.4437 - val_output_resource_accuracy: 0.5741 - val_output_activity_accuracy: 0.7295 - val_output_timestamp_mean_absolute_error: 0.4608 - lr: 0.0064\n",
      "Epoch 8/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.5074 - output_resource_loss: 1.4771 - output_activity_loss: 0.6005 - output_timestamp_loss: 0.4298 - output_resource_accuracy: 0.5860 - output_activity_accuracy: 0.7407 - output_timestamp_mean_absolute_error: 0.4363 - val_loss: 2.5687 - val_output_resource_loss: 1.5189 - val_output_activity_loss: 0.6094 - val_output_timestamp_loss: 0.4404 - val_output_resource_accuracy: 0.5681 - val_output_activity_accuracy: 0.7400 - val_output_timestamp_mean_absolute_error: 0.4149 - lr: 0.0073\n",
      "Epoch 9/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.4772 - output_resource_loss: 1.4578 - output_activity_loss: 0.5961 - output_timestamp_loss: 0.4233 - output_resource_accuracy: 0.5879 - output_activity_accuracy: 0.7432 - output_timestamp_mean_absolute_error: 0.4281 - val_loss: 2.5690 - val_output_resource_loss: 1.5136 - val_output_activity_loss: 0.6204 - val_output_timestamp_loss: 0.4350 - val_output_resource_accuracy: 0.5794 - val_output_activity_accuracy: 0.7444 - val_output_timestamp_mean_absolute_error: 0.4421 - lr: 0.0082\n",
      "Epoch 10/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.4861 - output_resource_loss: 1.4588 - output_activity_loss: 0.6000 - output_timestamp_loss: 0.4272 - output_resource_accuracy: 0.5912 - output_activity_accuracy: 0.7397 - output_timestamp_mean_absolute_error: 0.4309 - val_loss: 2.5816 - val_output_resource_loss: 1.5456 - val_output_activity_loss: 0.6137 - val_output_timestamp_loss: 0.4223 - val_output_resource_accuracy: 0.5639 - val_output_activity_accuracy: 0.7483 - val_output_timestamp_mean_absolute_error: 0.4218 - lr: 0.0091\n",
      "Epoch 11/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.4800 - output_resource_loss: 1.4572 - output_activity_loss: 0.5989 - output_timestamp_loss: 0.4238 - output_resource_accuracy: 0.5872 - output_activity_accuracy: 0.7407 - output_timestamp_mean_absolute_error: 0.4284 - val_loss: 2.5792 - val_output_resource_loss: 1.5451 - val_output_activity_loss: 0.6087 - val_output_timestamp_loss: 0.4254 - val_output_resource_accuracy: 0.5730 - val_output_activity_accuracy: 0.7444 - val_output_timestamp_mean_absolute_error: 0.4330 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.4381 - output_resource_loss: 1.4362 - output_activity_loss: 0.5869 - output_timestamp_loss: 0.4150 - output_resource_accuracy: 0.5926 - output_activity_accuracy: 0.7473 - output_timestamp_mean_absolute_error: 0.4189 - val_loss: 2.5650 - val_output_resource_loss: 1.5252 - val_output_activity_loss: 0.6076 - val_output_timestamp_loss: 0.4321 - val_output_resource_accuracy: 0.5760 - val_output_activity_accuracy: 0.7339 - val_output_timestamp_mean_absolute_error: 0.4402 - lr: 0.0091\n",
      "Epoch 13/25\n",
      "233/233 [==============================] - 2s 6ms/step - loss: 2.4110 - output_resource_loss: 1.4136 - output_activity_loss: 0.5817 - output_timestamp_loss: 0.4156 - output_resource_accuracy: 0.5959 - output_activity_accuracy: 0.7462 - output_timestamp_mean_absolute_error: 0.4225 - val_loss: 2.5529 - val_output_resource_loss: 1.5233 - val_output_activity_loss: 0.6083 - val_output_timestamp_loss: 0.4214 - val_output_resource_accuracy: 0.5731 - val_output_activity_accuracy: 0.7467 - val_output_timestamp_mean_absolute_error: 0.4378 - lr: 0.0082\n",
      "Epoch 14/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.3947 - output_resource_loss: 1.3999 - output_activity_loss: 0.5851 - output_timestamp_loss: 0.4097 - output_resource_accuracy: 0.5970 - output_activity_accuracy: 0.7483 - output_timestamp_mean_absolute_error: 0.4146 - val_loss: 2.5726 - val_output_resource_loss: 1.5239 - val_output_activity_loss: 0.6169 - val_output_timestamp_loss: 0.4318 - val_output_resource_accuracy: 0.5755 - val_output_activity_accuracy: 0.7153 - val_output_timestamp_mean_absolute_error: 0.4386 - lr: 0.0073\n",
      "Epoch 15/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.3794 - output_resource_loss: 1.3876 - output_activity_loss: 0.5772 - output_timestamp_loss: 0.4146 - output_resource_accuracy: 0.5973 - output_activity_accuracy: 0.7481 - output_timestamp_mean_absolute_error: 0.4166 - val_loss: 2.5351 - val_output_resource_loss: 1.5188 - val_output_activity_loss: 0.6038 - val_output_timestamp_loss: 0.4125 - val_output_resource_accuracy: 0.5695 - val_output_activity_accuracy: 0.7362 - val_output_timestamp_mean_absolute_error: 0.4105 - lr: 0.0064\n",
      "Epoch 16/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.3463 - output_resource_loss: 1.3715 - output_activity_loss: 0.5716 - output_timestamp_loss: 0.4033 - output_resource_accuracy: 0.6030 - output_activity_accuracy: 0.7510 - output_timestamp_mean_absolute_error: 0.4073 - val_loss: 2.5373 - val_output_resource_loss: 1.5167 - val_output_activity_loss: 0.6044 - val_output_timestamp_loss: 0.4163 - val_output_resource_accuracy: 0.5816 - val_output_activity_accuracy: 0.7384 - val_output_timestamp_mean_absolute_error: 0.4277 - lr: 0.0055\n",
      "Epoch 17/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.3529 - output_resource_loss: 1.3660 - output_activity_loss: 0.5706 - output_timestamp_loss: 0.4162 - output_resource_accuracy: 0.5997 - output_activity_accuracy: 0.7499 - output_timestamp_mean_absolute_error: 0.4189 - val_loss: 2.5326 - val_output_resource_loss: 1.5137 - val_output_activity_loss: 0.6061 - val_output_timestamp_loss: 0.4128 - val_output_resource_accuracy: 0.5764 - val_output_activity_accuracy: 0.7477 - val_output_timestamp_mean_absolute_error: 0.4255 - lr: 0.0046\n",
      "Epoch 18/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.3108 - output_resource_loss: 1.3473 - output_activity_loss: 0.5650 - output_timestamp_loss: 0.3985 - output_resource_accuracy: 0.6078 - output_activity_accuracy: 0.7529 - output_timestamp_mean_absolute_error: 0.4029 - val_loss: 2.5448 - val_output_resource_loss: 1.5163 - val_output_activity_loss: 0.6056 - val_output_timestamp_loss: 0.4230 - val_output_resource_accuracy: 0.5783 - val_output_activity_accuracy: 0.7461 - val_output_timestamp_mean_absolute_error: 0.4354 - lr: 0.0037\n",
      "Epoch 19/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.2938 - output_resource_loss: 1.3354 - output_activity_loss: 0.5610 - output_timestamp_loss: 0.3974 - output_resource_accuracy: 0.6118 - output_activity_accuracy: 0.7557 - output_timestamp_mean_absolute_error: 0.4019 - val_loss: 2.5330 - val_output_resource_loss: 1.5173 - val_output_activity_loss: 0.6048 - val_output_timestamp_loss: 0.4109 - val_output_resource_accuracy: 0.5741 - val_output_activity_accuracy: 0.7445 - val_output_timestamp_mean_absolute_error: 0.4160 - lr: 0.0028\n",
      "Epoch 20/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.2759 - output_resource_loss: 1.3243 - output_activity_loss: 0.5575 - output_timestamp_loss: 0.3941 - output_resource_accuracy: 0.6101 - output_activity_accuracy: 0.7582 - output_timestamp_mean_absolute_error: 0.3995 - val_loss: 2.5287 - val_output_resource_loss: 1.5156 - val_output_activity_loss: 0.6048 - val_output_timestamp_loss: 0.4083 - val_output_resource_accuracy: 0.5811 - val_output_activity_accuracy: 0.7431 - val_output_timestamp_mean_absolute_error: 0.4141 - lr: 0.0019\n",
      "Epoch 21/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.2575 - output_resource_loss: 1.3117 - output_activity_loss: 0.5543 - output_timestamp_loss: 0.3916 - output_resource_accuracy: 0.6138 - output_activity_accuracy: 0.7590 - output_timestamp_mean_absolute_error: 0.3981 - val_loss: 2.5276 - val_output_resource_loss: 1.5148 - val_output_activity_loss: 0.6054 - val_output_timestamp_loss: 0.4073 - val_output_resource_accuracy: 0.5826 - val_output_activity_accuracy: 0.7456 - val_output_timestamp_mean_absolute_error: 0.4057 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "233/233 [==============================] - 1s 6ms/step - loss: 2.2680 - output_resource_loss: 1.3185 - output_activity_loss: 0.5566 - output_timestamp_loss: 0.3930 - output_resource_accuracy: 0.6130 - output_activity_accuracy: 0.7558 - output_timestamp_mean_absolute_error: 0.3986 - val_loss: 2.5434 - val_output_resource_loss: 1.5227 - val_output_activity_loss: 0.6075 - val_output_timestamp_loss: 0.4133 - val_output_resource_accuracy: 0.5774 - val_output_activity_accuracy: 0.7458 - val_output_timestamp_mean_absolute_error: 0.4052 - lr: 0.0019\n",
      "Epoch 23/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.2799 - output_resource_loss: 1.3265 - output_activity_loss: 0.5584 - output_timestamp_loss: 0.3949 - output_resource_accuracy: 0.6101 - output_activity_accuracy: 0.7568 - output_timestamp_mean_absolute_error: 0.4009 - val_loss: 2.5385 - val_output_resource_loss: 1.5220 - val_output_activity_loss: 0.6077 - val_output_timestamp_loss: 0.4088 - val_output_resource_accuracy: 0.5793 - val_output_activity_accuracy: 0.7477 - val_output_timestamp_mean_absolute_error: 0.4074 - lr: 0.0028\n",
      "Epoch 24/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.2971 - output_resource_loss: 1.3355 - output_activity_loss: 0.5621 - output_timestamp_loss: 0.3994 - output_resource_accuracy: 0.6112 - output_activity_accuracy: 0.7534 - output_timestamp_mean_absolute_error: 0.4068 - val_loss: 2.5484 - val_output_resource_loss: 1.5267 - val_output_activity_loss: 0.6120 - val_output_timestamp_loss: 0.4097 - val_output_resource_accuracy: 0.5805 - val_output_activity_accuracy: 0.7372 - val_output_timestamp_mean_absolute_error: 0.4208 - lr: 0.0037\n",
      "Epoch 25/25\n",
      "233/233 [==============================] - 1s 5ms/step - loss: 2.3089 - output_resource_loss: 1.3443 - output_activity_loss: 0.5643 - output_timestamp_loss: 0.4003 - output_resource_accuracy: 0.6065 - output_activity_accuracy: 0.7521 - output_timestamp_mean_absolute_error: 0.4049 - val_loss: 2.5590 - val_output_resource_loss: 1.5342 - val_output_activity_loss: 0.6093 - val_output_timestamp_loss: 0.4155 - val_output_resource_accuracy: 0.5741 - val_output_activity_accuracy: 0.7397 - val_output_timestamp_mean_absolute_error: 0.4109 - lr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_targets_resource_cat = to_categorical(train_targets_resource, num_classes=num_resources)\n",
    "test_targets_resource_cat = to_categorical(test_targets_resource, num_classes=num_resources)\n",
    "\n",
    "train_targets_activity_cat = to_categorical(train_targets_activity, num_classes=num_activities)\n",
    "test_targets_activity_cat = to_categorical(test_targets_activity, num_classes=num_activities)\n",
    "\n",
    "\n",
    "# Note: Assuming timestamp targets are continuous and don't need to be converted to categorical\n",
    "\n",
    "history = model.fit([train_resource, train_activity, train_timestamp], \n",
    "                    [train_targets_resource_cat, train_targets_activity_cat, train_targets_timestamp],\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=([test_resource, test_activity, test_timestamp], \n",
    "                                     [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]),\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_scheduler])  # Add other callbacks as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 0s 2ms/step - loss: 2.5590 - output_resource_loss: 1.5342 - output_activity_loss: 0.6093 - output_timestamp_loss: 0.4155 - output_resource_accuracy: 0.5741 - output_activity_accuracy: 0.7397 - output_timestamp_mean_absolute_error: 0.4109\n",
      "[2.5590178966522217, 1.5341659784317017, 0.6093142628669739, 0.4155366122722626, 0.5740770101547241, 0.7396700978279114, 0.41085922718048096]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(\n",
    "    [test_resource, test_activity, test_timestamp],\n",
    "    [test_targets_resource_cat, test_targets_activity_cat, test_targets_timestamp]\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to an H5 file\n",
    "model.save('12O.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663/663 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all inputs\n",
    "predictions = model.predict([windows_resource, windows_activity, windows_timestamp])\n",
    "\n",
    "# Extract predictions for categorical attributes (softmax probabilities)\n",
    "predictions_resource = predictions[0]        # ID predictions\n",
    "predictions_activity = predictions[1]  # Resource predictions\n",
    "\n",
    "# Extract predictions for numerical attribute (timestamp)\n",
    "predictions_timestamp = predictions[2] # Timestamp predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def compute_anomaly_scores(predictions, actuals):\n",
    "    # For categorical predictions, convert actuals to one-hot for comparison\n",
    "    actuals_one_hot = to_categorical(actuals, num_classes=predictions.shape[-1])\n",
    "    \n",
    "    max_predictions = np.max(predictions, axis=-1)\n",
    "    actual_predictions = np.sum(predictions * actuals_one_hot, axis=-1)  # Extract the probability of the actual class\n",
    "    \n",
    "    anomaly_scores = (max_predictions - actual_predictions) / max_predictions\n",
    "    \n",
    "    return anomaly_scores\n",
    "\n",
    "# Assuming targets_id, targets_resource, targets_activity, targets_role are the true values for these attributes\n",
    "anomaly_scores_resource = compute_anomaly_scores(predictions_resource, targets_resource)\n",
    "anomaly_scores_activity = compute_anomaly_scores(predictions_activity, targets_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores_continuous(predictions, actuals, normalization_factor):\n",
    "    \"\"\"\n",
    "    Compute anomaly scores for continuous attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predicted values.\n",
    "    - actuals: numpy array of actual values.\n",
    "    - normalization_factor: normalization factor (e.g., standard deviation of the attribute).\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of anomaly scores.\n",
    "    \"\"\"\n",
    "    # Calculate absolute differences\n",
    "    differences = np.abs(predictions - actuals)\n",
    "    \n",
    "    # Normalize the differences\n",
    "    anomaly_scores = differences / normalization_factor\n",
    "    \n",
    "    return anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_factor = np.std(targets_timestamp)       # Example normalization factor (standard deviation)\n",
    "anomaly_scores_timestamp = compute_anomaly_scores_continuous(predictions_timestamp, targets_timestamp, normalization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp, threshold=0.98):\n",
    "    # Ensure all inputs are numpy arrays of the same shape\n",
    "    anomaly_scores_resource = np.array(anomaly_scores_resource).flatten()\n",
    "    anomaly_scores_activity = np.array(anomaly_scores_activity).flatten()\n",
    "    anomaly_scores_timestamp = np.array(anomaly_scores_timestamp).flatten()\n",
    "\n",
    "    # Check if all arrays have the same length\n",
    "    if not (len(anomaly_scores_resource) == len(anomaly_scores_activity) == len(anomaly_scores_timestamp)):\n",
    "        raise ValueError(\"All input anomaly scores must have the same length.\")\n",
    "\n",
    "    # Find the maximum anomaly score across all attributes for each case\n",
    "    max_scores = np.maximum.reduce([anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp])\n",
    "\n",
    "    # Classify cases as anomalous if the maximum anomaly score exceeds the threshold\n",
    "    anomalous_cases = max_scores > threshold\n",
    "    \n",
    "    return anomalous_cases\n",
    "\n",
    "# Now use the anomaly scores for resource, activity, and timestamp in the classification\n",
    "anomalous_cases = classify_cases(anomaly_scores_resource, anomaly_scores_activity, anomaly_scores_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True: anomaly, False: no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21209</th>\n",
       "      <td>5013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21210</th>\n",
       "      <td>5013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21211</th>\n",
       "      <td>5013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21212</th>\n",
       "      <td>5013</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21213</th>\n",
       "      <td>5014</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21214 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  predicted\n",
       "0         0      False\n",
       "1         0      False\n",
       "2         0      False\n",
       "3         1       True\n",
       "4         1      False\n",
       "...     ...        ...\n",
       "21209  5013      False\n",
       "21210  5013      False\n",
       "21211  5013      False\n",
       "21212  5013       True\n",
       "21213  5014      False\n",
       "\n",
       "[21214 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the case_indices_array corresponding to case_resource\n",
    "mapping = pd.DataFrame({'case': case_indices})\n",
    "mapping['predicted'] = anomalous_cases\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case\n",
       "0       False\n",
       "1        True\n",
       "2       False\n",
       "3       False\n",
       "4        True\n",
       "        ...  \n",
       "5010    False\n",
       "5011    False\n",
       "5012    False\n",
       "5013     True\n",
       "5014    False\n",
       "Name: predicted, Length: 5015, dtype: bool"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_prediction = mapping.groupby('case')['predicted'].any()\n",
    "case_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking):\n",
    "    from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "    from pm4py.algo.conformance.alignments.petri_net import variants\n",
    "    from pm4py.objects.petri_net.utils import align_utils\n",
    "    max_events=0\n",
    "    for trace in log:\n",
    "        counter=0\n",
    "        for event in trace:\n",
    "            counter+=1\n",
    "        if counter > max_events:\n",
    "            max_events=counter\n",
    "    parameters={}\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_SYNC_COST_FUNCTION] = list(map(lambda i: .1*i, range(max_events*2)))\n",
    "    parameters[alignments.Variants.VERSION_STATE_EQUATION_A_STAR.value.Parameters.PARAM_TRACE_COST_FUNCTION]=list(map(lambda i: align_utils.STD_MODEL_LOG_MOVE_COST-.1*i, range(max_events*2)))\n",
    "    aligned_traces = alignments.apply_log(log, net, initial_marking, final_marking, variant=variants.state_equation_a_star, parameters=parameters)\n",
    "    return aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa66cae0fcaf41a1a3c051ddf7f1f5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"../../data/model/Model_O.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "aligned_traces = generate_alignments_adjusted_tracecost_pkl(log, net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conformance_status_by_fitness(aligned_traces):\n",
    "    conformance_status = []\n",
    "    for alignment in aligned_traces:\n",
    "        fitness = alignment['fitness']\n",
    "        # If the fitness is 1.0, the trace is conforming\n",
    "        if fitness == 1.0:\n",
    "            conformance_status.append(0)\n",
    "        else:\n",
    "            conformance_status.append(1)\n",
    "    return conformance_status\n",
    "\n",
    "# Get the conformance status list from the aligned traces\n",
    "conformance = extract_conformance_status_by_fitness(aligned_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conformity</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5015 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conformity  predicted\n",
       "0              0          0\n",
       "1              1          1\n",
       "2              1          0\n",
       "3              0          0\n",
       "4              1          1\n",
       "...          ...        ...\n",
       "5010           1          0\n",
       "5011           1          0\n",
       "5012           0          0\n",
       "5013           1          1\n",
       "5014           1          0\n",
       "\n",
       "[5015 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame({'conformity': conformance})\n",
    "ground_truth['predicted'] = case_prediction\n",
    "\n",
    "# Convert False to 0 and True to 1\n",
    "ground_truth['predicted'] = [int(value) for value in ground_truth['predicted']]\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TP, TN, FP, FN\n",
    "TP = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 1)).sum()\n",
    "TN = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 0)).sum()\n",
    "FP = ((ground_truth['conformity'] == 0) & (ground_truth['predicted'] == 1)).sum()\n",
    "FN = ((ground_truth['conformity'] == 1) & (ground_truth['predicted'] == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Dev: 0.85\n"
     ]
    }
   ],
   "source": [
    "precision_dev = TP / (TP + FP)\n",
    "print(f\"Precision Dev: {precision_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Dev: 0.64\n"
     ]
    }
   ],
   "source": [
    "recall_dev = TP / (TP + FN)\n",
    "print(f\"Recall Dev: {recall_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision No Dev: 0.65\n"
     ]
    }
   ],
   "source": [
    "precision_no_dev = TN / (TN + FN)\n",
    "print(f\"Precision No Dev: {precision_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall No Dev: 0.85\n"
     ]
    }
   ],
   "source": [
    "recall_no_dev = TN / (TN + FP)\n",
    "print(f\"Recall No Dev: {recall_no_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_roc = roc_auc_score(ground_truth['conformity'], ground_truth['predicted'])\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
